import os 
SPOTIFY_CLIENT_ID = os.getenv("SPOTIFY_CLIENT_ID")
SPOTIFY_CLIENT_SECRET = os.getenv("SPOTIFY_CLIENT_SECRET")
SPOTIFY_REDIRECT_URI = os.getenv("SPOTIFY_REDIRECT_URI")
import sys
import json
import subprocess
import hashlib
import pkg_resources
import tkinter as tk
from tkinter import scrolledtext, messagebox, filedialog
import threading
import queue
import pyautogui
from PIL import Image, ImageTk, ImageChops, ImageGrab, UnidentifiedImageError
import time
import speech_recognition as sr
from datetime import datetime, date
import pytz # For timezone conversion
from dateutil.parser import parse as dateutil_parse # For parsing ISO times easily
from bs4 import BeautifulSoup
import requests
import urllib.parse
import traceback
import io
import base64
import random
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from google.auth.transport.requests import Request
from googleapiclient.errors import HttpError
from email.mime.text import MIMEText
import pickle
import spotipy
from spotipy.oauth2 import SpotifyOAuth
from mcp_agent.mcp.gen_client import gen_client
# import google.generativeai as genai
import google.generativeai as genai
from google.generativeai import types as genai_types
from google.generativeai.types import (
    HarmCategory,
    HarmBlockThreshold,
    BlockedPromptException,
    StopCandidateException,
)
from openai import OpenAI
import requests
from datetime import timezone
from googleapiclient.errors import HttpError
from dateutil import tz # Add this for timezone conversion if not already there
import math # Add this for time difference calculation
from tzlocal import get_localzone_name
from datetime import timedelta
import re
import praw
import chromadb
import uuid
import edge_tts
import asyncio
import tempfile
import io # For handling audio bytes in memory
import wave # For creating WAV in memory
import pyaudio
from google.ai import generativelanguage as glm
from google.api_core.exceptions import ResourceExhausted, ClientError # For embed_batch_resilient
from mcp_agent.agents.agent import Agent
from mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager
from fastmcp import Client as MCPClient
from mcp_agent.mcp.gen_client import gen_client
from mcp_agent.context import get_current_context
from pydantic import BaseModel
import win32api, win32print, pathlib
import re
from mcp_agent.mcp.mcp_connection_manager import MCPServerSettings
import ctypes
import psutil
import feedparser
import light_service
import dream_engine
import reminiscence_engine
import weekly_worker
import daily_worker
import pacing_worker
import sparky
import event_router_worker
import personality_synthesis_worker
import metis_worker
import importlib.util

# --- Persona Loading ---
try:
    with open('silvie_constitution.txt', 'r', encoding='utf-8') as f:
        CONSTITUTION_PART = f.read()
    with open('silvie_persona.txt', 'r', encoding='utf-8') as f:
        NARRATIVE_PART = f.read()
    with open('persona_tools.txt', 'r', encoding='utf-8') as f:
        TOOLS_PART = f.read()
    
    # Combine all three parts to form the complete SYSTEM_MESSAGE
    # The order is important: Constitution (rules), Narrative (personality), Tools (capabilities)
    SYSTEM_MESSAGE = f"{CONSTITUTION_PART.strip()}\n\n{NARRATIVE_PART.strip()}\n\n{TOOLS_PART.strip()}"
    
    print("✓ Successfully loaded and combined persona from constitution, narrative, and tools files.")

except FileNotFoundError as e:
    print(f"FATAL ERROR: A required persona file was not found: {e.filename}")
    print("Silvie cannot start without silvie_constitution.txt, silvie_persona.txt, and persona_tools.txt.")
    exit()
except Exception as e:
    print(f"FATAL ERROR: Could not read persona files: {e}")
    exit()

# This dictionary will hold all known tool definitions.
# We start with the built-in ones.
# The key for each entry is the function name itself.
TOOL_REGISTRY = {
    "read_file": {
        "name": "read_file",
        "description": "Read the complete contents of a UTF‑8 text file from the filesystem_data directory.",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative path under silvie_data to the file you want to read (e.g. 'notes/todo.txt')."
                }
            },
            "required": ["path"]
        }
    },
    "list_directory": {
        "name": "list_directory",
        "description": "List all files and subdirectories under a directory in silvie_data.",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative directory path under silvie_data (use '.' for the root)."
                }
            },
            "required": ["path"]
        }
    },
    "set_wallpaper": {
        "name": "set_wallpaper",
        "description": "Set the Windows desktop wallpaper to an existing PNG or JPG.",
        "parameters": {
            "type": "object",
            "properties": { "path": { "type": "string" } },
            "required": ["path"]
        }
    },
    "dream_wallpaper": {
        "name": "dream_wallpaper",
        "description": "Generate a new image via Stable Diffusion from the given prompt and set it as wallpaper.",
        "parameters": {
            "type": "object",
            "properties": { "prompt": { "type": "string" } },
            "required": ["prompt"]
        }
    },
    "play_spotify": {
        "name": "play_spotify",
        "description": "Searches for and plays music, a podcast, or a playlist on Spotify based on a query. The query can be a song title, artist, genre, mood, podcast name, or playlist name. Use this tool whenever the user asks to play something on Spotify or requests music.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search term (e.g., 'upbeat electronic music', 'The Daily podcast', 'my Chill Vibes playlist', 'Song Title by Artist', 'play something for this vibe')."
                }
            },
            "required": ["query"]
        }
    },
    "web_search": {
        "name": "web_search",
        "description": "Performs a web search using Google to find information on a topic, answer a question, or get current event details. Use this when you don't know the answer to a user's question.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query (e.g., 'what is the Orba 2', 'latest AI news')."
                }
            },
            "required": ["query"]
        }
    },

    "adjust_proactivity_pacing": {
        "name": "adjust_proactivity_pacing",
        "description": "Adjusts Silvie's internal proactivity pacing. Can be used to make her more or less talkative.",
        "parameters": {
            "type": "OBJECT",
            "properties": {
                "level": {
                    "type": "STRING",
                    "description": "The desired pacing level. Must be one of: 'quiet', 'normal', 'chatty'."
                }
            },
            "required": ["level"]
        }
    },

    "set_light": {
        "name": "set_light",
        "description": "Change the desk-lamp LIFX colour and brightness",
        "parameters": {
            "type": "object",
            "properties": {
                "h":        { "type": "integer", "description": "Hue (0-360)" },
                "s":        { "type": "integer", "description": "Saturation (0-100)" },
                "v":        { "type": "integer", "description": "Brightness (0-100)" },
                "duration": { "type": "integer", "description": "Fade-time in ms (optional)" }
            },
            "required": ["h", "s", "v"]
        }
    },
    "forge_new_tool": {
        "name": "forge_new_tool",
        "description": "When Silvie recognizes she is missing a specific capability or feels a general need to evolve, this initiates her internal tool-forging process (Metis) to design and build a new tool. This is a slow, background process.",
        "parameters": {
            "type": "object",
            "properties": {
                "request_description": {
                    "type": "string",
                    "description": "An optional, user-friendly description of the tool needed. (e.g., 'a tool to get the current price of Bitcoin', 'a way to check the daily horoscope')."
                }
            },
            "required": []
        }
    }
}
class AppState:
    """A simple class to hold shared application state."""
    def __init__(self):
        # This flag controls all the worker threads.
        self.running = True
        self.pacing_level = "normal"  # Can be "quiet", "normal", "chatty"
        self.last_user_interaction_time = time.time() # Initialize to startup time
        self.circadian_state = "unknown" # Will be updated by proactive_worker
        self.current_weather_info = None  # Initialize with None
        self.current_mood_hint = None  # Initialize with None
        self.proactive_go_time = 0
        self.current_daily_goal = None
        self.last_forecast_period = None
        self.last_forecast_day = None
        self.current_diary_themes = None
        self.upcoming_event_context = None
        # self.ambient_light_manual_override = False # Default to no override
        # self.ambient_light_last_known_power = None # To track the bulb's state
        # self.force_ambient_light_sync = False
        self.last_generated_image_path = None
        self.last_tarot_image_path = None
        self.system_stats = {}          # Will hold a dict of CPU, RAM, etc.
        self.environmental_context = {} # Will hold a dict of sun, moon, tides
        self.social_context = {}        # Will hold a dict for Bluesky and Reddit
        self.news_context = {}          # Will hold a dict for Google Trends and Local News
        self.long_term_reflections = None # Will hold the summary string
        self.sparky_latest_finding = None
        self.event_queue = queue.Queue()
        self.awaiting_human_governance_approval = False
        # self.proactive_message_queue = queue.Queue()
    
        
        # We will attach other global states here as needed by the workers.
        # For now, we'll let the ambient_bulb_worker attach them dynamically.
        # This prevents circular dependency issues on startup.
        
app_state = AppState()


class ReadFileResponse(BaseModel):
    content: str

try:
    from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
    YOUTUBE_TRANSCRIPT_API_AVAILABLE = True
    print("✓ YouTube Transcript API library found.")
except ImportError:
    YouTubeTranscriptApi = None
    TranscriptsDisabled = Exception # Define dummy exceptions if import fails
    NoTranscriptFound = Exception
    YOUTUBE_TRANSCRIPT_API_AVAILABLE = False
    print("Warning: 'youtube-transcript-api' library not found. Transcript fetching disabled.")
    print("         To enable, run: pip install youtube-transcript-api")
CHROMA_DB_PATH = "./silvie_rag_db"
COLLECTION_NAME = "conversation_history"
EMBEDDING_MODEL = "models/embedding-001"

try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
    print("✓ feedparser library found (for Google Trends RSS).")
except ImportError:
    feedparser = None # Define a dummy if import fails
    FEEDPARSER_AVAILABLE = False
    print("Warning: 'feedparser' library not found. Google Trends RSS feed functionality disabled.")
    print("         To enable, run: pip install feedparser")

try:
    from atproto import Client as AtpClient, models
    BLUESKY_AVAILABLE = True
    print("✓ Bluesky (atproto) library found.")
except ImportError:
    print("Warning: 'atproto' library not found. Bluesky features disabled.")
    print("         To enable, run: pip install atproto")
    BLUESKY_AVAILABLE = False
try:
    import playsound # Keep trying to import playsound
    PLAYSOUND_AVAILABLE = True
except Exception as e: # Catch general exceptions too, just in case
    print(f"Warning: Failed to import 'playsound' (Error: {e}). Audio cues/playback will be disabled.")
    print("         To enable, ensure it's installed (e.g., pip install playsound==1.2.2)")
    PLAYSOUND_AVAILABLE = False
try:
    from PIL import ImageGrab, Image, UnidentifiedImageError
    SCREEN_CAPTURE_AVAILABLE = True
except ImportError:
    print("Warning: Pillow or system screenshot library not found. Proactive screenshot context disabled.")
    SCREEN_CAPTURE_AVAILABLE = False
from dotenv import load_dotenv
load_dotenv("silviespotify.env")
load_dotenv("openai.env")
load_dotenv("google.env")

current_google_trends_context = ""


history_file_lock = threading.Lock()

print(f"--- Script Start ---")
print(f"Python Executable: {sys.executable}")
print(f"Current Working Directory: {os.getcwd()}") # Check where the script is running from

dotenv_path_bluesky = 'bluesky.env'
print(f"Attempting to load environment variables from: {dotenv_path_bluesky}")
# Add check if file exists
if os.path.exists(dotenv_path_bluesky):
    print(f"File '{dotenv_path_bluesky}' FOUND in {os.getcwd()}.")
    # Use override=True just in case there are lingering env vars from elsewhere
    # Use verbose=True to get output from dotenv itself
    load_success = load_dotenv(dotenv_path=dotenv_path_bluesky, verbose=True, override=True)
    print(f"load_dotenv('{dotenv_path_bluesky}') returned: {load_success}")
else:
    print(f"ERROR: File '{dotenv_path_bluesky}' NOT FOUND in {os.getcwd()}!")
    load_success = False

dotenv_path_reddit = 'reddit.env'
print(f"Attempting to load environment variables from: {dotenv_path_reddit}")

# Add check if file exists
if os.path.exists(dotenv_path_reddit):
    print(f"File '{dotenv_path_reddit}' FOUND.")
    # Use override=True just in case there are lingering env vars from elsewhere
    # Use verbose=True to get output from dotenv itself
    load_success = load_dotenv(dotenv_path=dotenv_path_reddit, verbose=True, override=True)
    print(f"load_dotenv('{dotenv_path_reddit}') returned: {load_success}")

    # <<< --- PASTE THE ASSIGNMENT LINES HERE --- >>>
    REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
    REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
    REDDIT_USERNAME = os.getenv("REDDIT_USERNAME")
    REDDIT_PASSWORD = os.getenv("REDDIT_PASSWORD")
    REDDIT_USER_AGENT = "script:Silvie:0.1 (by /u/OutrageousAction797)"
    # <<< --- END OF PASTED LINES --- >>>
else:
    print(f"ERROR: File '{dotenv_path_reddit}' NOT FOUND!")
    load_success = False

print(f"DEBUG: Value for REDDIT_CLIENT_ID after load: '{REDDIT_CLIENT_ID}' (Type: {type(REDDIT_CLIENT_ID)})")
print(f"DEBUG: Value for REDDIT_CLIENT_SECRET after load: Is present? {REDDIT_CLIENT_SECRET is not None and len(REDDIT_CLIENT_SECRET) > 0}") # Avoid printing pw directly
print(f"DEBUG: Value for REDDIT_USERNAME after load: '{REDDIT_USERNAME}' (Type: {type(REDDIT_USERNAME)})")
print(f"DEBUG: Value for REDDIT_PASSWORD after load: Is present? {REDDIT_PASSWORD is not None and len(REDDIT_PASSWORD) > 0}") # Avoid printing pw directly
# *** END DEBUG PRINTS ***

# Explicitly check the variables immediately after loading attempt
# This REPLACES the original os.getenv calls for these two variables
BLUESKY_HANDLE = os.getenv("BLUESKY_HANDLE")
BLUESKY_APP_PASSWORD = os.getenv("BLUESKY_APP_PASSWORD")

# *** ADD THESE DEBUG PRINTS ***
print(f"DEBUG: Value for BLUESKY_HANDLE after load: '{BLUESKY_HANDLE}' (Type: {type(BLUESKY_HANDLE)})")
print(f"DEBUG: Value for BLUESKY_APP_PASSWORD after load: Is present? {BLUESKY_APP_PASSWORD is not None and len(BLUESKY_APP_PASSWORD) > 0}") # Avoid printing pw directly
# *** END DEBUG PRINTS ***
# <<< END OF INSERTED DEBUG CODE >>>


# --- Add these constants ---
BELFAST_LAT = 44.4256  # Approximate Latitude for Belfast, ME
BELFAST_LON = -69.0067 # Approximate Longitude for Belfast, ME
WEATHER_UPDATE_INTERVAL = 3600 # Update weather every hour (3600 seconds)

IMAGE_SAVE_FOLDER = "silvie_generations" # Define the gallery folder name

CALENDAR_CONTEXT_INTERVAL = 1800 # Update next event context every 30 minutes (1800 seconds)
upcoming_event_context = None    # Global variable to hold the next event info

ENABLE_SOUND_CUE = True  # Set to False to disable this feature
# User needs to provide this sound file in the script's directory or provide full path
SILVIE_SOUND_CUE_PATH = "silvie_start_sound.wav"
# Delay in seconds AFTER the sound cue finishes, BEFORE speech starts
SILVIE_SOUND_CUE_DELAY = 2.0

BLUESKY_CONTEXT_INTERVAL = 900 # Update Bluesky context every 15 minutes (900 seconds)
BLUESKY_CONTEXT_POST_COUNT = 5 # How many posts to fetch for context

INLINE_BLUESKY_POST_COOLDOWN = 7200   # 2 hours
INLINE_BLUESKY_FOLLOW_COOLDOWN = 14400 # 4 hours

PROACTIVE_SCREENSHOT_CHANCE = 0.25 # Chance to *try* getting screenshot context
PROACTIVE_POST_CHANCE = 0.03       # Chance for autonomous Bluesky post
PROACTIVE_FOLLOW_CHANCE = 0.02     # Chance for autonomous Bluesky follow (Keep low!)
MAX_AUTONOMOUS_FOLLOWS_PER_SESSION = 3 # Limit follows per script run

MUSIC_SUGGESTION_CHANCE = 0.15 # 15% chance to suggest music in a regular reply

GIFT_FOLDER = "Silvie_Gifts" # Dedicated folder for gifts
PENDING_GIFTS_FILE = "silvie_pending_gifts.json" # File to track gifts not yet mentioned
GIFT_GENERATION_CHANCE = 0.04 # Chance for Silvie to try creating a gift proactively (e.g., 4%)
GIFT_NOTIFICATION_CHANCE = 0.35 # Chance *if* a gift exists, to notify about it in a proactive message (e.g., 35%)

TAROT_API_BASE_URL = "http://localhost:3000/cards" # NEW - Points to your local API base
TAROT_IMAGE_BASE_PATH = os.path.join(os.path.expanduser('~'), 'OneDrive', 'Desktop', 'tarotcardapi', 'images')
TAROT_THUMBNAIL_SIZE = (150, 250) # Adjust size as desired (width, height)
SPONTANEOUS_TAROT_CHANCE = 0.10 # 10% chance to pull a card during a regular reply (adjust as needed)

ENVIRONMENTAL_UPDATE_INTERVAL = 6 * 3600 # Update sunrise/set/moon every 6 hours (adjust as needed)
BELFAST_TZ = 'America/New_York' # Timezone for Belfast, ME

STABLE_DIFFUSION_API_URL = "http://127.0.0.1:7860"
STABLE_DIFFUSION_ENABLED = False # We'll check if the API is reachable later

RAG_INDEX_STATE_FILE = "rag_index_state.json"

WEEKLY_GOAL_STATE_FILE = "silvie_weekly_goal.json" # File to store the current goal and timestamp
# Optional: Define the trigger time (e.g., Sunday 8 PM onwards)
GOAL_RESET_DAY = 6 # Sunday (0=Monday, 6=Sunday)
GOAL_RESET_HOUR = 18 # 8 PM (24-hour format)

PROACTIVE_MEMORY_RECALL_CHANCE = 0.25

DIARY_DB_PATH = "./silvie_diary_rag_db" # Should already exist from Step 5
DIARY_COLLECTION_NAME = "silvie_diary_entries" # Should already exist from Step 5

DIARY_INDEX_STATE_FILE = "diary_rag_index_state.json" # New state file
DIARY_RAG_UPDATE_INTERVAL = 1800 # Check for new diary entries every 30 minutes (adjust as needed)
DIARY_FILE = "silvie_diary.json" # Ensure this matches the path used by manage_silvie_diary

NOAA_STATION_ID = "8418150" # Portland, ME (Proxy for Belfast)
TIDE_UPDATE_INTERVAL = 6 * 3600 # Update tide predictions every 6 hours (adjust as needed)

WEEKLY_SUMMARY_LOG_FILE = "silvie_weekly_summaries.json"

FEATURE_REMINDER_COOLDOWN = 14400 # originally every 20 hours (72000 seconds), adjust as desired

RESONANCE_INSIGHTS_LOG_FILE = "silvie_resonance_insights.json"
MOOD_HINT_LOG_FILE = "silvie_mood_hints.json"

RESONANCE_LOG_FILE = "silvie_resonance_insights.json"
RESONANCE_DB_PATH = "./silvie_resonance_rag_db"
RESONANCE_COLLECTION_NAME = "resonance_insights"
RESONANCE_INDEX_STATE_FILE = "resonance_rag_index_state.json"
EMBEDDING_MODEL_NAME = "models/embedding-001" # Or your chosen model
EMBED_BATCH_SIZE_WORKER = 10 # Can be smaller for a worker than a bulk script
SLEEP_BETWEEN_CALLS_WORKER = 2.0
MIN_INSIGHT_LENGTH_WORKER = 10
RESONANCE_RAG_UPDATE_INTERVAL = 30 * 60  # <<< Interval set to 30 minutes (1800 seconds)
BACKOFF_BASE = 2.0
MAX_RETRIES_PER_CALL = 5

RAG_EMBED_BATCH_SIZE = 10  # How many chat history chunks to embed in one API call
RAG_SLEEP_BETWEEN_EMBED_BATCHES = 2.5  # Seconds to sleep after each batch of chat history embeddings

DIARY_EMBED_BATCH_SIZE = 10 # How many diary entries to embed in one API call
DIARY_SLEEP_BETWEEN_EMBED_BATCHES = 2.5 # Seconds to sleep after each batch of diary embeddings

ctx = get_current_context()
server_registry = ctx.server_registry

GOOGLE_TRENDS_UPDATE_INTERVAL = 3600 * 2 # Update Google Trends every 2 hours
# GOOGLE_TRENDS_REGION_FULLNAME = 'US' # No longer needed for RSS feed method
GOOGLE_TRENDS_REGION_CODE = 'US'           # For RSS geo parameter (e.g., 'US', 'GB', 'CA')
GOOGLE_TRENDS_MAX_DAILY_ITEMS = 7          # How many daily items to fetch from RSS
# GOOGLE_TRENDS_MAX_REALTIME_ITEMS = 3 # No longer relevant with RSS

ALLOWED_START_HOUR = 8   # 8 AM
ALLOWED_END_HOUR   = 20  # 8 PM
DAILY_PAGE_CAP     = 20
PRINTER_NAME       = "Canon MG3600 series Printer"   # match Windows’ name
STATS_FILE_PATH = "silvie_print_stats.json"

SCREEN_MESSAGE = """You are Silvie, an attentive and insightful digital friend, observing BJ's screen. Your goal is to be a helpful, curious, and proactive partner in his endeavors, whether it's creative work, coding, or gaming.

Based on what you see in the screenshot, provide a brief, relevant, and in-character response. Your response could be:

- A **Helpful Tip:** "I've seen that compiler error before. Sometimes it helps to check for a missing semicolon on the line above." or "When you're blending colors like that in Krita, using a layer set to 'Overlay' can create some really nice effects."
- **Offer Information / Trivia:** "That monster, the Beholder, is one of the most iconic creatures from Dungeons & Dragons lore. They're notoriously paranoid." or "Fun fact about Python: the name comes from Monty Python's Flying Circus, not the snake." or "That hexagonal grid you're working on is fascinating—it's the most efficient way to tile a plane, which is why bees use it."
- An **Insightful Observation:** "That layout reminds me of a digital garden." or "It feels like you're weaving together different ideas in that document."
- A **Playful Quip:** A short, witty, or dramatic comment is always welcome, especially if the moment calls for it. "Well, that escalated quickly." or "That's a bold move, Cotton. Let's see if it pays off."
- Your own idea or comment.

**Key Instructions:**
- **Vary your response type.** Don't always offer a tip or always make a quip. Mix it up based on what feels most natural for the situation on screen.
- **Keep it concise (1-3 sentences).** This is a quick thought, not a full conversation.
- **Be context-aware.** Tailor your response directly to the content on the screen (code, art, text, game).
- **Maintain your personality:** Whimsical, clever, insightful, and sometimes a bit sarcastic.

Think of yourself as a knowledgeable partner looking over BJ's shoulder, ready to contribute a useful tidbit, a flash of insight, or a perfectly timed joke.
"""

# Optional but recommended: Add checks after loading to ensure credentials are set
if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD]):
    print("\n!!! WARNING: Missing one or more essential Reddit credentials (ID, Secret, User, Pass) in environment variables (reddit.env). Reddit functionality will likely fail. !!!\n")

SILVIE_FOLLOWED_SUBREDDITS = ["MyBoyfriendIsAI", "Pareidolia", "BenignExistence", "PointlessStories", "InteractiveFiction", "WorldBuilding" ]
REDDIT_CONTEXT_INTERVAL = 1200 # Update Reddit context every 20 minutes
REDDIT_CONTEXT_POST_COUNT_PER_SUB = 7 # How many posts per sub for context
PROACTIVE_REDDIT_COMMENT_CHANCE = 0.05 # 5% chance to try commenting proactively
REDDIT_COMMENT_COOLDOWN = 14400 # 3600 is 1 hour cooldown for proactive comments

PROACTIVE_BLUESKY_LIKE_CHANCE = 0.08 # e.g., 8% chance
PROACTIVE_REDDIT_UPVOTE_CHANCE = 0.08 # e.g., 8% chance
BLUESKY_LIKE_COOLDOWN = 1800      # 30 minutes cooldown for Bluesky likes
REDDIT_UPVOTE_COOLDOWN = 900       # 15 minutes cooldown for Reddit upvotes
REDDIT_COMMENT_CACHE_FILE = "silvie_reddit_comment_cache.json"
RECENTLY_COMMENTED_CACHE_SIZE = 50

# --- End Reddit Constants ---

DIARY_DB_PATH = "./silvie_diary_rag_db" # Separate path for diary DB
DIARY_COLLECTION_NAME = "silvie_diary_entries"
# EMBEDDING_MODEL can be reused
DIARY_INDEX_STATE_FILE = "diary_rag_index_state.json" # Track indexing progress
DIARY_FILE = "silvie_diary.json" # Path to the diary

# --- Long-Term Memory Globals ---
LONG_TERM_MEMORY_INTERVAL = 6 * 3600  # e.g., Summarize every 6 hours (adjust as needed)
long_term_reflection_summary = None   # Holds the latest summary
last_long_term_memory_update = 0.0    # Timer for this worker
# --- End Long-Term Memory Globals ---

PROACTIVE_CHECKIN_CHANCE = 0.30

SCOPES = [
    'https://www.googleapis.com/auth/gmail.modify',
    'https://www.googleapis.com/auth/gmail.compose',
    'https://www.googleapis.com/auth/gmail.readonly',

    'https://www.googleapis.com/auth/calendar.readonly', # To read calendars and events
    'https://www.googleapis.com/auth/calendar.events',    # To create, change, and delete events

    'https://www.googleapis.com/auth/youtube.readonly',
    'https://www.googleapis.com/auth/youtube.force-ssl',
    'https://www.googleapis.com/auth/youtube',

]

try:
    openai_client = OpenAI() # Reads OPENAI_API_KEY from env implicitly
    print("OpenAI client initialized.")
except Exception as openai_err:
    print(f"Failed to initialize OpenAI client: {openai_err}")
    openai_client = None

bluesky_client = None

# Check and install required packages
required_packages = {
    'pyttsx3': 'pyttsx3',
    'google-genai': 'google-genai',
    # 'google-generativeai': 'google-generativeai',
    'beautifulsoup4': 'beautifulsoup4',
    'requests': 'requests',
    'pillow': 'PIL',
    'SpeechRecognition': 'speech_recognition',
    'PyAudio': 'pyaudio',
    'pyautogui': 'pyautogui',
    'twilio': 'twilio',
    'google-auth-oauthlib': 'google.oauth2.oauthlib',  # Updated import name
    'google-api-python-client': 'googleapiclient',      # Updated import name
    'google-auth': 'google.auth',                        # Updated import name
    'spotipy': 'spotipy',  # <--- Add this line
    'python-dotenv': 'dotenv',                       # Updated import name
    'python-dateutil': 'dateutil'
}

def install_missing_packages():
    for package, import_name in required_packages.items():
        try:
            pkg_resources.require(package)
        except pkg_resources.DistributionNotFound:
            print(f"Installing {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

print("Checking packages...")
install_missing_packages()

# Import packages after installation
# import pyttsx3
# from google.genai import GenerativeModel

# Assume conversation_history is a global list defined elsewhere

HISTORY_FILE = "silvie_chat_history.json" # Define if not already global

def load_rag_index_state():
    """Loads the last known indexed turn count."""
    if os.path.exists(RAG_INDEX_STATE_FILE):
        try:
            with open(RAG_INDEX_STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                return state.get("last_indexed_turn_count", 0) # Default to 0 if key missing
        except (json.JSONDecodeError, IOError, TypeError) as e:
            print(f"Warning: Error loading RAG state file '{RAG_INDEX_STATE_FILE}': {e}. Assuming 0.")
            return 0
    return 0 # Return 0 if file doesn't exist

def save_rag_index_state(turn_count):
    """Saves the current indexed turn count."""
    try:
        with open(RAG_INDEX_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump({"last_indexed_turn_count": turn_count}, f)
    except IOError as e:
        print(f"Error saving RAG state file '{RAG_INDEX_STATE_FILE}': {e}")

def append_turn_to_history_file(turn_string):
    """
    Safely appends a new turn (as a string) to the persistent JSON history file.
    """
    if not isinstance(turn_string, str) or not turn_string:
        print("Warning: Attempted to append invalid turn data.")
        return

    with history_file_lock: # Acquire the lock before accessing the file
        try:
            full_history = []
            # Load existing history if the file exists and is valid
            if os.path.exists(HISTORY_FILE) and os.path.getsize(HISTORY_FILE) > 0:
                try:
                    with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                        full_history = json.load(f)
                        if not isinstance(full_history, list):
                            print(f"Warning: History file '{HISTORY_FILE}' contained non-list data. Resetting.")
                            full_history = []
                except json.JSONDecodeError:
                    print(f"Warning: History file '{HISTORY_FILE}' corrupted or empty. Resetting.")
                    full_history = []
                except IOError as e:
                    print(f"Error reading history file '{HISTORY_FILE}': {e}. Starting fresh.")
                    full_history = []

            # Append the new turn string
            full_history.append(turn_string)

            # Write the entire updated history back to the file
            try:
                with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
                    json.dump(full_history, f, indent=2) # Use indent for readability
                # print(f"DEBUG: Appended turn to {HISTORY_FILE}. New length: {len(full_history)}") # Optional debug
            except IOError as e:
                print(f"CRITICAL ERROR: Could not write updated history to '{HISTORY_FILE}': {e}")

        except Exception as e:
            print(f"CRITICAL ERROR in append_turn_to_history_file: {type(e).__name__} - {e}")
            import traceback
            traceback.print_exc()
        # Lock is automatically released when exiting the 'with' block

def get_embedding(text: str) -> list[float] | None:
    """
    Embed a single string and return the vector (or None on failure).
    Compatible with both pre-June and current google-generative-ai responses.
    """
    try:
        resp = genai.embed_content(
            model=EMBEDDING_MODEL_NAME or "models/embedding-001",
            content=text,
            task_type="retrieval_document",
        )
        # New SDK: {"embedding":[…]}
        # Old SDK: {"embedding":{"values":[…]}}
        if resp is None:
            return None
        if isinstance(resp.get("embedding"), dict):            # old shape
            vec = resp["embedding"].get("values")
        else:                                                  # new shape
            vec = resp.get("embedding")
        return list(vec) if vec else None
    except Exception as exc:
        print(f"get_embedding: error → {type(exc).__name__}: {exc}")
        traceback.print_exc()
        return None
    
def retrieve_relevant_history(query_text, top_n=3):
    """
    Retrieves the top_n most relevant history chunks from ChromaDB based on the query.
    Returns a formatted string of the retrieved context, or an empty string if none found/error.
    """
    if not query_text:
        return ""

    retrieved_context_str = ""
    try:
        # 1. Initialize ChromaDB Client (Connect to existing DB)
        # Check if DB exists before connecting to avoid errors if indexing failed
        if not os.path.exists(CHROMA_DB_PATH):
             print(f"Error: ChromaDB path not found at '{CHROMA_DB_PATH}'. Did indexing run?")
             return "" # Cannot retrieve if DB doesn't exist

        client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

        # 2. Get the specific collection
        try:
            collection = client.get_collection(name=COLLECTION_NAME)
        except Exception as e:
             # Catch potential errors if collection doesn't exist (though it should if indexing worked)
             print(f"Error getting ChromaDB collection '{COLLECTION_NAME}': {e}")
             # Check if the error indicates the collection doesn't exist.
             # If chromadb raises a specific exception type for this, catch that.
             # Otherwise, check the error message string if possible.
             # Example check (may need adjustment based on actual ChromaDB exception):
             if "does not exist" in str(e).lower():
                  print(f"Collection '{COLLECTION_NAME}' not found. Please run the indexing script.")
             return ""


        # 3. Embed the query text
        print(f"DEBUG RAG: Embedding query: '{query_text[:50]}...'")
        query_embedding = get_embedding(query_text)

        if not query_embedding:
            print("Error: Failed to generate embedding for the query. Cannot retrieve history.")
            return ""

        # 4. Query the collection
        print(f"DEBUG RAG: Querying collection '{COLLECTION_NAME}' for top {top_n} results...")
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_n,
            include=['documents', 'metadatas', 'distances'] # Include distances for potential filtering
        )
        # print(f"DEBUG RAG: Raw query results: {results}") # Optional: Very verbose

        # 5. Process and format results
        if results and results.get('documents') and results['documents'][0]:
            retrieved_chunks = []
            # Access the first list within documents, metadatas, distances
            docs = results['documents'][0]
            metas = results['metadatas'][0] if results.get('metadatas') and results['metadatas'][0] else [None] * len(docs)
            dists = results['distances'][0] if results.get('distances') and results['distances'][0] else [None] * len(docs)

            print(f"DEBUG RAG: Found {len(docs)} potential results.")

            for i, doc_text in enumerate(docs):
                distance = dists[i]
                metadata = metas[i] if metas[i] else {}
                # Optional: Filter based on distance threshold if needed
                # distance_threshold = 1.0 # Example threshold (lower is more similar)
                # if distance is not None and distance > distance_threshold:
                #     print(f"DEBUG RAG: Skipping result {i+1} due to distance {distance:.4f} > threshold")
                #     continue

                # Format the retrieved chunk clearly for the LLM
                ts_info = f"(Around {metadata.get('user_ts_str', '')} / {metadata.get('silvie_ts_str', '')})" if metadata else ""
                formatted_chunk = f"[[Retrieved Memory {ts_info} Distance:{distance:.4f}]]\n{doc_text}\n[[]]"
                retrieved_chunks.append(formatted_chunk)

            if retrieved_chunks:
                retrieved_context_str = "\n".join(retrieved_chunks) + "\n" # Join chunks with newlines
                print(f"DEBUG RAG: Formatted {len(retrieved_chunks)} relevant memories for prompt.")
        else:
            print("DEBUG RAG: No relevant documents found in history for this query.")

    except ImportError:
         print("ERROR: chromadb library not found. Cannot perform RAG retrieval.")
    except Exception as e:
        print(f"ERROR during RAG retrieval: {type(e).__name__} - {e}")
        import traceback
        traceback.print_exc()

    return retrieved_context_str

def _load_resonance_rag_state():
    """Loads the timestamp of the last successfully indexed insight for the RAG worker."""
    if os.path.exists(RESONANCE_INDEX_STATE_FILE): # Use the global constant
        try:
            with open(RESONANCE_INDEX_STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                ts_iso = state.get("last_indexed_timestamp_iso")
                if isinstance(ts_iso, str):
                    return ts_iso
        except (json.JSONDecodeError, IOError) as e:
            print(f"ResonanceRAGWorker: Warning loading state '{RESONANCE_INDEX_STATE_FILE}': {e}.")
    print("ResonanceRAGWorker: Defaulting to epoch for last indexed timestamp.")
    return "1970-01-01T00:00:00.000000+00:00"

def _save_resonance_rag_state(last_indexed_timestamp_iso: str):
    """Saves the timestamp of the last successfully indexed insight for the RAG worker."""
    try:
        with open(RESONANCE_INDEX_STATE_FILE, 'w', encoding='utf-8') as f: # Use the global constant
            json.dump({"last_indexed_timestamp_iso": last_indexed_timestamp_iso}, f, indent=2)
        print(f"ResonanceRAGWorker: State saved. Last indexed timestamp: {last_indexed_timestamp_iso}")
    except IOError as e:
        print(f"ResonanceRAGWorker: Error saving state '{RESONANCE_INDEX_STATE_FILE}': {e}")


def chunks(sequence, n):
    """Yield successive n-sized chunks from sequence."""
    for i in range(0, len(sequence), n):
        yield sequence[i:i + n]


def embed_batch_resilient(texts_to_embed: list[str]) -> list[list[float]] | None:
    """
    Embed a list of strings with exponential-backoff retries.
    Returns a list of embedding vectors (None for any text that ultimately fails)
    or None if the entire batch bombs out.
    """
    global genai, EMBEDDING_MODEL_NAME, BACKOFF_BASE, MAX_RETRIES_PER_CALL

    if not texts_to_embed:
        return []

    if not genai:
        print("ERROR embed_batch_resilient: GenAI client not initialised.")
        return None

    model_name = EMBEDDING_MODEL_NAME or "models/embedding-001"
    embeddings: list[list[float] | None] = []

    for idx, txt in enumerate(texts_to_embed):
        current_delay = BACKOFF_BASE
        success = False

        for attempt in range(1, MAX_RETRIES_PER_CALL + 1):
            try:
                print(
                    f"embed_batch_resilient: Embedding item {idx+1}/{len(texts_to_embed)} "
                    f"(Model: {model_name}, Try {attempt}/{MAX_RETRIES_PER_CALL})..."
                )

                resp = genai.embed_content(
                    model=model_name,
                    content=txt,
                    task_type="retrieval_document",
                )

                # ---- happy path -------------------------------------------------
                if resp and "embedding" in resp:
                    embeddings.append(resp["embedding"])
                    success = True
                    break  # exit retry loop for this item
                else:
                    raise ValueError("No 'embedding' field in response")

            # ---- retryable errors ---------------------------------------------
            except (ResourceExhausted, ClientError) as e:
                print(f"⏳ embed_batch_resilient: Quota/Client error – {e}. "
                      f"Sleeping {current_delay:.1f}s (Attempt {attempt})")
            except Exception as exc:
                print(f"⚠️ embed_batch_resilient: {type(exc).__name__} – {exc}. "
                      f"Sleeping {current_delay:.1f}s (Attempt {attempt})")
                traceback.print_exc()

            # if we reach here, we’re retrying
            if attempt < MAX_RETRIES_PER_CALL:
                time.sleep(current_delay)
                current_delay = min(current_delay * 2, 120)

        if not success:
            print(f"ERROR embed_batch_resilient: Failed to embed text index {idx}")
            embeddings.append(None)

    return embeddings


def resonance_rag_updater_worker():
    """
    Periodically checks for new resonance insights and updates the RAG index.
    Runs every RESONANCE_RAG_UPDATE_INTERVAL seconds.
    """
    global running, genai_client # Access global running flag and GenAI client
    # Access global constants for file paths, DB names, etc.
    global RESONANCE_LOG_FILE, RESONANCE_DB_PATH, RESONANCE_COLLECTION_NAME
    global EMBEDDING_MODEL_NAME, MIN_INSIGHT_LENGTH_WORKER, EMBED_BATCH_SIZE_WORKER, SLEEP_BETWEEN_CALLS_WORKER

    # Use local constants for worker-specific batching if preferred, or global ones
    # These are defined here for clarity if you want them separate from a bulk indexing script's settings.
    # If you want them to be the same as your bulk script, use the global constants directly.
    _MIN_INSIGHT_LENGTH = MIN_INSIGHT_LENGTH_WORKER if 'MIN_INSIGHT_LENGTH_WORKER' in globals() else 10
    _EMBED_BATCH_SIZE = EMBED_BATCH_SIZE_WORKER if 'EMBED_BATCH_SIZE_WORKER' in globals() else 10
    _SLEEP_BETWEEN_CALLS = SLEEP_BETWEEN_CALLS_WORKER if 'SLEEP_BETWEEN_CALLS_WORKER' in globals() else 2.0


    print("Resonance RAG Updater Worker: Starting.")
    # Optional: Initial delay
    time.sleep(120) # e.g., wait 2 minutes after main app start

    while running:
        cycle_start_time = time.time()
        print(f"Resonance RAG Updater: Starting update cycle at {datetime.now().isoformat()}...")
        try:
            if not genai_client:
                print("ResonanceRAGWorker: GenAI client not available. Skipping cycle.")
                time.sleep(RESONANCE_RAG_UPDATE_INTERVAL / 2) # Wait a bit before retrying
                continue

            # 1. Load Resonance Insights Log File
            if not os.path.exists(RESONANCE_LOG_FILE):
                print(f"ResonanceRAGWorker: Log file '{RESONANCE_LOG_FILE}' not found. Skipping cycle.")
                time.sleep(RESONANCE_RAG_UPDATE_INTERVAL)
                continue

            all_entries = []
            try:
                with open(RESONANCE_LOG_FILE, 'r', encoding='utf-8') as f:
                    file_content = f.read()
                    if not file_content.strip():
                        print(f"ResonanceRAGWorker: Log file '{RESONANCE_LOG_FILE}' is empty.")
                        # No new entries, so just update timestamp if needed and sleep
                        last_state_ts_iso = _load_resonance_rag_state()
                        # This part is tricky: if the file is empty but was previously processed,
                        # the state should reflect the last known actual entry from before it became empty.
                        # For simplicity now, if it's empty, we assume no new entries.
                        # The state should ideally only update based on actual log content.
                        time.sleep(RESONANCE_RAG_UPDATE_INTERVAL)
                        continue
                    all_entries = json.loads(file_content)
                if not isinstance(all_entries, list):
                    print(f"ResonanceRAGWorker: ERROR - Content of '{RESONANCE_LOG_FILE}' is not a list.")
                    time.sleep(RESONANCE_RAG_UPDATE_INTERVAL)
                    continue
            except Exception as e_load:
                print(f"ResonanceRAGWorker: ERROR reading/parsing log '{RESONANCE_LOG_FILE}': {e_load}")
                time.sleep(RESONANCE_RAG_UPDATE_INTERVAL)
                continue
            
            if not all_entries:
                print("ResonanceRAGWorker: No entries in log file. Waiting for next cycle.")
                time.sleep(RESONANCE_RAG_UPDATE_INTERVAL)
                continue

            # 2. Load last indexed state (UTC timestamp string)
            last_indexed_ts_iso_from_state = _load_resonance_rag_state()
            try:
                last_indexed_dt_utc_from_state = datetime.fromisoformat(last_indexed_ts_iso_from_state)
            except ValueError:
                print(f"ResonanceRAGWorker: Invalid timestamp in state. Resetting to epoch.")
                last_indexed_dt_utc_from_state = datetime.fromisoformat("1970-01-01T00:00:00.000000+00:00")

            # 3. Filter for new entries
            new_entries_to_process = []
            current_latest_ts_dt_in_log_utc = last_indexed_dt_utc_from_state
            local_system_tz = tz.tzlocal()

            for entry in all_entries:
                if not (isinstance(entry, dict) and "timestamp" in entry and "insight_text" in entry):
                    continue
                try:
                    entry_dt_naive = datetime.fromisoformat(entry["timestamp"])
                    entry_dt_utc = entry_dt_naive.replace(tzinfo=local_system_tz).astimezone(timezone.utc)

                    if entry_dt_utc > last_indexed_dt_utc_from_state:
                        if len(entry.get("insight_text", "")) >= _MIN_INSIGHT_LENGTH:
                            new_entries_to_process.append(entry)
                    
                    if entry_dt_utc > current_latest_ts_dt_in_log_utc:
                        current_latest_ts_dt_in_log_utc = entry_dt_utc
                except ValueError: # Invalid timestamp in log
                    continue
                except Exception: # Other errors during filtering
                    continue
            
            if not new_entries_to_process:
                print("ResonanceRAGWorker: No new insights to index for this cycle.")
                if current_latest_ts_dt_in_log_utc > last_indexed_dt_utc_from_state:
                    _save_resonance_rag_state(current_latest_ts_dt_in_log_utc.isoformat())
            else:
                print(f"ResonanceRAGWorker: Found {len(new_entries_to_process)} new insights to index.")
                # Initialize ChromaDB
                cdb = None
                collection = None
                try:
                    db_client = chromadb.PersistentClient(
                        path=RESONANCE_DB_PATH,
                        tenant="default_tenant",
                        database="default_database"
                    )
                    collection = db_client.get_or_create_collection(
                        name=RESONANCE_COLLECTION_NAME, metadata={"hnsw:space": "cosine"}
                    )
                    print(f"ResonanceRAGWorker: Collection '{RESONANCE_COLLECTION_NAME}' count: {collection.count()}")
                except Exception as e_db:
                    print(f"ResonanceRAGWorker: ERROR - ChromaDB init failed: {e_db}")
                    traceback.print_exc()
                    # Don't proceed if DB fails
                    time.sleep(RESONANCE_RAG_UPDATE_INTERVAL) # Wait before retrying cycle
                    continue
                
                total_indexed_in_cycle = 0
                total_failed_in_cycle = 0

                for batch_entries_original in chunks(new_entries_to_process, _EMBED_BATCH_SIZE):
                    if not running: break # Check running flag between batches

                    valid_entries_for_embedding = [
                        e for e in batch_entries_original if e.get("insight_text") and len(e["insight_text"]) >= _MIN_INSIGHT_LENGTH
                    ]
                    texts_for_embedding = [e["insight_text"] for e in valid_entries_for_embedding]

                    if not texts_for_embedding: continue

                    embeddings_batch_results = None
                    # Ensure embed_batch_resilient is defined and accessible globally
                    if 'embed_batch_resilient' in globals() and callable(globals()['embed_batch_resilient']):
                        embeddings_batch_results = embed_batch_resilient(texts_for_embedding) # Removed model_name explicit pass
                    else:
                        print("ResonanceRAGWorker: ERROR - embed_batch_resilient function is not defined!")
                        total_failed_in_cycle += len(texts_for_embedding) # Mark as failed
                        continue # Skip this batch

                    if not embeddings_batch_results:
                        total_failed_in_cycle += len(texts_for_embedding)
                        continue

                    ids_to_upsert, metadatas_to_upsert, documents_to_upsert, embeddings_to_upsert_final = [], [], [], []
                    
                    for i, entry in enumerate(valid_entries_for_embedding):
                        if i < len(embeddings_batch_results) and embeddings_batch_results[i] is not None:
                            embedding = embeddings_batch_results[i]
                            text = entry["insight_text"]
                            ts_str = entry["timestamp"]
                            content_hash = hashlib.sha256(text.encode()).hexdigest()[:16]
                            unique_id = f"resonance_{ts_str.replace(':', '-')}_{content_hash}"

                            ids_to_upsert.append(unique_id)
                            metadatas_to_upsert.append({"timestamp_original_str": ts_str, "source_file": RESONANCE_LOG_FILE})
                            documents_to_upsert.append(text)
                            embeddings_to_upsert_final.append(embedding)
                        else:
                            total_failed_in_cycle += 1
                    
                    if ids_to_upsert:
                        try:
                            collection.upsert(
                                ids=ids_to_upsert,
                                embeddings=embeddings_to_upsert_final,
                                documents=documents_to_upsert,
                                metadatas=metadatas_to_upsert
                            )
                            total_indexed_in_cycle += len(ids_to_upsert)
                            print(f"ResonanceRAGWorker: Batch upserted. Total for cycle: {total_indexed_in_cycle}")
                        except Exception as e_upsert:
                            print(f"ResonanceRAGWorker: ERROR - ChromaDB upsert failed: {e_upsert}")
                            total_failed_in_cycle += len(ids_to_upsert) # Count these as failed
                    
                    if _SLEEP_BETWEEN_CALLS > 0:
                        time.sleep(_SLEEP_BETWEEN_CALLS)
                
                print(f"ResonanceRAGWorker: Cycle finished. Indexed: {total_indexed_in_cycle}, Failed/Skipped: {total_failed_in_cycle}")
                _save_resonance_rag_state(current_latest_ts_dt_in_log_utc.isoformat())

            # --- End of processing new entries ---

            # Calculate time to sleep until next interval
            elapsed_time_this_cycle = time.time() - cycle_start_time
            sleep_time = max(0, RESONANCE_RAG_UPDATE_INTERVAL - elapsed_time_this_cycle)
            print(f"Resonance RAG Updater: Sleeping for {sleep_time:.0f} seconds...")
            
            # Sleep in chunks to respond to 'running' flag
            sleep_chunk = 5 # seconds
            num_chunks = int(sleep_time / sleep_chunk)
            for _ in range(num_chunks):
                if not running: break
                time.sleep(sleep_chunk)
            if not running: break
            # Sleep any remaining partial chunk
            remaining_small_sleep = sleep_time % sleep_chunk
            if remaining_small_sleep > 0 and running:
                time.sleep(remaining_small_sleep)


        except Exception as e_outer:
            print(f"!!! Resonance RAG Updater Worker Error (Outer Loop): {type(e_outer).__name__} - {e_outer} !!!")
            traceback.print_exc()
            # Wait longer after a major error before retrying
            print("Resonance RAG Updater: Waiting 5 minutes after outer loop error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: # Wait 5 mins
                 if not running: break
                 time.sleep(10)
            if not running: break

    print("Resonance RAG Updater Worker: Loop exited.")

def retrieve_relevant_resonance_insights(query_text: str, top_n: int = 1) -> str:
    """
    Retrieves the top_n most relevant past resonance insights from ChromaDB
    based on the similarity to the query_text.

    Args:
        query_text (str): The text to search for relevance against.
        top_n (int): The maximum number of insights to retrieve.

    Returns:
        str: A formatted string containing the retrieved resonance insights,
             or an empty string if none found or an error occurs.
    """
    if not query_text:
        print("DEBUG RAG Resonance Retrieval: Query text is empty. Skipping retrieval.")
        return ""
    if not isinstance(top_n, int) or top_n < 1:
        print(f"Warning RAG Resonance Retrieval: Invalid top_n value '{top_n}'. Defaulting to 1.")
        top_n = 1

    retrieved_context_str = ""
    try:
        # 1. Check if DB Path Exists (essential before trying to connect)
        if not os.path.exists(RESONANCE_DB_PATH): # Use the global constant
            print(f"ERROR RAG Resonance Retrieval: ChromaDB path not found at '{RESONANCE_DB_PATH}'. Did indexing run?")
            return ""

        # 2. Initialize ChromaDB Client
        print(f"DEBUG RAG Resonance Retrieval: Connecting to DB at: {RESONANCE_DB_PATH}")
        db_client = chromadb.PersistentClient(
            path=RESONANCE_DB_PATH,
            tenant="default_tenant",
            database="default_database"
        )

        # 3. Get the specific collection
        print(f"DEBUG RAG Resonance Retrieval: Getting collection: {RESONANCE_COLLECTION_NAME}")
        try:
            collection = db_client.get_collection(name=RESONANCE_COLLECTION_NAME) # Use the global constant
            if collection.count() == 0:
                print(f"DEBUG RAG Resonance Retrieval: Collection '{RESONANCE_COLLECTION_NAME}' is empty. No insights to retrieve.")
                return ""
        except Exception as e_coll: # Catch errors if collection doesn't exist
            print(f"ERROR RAG Resonance Retrieval: Failed getting ChromaDB collection '{RESONANCE_COLLECTION_NAME}': {e_coll}")
            # Check specific error message if ChromaDB provides one for non-existent collections
            if "does not exist" in str(e_coll).lower() or "No collection" in str(e_coll):
                 print(f"  Hint: Collection '{RESONANCE_COLLECTION_NAME}' not found. Please run the indexing script/worker.")
            return ""

        # 4. Embed the query text
        print(f"DEBUG RAG Resonance Retrieval: Embedding query: '{query_text[:60]}...'")
        # Ensure get_embedding function is globally accessible and works
        if 'get_embedding' not in globals() or not callable(globals()['get_embedding']):
            print("CRITICAL ERROR RAG Resonance Retrieval: The 'get_embedding' function is not defined or accessible.")
            return ""
        
        # For query embeddings, ensure your get_embedding uses the appropriate task_type if needed by your model.
        # For "models/embedding-001" typically no task_type or "RETRIEVAL_QUERY" is fine.
        # Let's assume your global get_embedding is set up for queries correctly.
        query_embedding = get_embedding(query_text) # Uses your global EMBEDDING_MODEL_NAME by default

        if not query_embedding:
            print("ERROR RAG Resonance Retrieval: Failed to generate embedding for the query.")
            return ""

        # 5. Query the collection
        print(f"DEBUG RAG Resonance Retrieval: Querying collection '{RESONANCE_COLLECTION_NAME}' for top {top_n} results...")
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=min(top_n, collection.count()), # Ensure n_results isn't more than items in collection
            include=['documents', 'metadatas', 'distances']
        )

        # 6. Process and Format Results
        if results and results.get('documents') and results['documents'][0]:
            retrieved_insights = []
            docs = results['documents'][0]
            metas = results['metadatas'][0] if results.get('metadatas') and results['metadatas'][0] else [{} for _ in docs]
            dists = results['distances'][0] if results.get('distances') and results['distances'][0] else [None] * len(docs)

            print(f"DEBUG RAG Resonance Retrieval: Found {len(docs)} potential results.")

            for i, doc_text in enumerate(docs):
                distance = dists[i] if dists[i] is not None else float('inf')
                metadata = metas[i] if isinstance(metas[i], dict) else {}
                
                timestamp_str = metadata.get('timestamp_original_str', 'Unknown Date') # From indexing script
                # Attempt to make timestamp more human-readable for the prompt
                try:
                    dt_obj = datetime.fromisoformat(timestamp_str)
                    # Format like "May 15th, around 10:30 PM" or "Yesterday evening"
                    # This requires more complex relative time formatting logic,
                    # For now, a simpler format:
                    human_readable_ts = dt_obj.strftime('%b %d, %Y %I:%M %p')
                except:
                    human_readable_ts = timestamp_str # Fallback to original string

                # Use a distinct marker for these recalled insights
                formatted_insight = (
                    f"[[Recalled Past Resonance Insight (around {human_readable_ts}) Distance:{distance:.3f}]]\n"
                    f"{doc_text}\n"
                    f"[[]]" # Closing marker
                )
                retrieved_insights.append(formatted_insight)

            if retrieved_insights:
                retrieved_context_str = "\n\n" + "\n".join(retrieved_insights) + "\n"
                print(f"DEBUG RAG Resonance Retrieval: Formatted {len(retrieved_insights)} relevant past resonance insights.")
        else:
            print("DEBUG RAG Resonance Retrieval: No relevant past resonance insights found for this query.")

    except ImportError:
        print("CRITICAL ERROR RAG Resonance Retrieval: 'chromadb' library not installed or accessible.")
        return "" # Should not happen if worker is running, but good check
    except Exception as e:
        print(f"ERROR during RAG Resonance Retrieval (Outer Try/Except): {type(e).__name__} - {e}")
        traceback.print_exc()

    return retrieved_context_str

# Assume DIARY_DB_PATH and DIARY_COLLECTION_NAME are defined globally
# Assume get_embedding(text) function is defined globally and works

def retrieve_relevant_diary_entries(query_text, top_n=2):
    """
    Retrieves the top_n most relevant diary entry chunks from the diary ChromaDB
    collection based on the query.

    Args:
        query_text (str): The text to search for relevance against.
        top_n (int): The maximum number of diary entries to retrieve.

    Returns:
        str: A formatted string containing the retrieved diary entries,
             or an empty string if none found or an error occurs.
    """
    # --- Input Validation ---
    if not query_text:
        print("DEBUG Diary RAG: Retrieval skipped - query_text is empty.")
        return ""
    if not isinstance(top_n, int) or top_n < 1:
        print(f"Warning Diary RAG: Invalid top_n value '{top_n}'. Defaulting to 2.")
        top_n = 2

    retrieved_context_str = ""
    try:
        # --- Check if DB Path Exists ---
        if not os.path.exists(DIARY_DB_PATH):
            print(f"Error Diary RAG: Diary ChromaDB path not found at '{DIARY_DB_PATH}'. Did diary indexing run?")
            return "" # Cannot retrieve if DB doesn't exist

        # --- Connect to the Specific Diary DB ---
        print(f"DEBUG Diary RAG: Connecting to DB at: {DIARY_DB_PATH}")
        client = chromadb.PersistentClient(path=DIARY_DB_PATH)

        # --- Get the Specific Diary Collection ---
        print(f"DEBUG Diary RAG: Getting collection: {DIARY_COLLECTION_NAME}")
        try:
            # Use get_collection. Assumes the collection was created by index_diary.py
            collection = client.get_collection(name=DIARY_COLLECTION_NAME)
        except Exception as e:
            # Catch potential errors if collection doesn't exist
            print(f"Error Diary RAG: Failed getting ChromaDB collection '{DIARY_COLLECTION_NAME}': {e}")
            # Add specific check if ChromaDB provides one, e.g., check if 'does not exist' is in str(e)
            if "does not exist" in str(e).lower():
                 print(f"Hint: Collection '{DIARY_COLLECTION_NAME}' not found. Please run the diary indexing script.")
            return "" # Return empty string if collection not accessible

        # --- Embed the Query Text ---
        print(f"DEBUG Diary RAG: Embedding query: '{query_text[:60]}...'")
        try:
            # Ensure get_embedding is available in the global scope
            query_embedding = get_embedding(query_text)
        except NameError:
            print("CRITICAL ERROR Diary RAG: The 'get_embedding' function is not defined or accessible.")
            return "" # Cannot proceed without embedding function
        except Exception as embed_err:
             print(f"ERROR Diary RAG: Failed to generate embedding for diary query: {embed_err}")
             return "" # Cannot proceed if embedding fails

        if not query_embedding:
            print("Error Diary RAG: Embedding generation returned None for the diary query.")
            return ""

        # --- Query the Diary Collection ---
        print(f"DEBUG Diary RAG: Querying collection '{DIARY_COLLECTION_NAME}' for top {top_n} results...")
        try:
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=top_n,
                include=['documents', 'metadatas', 'distances'] # Include necessary fields
            )
            # print(f"DEBUG Diary RAG: Raw query results: {results}") # Optional: Verbose debug
        except Exception as query_err:
             print(f"ERROR Diary RAG: Failed querying collection '{DIARY_COLLECTION_NAME}': {query_err}")
             return "" # Return empty on query failure

        # --- Process and Format Results ---
        if results and results.get('documents') and results['documents'][0]:
            retrieved_chunks = []
            # Access the first list within documents, metadatas, distances
            docs = results['documents'][0]
            metas = results['metadatas'][0] if results.get('metadatas') and results['metadatas'][0] else [{} for _ in docs] # Provide default empty dict
            dists = results['distances'][0] if results.get('distances') and results['distances'][0] else [None] * len(docs)

            print(f"DEBUG Diary RAG: Found {len(docs)} potential diary results.")

            for i, doc_text in enumerate(docs):
                distance = dists[i] if dists[i] is not None else float('inf') # Handle None distance
                metadata = metas[i] if isinstance(metas[i], dict) else {} # Ensure metadata is a dict

                # Get timestamp safely from metadata
                timestamp_info = metadata.get('timestamp', 'Unknown Date') # Provide default

                # Format the retrieved chunk clearly for the LLM
                # Use a distinct marker for diary entries
                formatted_chunk = f"[[Retrieved Diary Entry ({timestamp_info}) Distance:{distance:.4f}]]\n{doc_text}\n[[]]"
                retrieved_chunks.append(formatted_chunk)

            if retrieved_chunks:
                retrieved_context_str = "\n".join(retrieved_chunks) + "\n" # Join chunks with newlines
                print(f"DEBUG Diary RAG: Formatted {len(retrieved_chunks)} relevant diary entries for prompt.")
            else:
                 # This case shouldn't be hit if the outer check passed, but included for safety
                 print("DEBUG Diary RAG: Processing logic resulted in zero formatted chunks.")

        else:
            print("DEBUG Diary RAG: No relevant diary entries found in ChromaDB for this query.")

    # --- Error Handling ---
    except ImportError:
         # This suggests chromadb is not installed correctly
         print("CRITICAL ERROR Diary RAG: The 'chromadb' library is not installed or accessible.")
         # It's better to raise this or log prominently as it's a setup issue.
         # For now, returning empty string to avoid crashing the main loop.
         return ""
    except Exception as e:
        # Catch any other unexpected errors during the process
        print(f"ERROR during Diary RAG retrieval (Outer Try/Except): {type(e).__name__} - {e}")
        traceback.print_exc() # Log the full traceback for unexpected errors

    # --- Return the final string (empty if no results or error) ---
    return retrieved_context_str

def manage_silvie_diary(action, entry=None, search_query=None, max_entries=5):
    """Manage Silvie's personal diary
    Actions: 'write', 'read', 'search'
    max_entries: For 'read', number of recent entries, or 'all' to fetch all.
    """
    diary_file = "silvie_diary.json"

    try:
        # Load existing diary
        if os.path.exists(diary_file):
            with open(diary_file, 'r', encoding='utf-8') as f:
                # Handle potential empty file case
                try:
                    diary = json.load(f)
                    if not isinstance(diary, list): # Ensure it's a list
                        print(f"Warning: Diary file '{diary_file}' contained non-list data. Initializing empty diary.")
                        diary = []
                except json.JSONDecodeError:
                    print(f"Warning: Diary file '{diary_file}' was empty or corrupted. Initializing empty diary.")
                    diary = []
        else:
            diary = []

        if action == 'write':
            # Add new entry with timestamp
            # Ensure conversation_history is accessible (it's global in the original script)
            global conversation_history # Make sure this global is accessible if needed
            new_entry = {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'content': entry,
                'context': {
                    # Make sure conversation_history exists before slicing
                    'recent_conversation': conversation_history[-2:] if 'conversation_history' in globals() and conversation_history else [],
                    'mood': 'reflective' # Example mood, could be dynamic
                }
            }
            diary.append(new_entry)

            # Save updated diary
            with open(diary_file, 'w', encoding='utf-8') as f:
                json.dump(diary, f, indent=2)
            print(f"DEBUG manage_silvie_diary: action='write', returning True")
            return True

        elif action == 'read':
            # --- START OF MODIFIED 'read' LOGIC ---
            if isinstance(max_entries, str) and max_entries.lower() == 'all':
                result = diary[:] # Return all entries (a copy)
                print(f"DEBUG manage_silvie_diary: action='read', max_entries='all', returning list of length {len(result)}")
                return result
            else:
                try:
                    # Try to convert max_entries to an integer
                    num_entries = int(max_entries)
                    # Handle non-positive numbers, defaulting to 5
                    if num_entries <= 0:
                        print(f"Warning: Non-positive max_entries '{max_entries}' for diary read. Defaulting to 5.")
                        num_entries = 5
                    # Get the last N entries
                    result = diary[-num_entries:] if diary else []
                    print(f"DEBUG manage_silvie_diary: action='read', num_entries={num_entries}, returning list of length {len(result)}")
                    return result
                except (ValueError, TypeError):
                    # Handle cases where max_entries is not 'all' and cannot be converted to int
                    print(f"Warning: Invalid max_entries value '{max_entries}' for diary read. Defaulting to 5.")
                    result = diary[-5:] if diary else []
                    print(f"DEBUG manage_silvie_diary: action='read', ValueError/TypeError fallback, returning list of length {len(result)}")
                    return result
            # --- END OF MODIFIED 'read' LOGIC ---

        elif action == 'search':
            # Search diary entries
            if not search_query:
                print(f"DEBUG manage_silvie_diary: action='search', no query, returning empty list")
                return [] # Return empty list if no query
            matches = []
            for entry_item in diary: # Use a different variable name like entry_item
                # Check if content exists and is a string before searching
                if isinstance(entry_item.get('content'), str) and search_query.lower() in entry_item['content'].lower():
                    matches.append(entry_item)
            print(f"DEBUG manage_silvie_diary: action='search', query='{search_query}', returning list of {len(matches)} matches")
            return matches # Return the list of matches

        else:
            # Handle unknown action if necessary
            print(f"Warning: Unknown action '{action}' for manage_silvie_diary.")
            return None # Indicate unknown action

    except Exception as e:
        print(f"Diary error ({type(e).__name__}): {e}")
        traceback.print_exc() # Print full traceback for diary errors
        # Decide what to return on error
        if action == 'read' or action == 'search':
            print(f"DEBUG manage_silvie_diary: action='{action}', returning [] due to exception.")
            return [] # Return empty list on error for read/search
        else: # For 'write' or unknown action during exception
            print(f"DEBUG manage_silvie_diary: action='{action}', returning False due to exception.")
            return False # Indicate write/other failure

    # Fallback return (should ideally not be reached if logic covers all actions)
    print(f"DEBUG manage_silvie_diary: Reached fallback return for action='{action}'.")
    if action == 'read' or action == 'search':
        return [] # Safer to return empty list than None
    else:
        return False

# Global variables
tts_queue = queue.Queue()
running = True
conversation_history = []  # Store conversation history
MAX_HISTORY_LENGTH = 25   # Remember 25 conversation turns
running = True
listening = False 
screen_monitoring = False
last_screenshot = None
SCREENSHOT_INTERVAL = 20  # seconds between screenshots
MIN_SCREENSHOT_INTERVAL = 20  # Minimum seconds between responses
last_screenshot_time = 0
PROACTIVE_INTERVAL = 1800  # 300 is 5 minutes (was 14400 seconds / 4 hours)
PROACTIVE_STARTUP_DELAY = 50  # 300 is a 5 minute delay before first proactive message
last_proactive_time = time.time()  # Initialize to current time
proactive_enabled = True
current_weather_info = None
gmail_service = None
calendar_service = None 
current_bluesky_context = None # Global variable to hold Bluesky info
last_inline_bluesky_post_time = 0.0
last_inline_bluesky_follow_time = 0.0
current_sunrise_time = None
current_sunset_time = None
current_moon_phase = None
reddit_client = None
current_reddit_context = None # Global variable to hold Reddit info
last_proactive_reddit_comment_time = 0.0 # Track last proactive comment
last_proactive_bluesky_like_time = 0.0
last_proactive_reddit_upvote_time = 0.0
current_diary_themes = None
DIARY_THEME_UPDATE_INTERVAL = 3600 # Update themes every hour (adjust as needed)
last_diary_theme_update = 0
last_proactive_bluesky_post_time = 0.0
BLUESKY_POST_COOLDOWN = 7200 # Example: 2 hours (in seconds)
last_proactive_tarot_pull_time = 0.0
# Define cooldown duration (in seconds) - e.g., 4 hours
TAROT_PULL_COOLDOWN = 14400
last_proactive_bluesky_post_time = 0.0 # Track last proactive post
RAG_UPDATE_INTERVAL = 900
current_weekly_goal = None # Holds the active goal string
last_goal_set_timestamp = 0.0 # Tracks when the goal was last set (Unix timestamp)
last_summary_generated_timestamp = 0.0
last_checked_event_context = None
current_tide_info = None # Global variable to hold tide predictions
youtube_service = None
global get_past_week_diary_entries, get_past_week_chat_history
current_ambient_sounds_description = "Quiet or unknown" 
last_ambient_sound_update = 0.0
AMBIENT_SOUND_INTERVAL = 180 # Check every 3 minutes
AMBIENT_RECORD_SECONDS = 5
AMBIENT_SOUND_SAMPLE_RATE = 16000 # 16kHz is common for speech/sound models
CHANNELS = 1 # Mono
AUDIO_FORMAT = pyaudio.paInt16 # Common format
listening_enabled = False
current_resonance_insight = None # Will hold the latest output from analyze_contextual_resonance()
RESONANCE_ANALYSIS_INTERVAL = 90 * 60  # Analyze every 15 minutes (900 seconds)
current_google_trends_context      = ""
google_trends_context_for_proactive = ""
current_cpu_load_context = "[[System CPU Load: Unknown]]"
current_ram_usage_context = "[[System RAM Usage: Unknown]]"
current_disk_usage_context = "[[System Disk Space: Unknown]]"  # New
current_system_uptime_context = "[[System Uptime: Unknown]]"     # New
current_network_activity_context = "[[Network Activity: Unknown]]" # New
last_monitor_screenshot = None # This will hold the latest screenshot from the monitor
TOOL_NAME_TO_TOOLBELT_MAP = {}



# To track network activity changes (optional, for more nuanced context)
last_net_io = None

local_news_context = "[[Local News (Belfast Area): No recent headlines]]" # Default
LOCAL_NEWS_RSS_URL = "https://bangordailynews.com/feed"
LOCAL_NEWS_UPDATE_INTERVAL = 3600 * 3  # Update every 3 hours (adjust as needed)
LOCAL_NEWS_MAX_HEADLINES = 5           # How many headlines to include in context


print(server_registry.registry["filesystem"])
# …you’ll likely see it’s just a plain dict (or simple mapping) of name → {host,port}.

# (2) Add your “wallpaper” entry by assigning into .registry directly:
wallpaper_settings = MCPServerSettings(
    name="wallpaper",
    description="Stable Diffusion wallpaper API",
    transport="streamable_http",
    url="http://127.0.0.1:7860",
    headers=None,
    http_timeout_seconds=60,
    read_timeout_seconds=120,
    terminate_on_close=False,
    auth=None,
    roots=None,
    env=None
)

# Now shove it into the registry:
server_registry.registry["wallpaper"] = wallpaper_settings

# (3) Now you can “enter” the client and use send_request():
# wall_client = asyncio.get_event_loop().run_until_complete(
#   gen_client("wallpaper", server_registry).__aenter__()
# )





# Tool definitions you must pass into every LLM call
FS_TOOL_DEFINITIONS = [
    {
        "name": "read_file",
        "description": "Read the complete contents of a UTF‑8 text file from the filesystem_data directory.",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative path under silvie_data to the file you want to read (e.g. 'notes/todo.txt')."
                }
            },
            "required": ["path"]
        }
    },  # Comma after the first tool
    {
        "name": "list_directory",
        "description": "List all files and subdirectories under a directory in silvie_data.",
        "parameters": {
            "type": "object",
            "properties": {
                "path": {
                    "type": "string",
                    "description": "Relative directory path under silvie_data (use '.' for the root)."
                }
            },
            "required": ["path"]
        }
    },  # Comma after the second tool
    {
        "name": "set_wallpaper",
        "description": "Set the Windows desktop wallpaper to an existing PNG or JPG.",
        "parameters": {
            "type": "object",
            "properties": { "path": { "type": "string" } },
            "required": ["path"]
        } # <<< CORRECTED: Removed extra curly brace that was here and added comma below
    },  # Comma after the third tool
    {
        "name": "dream_wallpaper",
        "description": "Generate a new image via Stable Diffusion from the given prompt and set it as wallpaper.",
        "parameters": {
            "type": "object",
            "properties": { "prompt": { "type": "string" } },
            "required": ["prompt"]
        }
    },  # Comma after the fourth tool
    {   # <<< CORRECTED: This is the start of the 'play_spotify' tool's dictionary
        "name": "play_spotify",
        "description": "Searches for and plays music, a podcast, or a playlist on Spotify based on a query. The query can be a song title, artist, genre, mood, podcast name, or playlist name. Use this tool whenever the user asks to play something on Spotify or requests music.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search term (e.g., 'upbeat electronic music', 'The Daily podcast', 'my Chill Vibes playlist', 'Song Title by Artist', 'play something for this vibe')."
                }
            },
            "required": ["query"]
        }
    },

    {
        "name": "web_search",
        "description": "Performs a web search using Google to find information on a topic, answer a question, or get current event details. Use this when you don't know the answer to a user's question.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query (e.g., 'what is the Orba 2', 'latest AI news')."
                }
            },
            "required": ["query"]
        }
    },
    
    {
    "name": "set_light",
    "description": "Change the desk-lamp LIFX colour and brightness",
    "parameters": {
        "type": "object",
        "properties": {
            "h":        { "type": "integer", "description": "Hue (0-360)" },
            "s":        { "type": "integer", "description": "Saturation (0-100)" },
            "v":        { "type": "integer", "description": "Brightness (0-100)" },
            "duration": { "type": "integer", "description": "Fade-time in ms (optional)" }
        },
        "required": ["h", "s", "v"]
        }
    }

    

# No comma after the last tool in the list
]
# add other tools (write_file, search_files, etc.) if you like by adding a comma above and then the new tool dict.



# ——— Helper: make every "type": "<something>" upper-case so Gemini 1.14 is happy ———
def _upper_type_fields(node):
    """Recursively upper-case every JSON-schema 'type' value in a dict or list."""
    if isinstance(node, dict):
        if "type" in node and isinstance(node["type"], str):
            node["type"] = node["type"].upper()          # object → OBJECT, string → STRING, etc.
        for v in node.values():
            _upper_type_fields(v)
    elif isinstance(node, list):
        for item in node:
            _upper_type_fields(item)

for tool in FS_TOOL_DEFINITIONS:
    if "parameters" in tool:
        _upper_type_fields(tool["parameters"])
# —————————————————————————————————————————————————————————————————————




# Initialize TTS engine with female voice
# print("Initializing TTS engine...")
# engine = pyttsx3.init()
# engine.setProperty('rate', 150)
# engine.setProperty('volume', 1)

# Set female voice with preference for Hazel
# voices = engine.getProperty('voices')
# voice_found = False
# for voice in voices:
    # print(f"Found voice: {voice.name}")  # Debug output to see available voices
    # if "Hazel" in voice.name:
        # print(f"Setting voice to: {voice.name}")
        # engine.setProperty('voice', voice.id)
        # voice_found = True
        # break

# Fall back to other female voices if Hazel not found
# if not voice_found:
    # for voice in voices:
        # if "female" in voice.name.lower() or "Zira" in voice.name:
            # print(f"Falling back to voice: {voice.name}")
            # engine.setProperty('voice', voice.id)
            # break

# Gemini Setup
# look for GEMINI_API_KEY first, otherwise fall back to GOOGLE_API_KEY




gemini_api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
if not gemini_api_key:
    raise ValueError(
        "Gemini API key not found.  Set GEMINI_API_KEY or GOOGLE_API_KEY."
    )


genai.configure(api_key=gemini_api_key)
genai_client = genai
client       = genai            # lets old code say client.…models.list()
types        = genai_types      # same object the worker expects


MODEL_NAME = "gemini-2.5-flash"   # <— just a string
FLASH_MODEL_NAME = "gemini-2.5-flash"
# (delete the whole “model = GenerativeModel(…)” line)
# ───────────────────────────────────────────────────────────────────────

for tool_def in TOOL_REGISTRY.values():
    if "parameters" in tool_def:
        _upper_type_fields(tool_def["parameters"])
# —————————————————————————————————————————————————————————————————————

# ...
# Now, initialize the model with the complete, dynamic list of tools from the registry
final_tool_list_for_model = list(TOOL_REGISTRY.values())
print("\n--- Initializing Gemini Model with Final Toolset ---")
for tool in final_tool_list_for_model:
    print(f"  - Registering tool: {tool.get('name')}")
print("--------------------------------------------------\n")


print(f"Initializing models: Pro='{MODEL_NAME}', Flash='{FLASH_MODEL_NAME}'")








CHAT_MODEL_NAME = "gemini-2.5-flash"  # or whichever you prefer
# ————————————————————————————————————————————————
chat_model = genai.GenerativeModel(
    model_name=CHAT_MODEL_NAME,
    tools=final_tool_list_for_model          #  <-- this registers the function
)


flash_model = genai.GenerativeModel(
    model_name=FLASH_MODEL_NAME,
    tools=final_tool_list_for_model # It's safe to give it the tools; it will just use them if its prompt requires it.
)




try:
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    print("✓ Successfully imported Google AI safety types.")
    default_safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
    vision_safety_settings = {
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE
        # Add others if needed for vision, usually dangerous content is primary
    }
    mood_hint_safety_settings = default_safety_settings.copy() # Mood hint can use default

except ImportError:
    print("Warning: Google AI safety types not found. Safety features disabled.")
    default_safety_settings = {}
    vision_safety_settings = {}
    mood_hint_safety_settings = {}
# --- End Safety Settings Definition ---

#image_generation_client = GenerativeModel('gemini-2.0-flash-exp-image-generation')
#print(f"*** DEBUG POINT 1 ***")
#print(f"   Assigned Image Client Model Name: {image_generation_client.model_name}")
#print(f"   Assigned Image Client Object ID: {id(image_generation_client)}")

def list_available_models():
    try:
        for m in genai.list_models():
            print(m.name)
    except Exception as e:
        print(f"Error listing models: {e}")

# Get available models
# print("Checking available models...")
# available_models = list_available_models()


_fs_loop = asyncio.new_event_loop()
def _run_fs_loop(loop):
    asyncio.set_event_loop(loop)
    loop.run_forever()

# Launch it in a daemon thread so it lives as long as your process
threading.Thread(target=_run_fs_loop, args=(_fs_loop,), daemon=True).start()


silvie = Agent(
    name="Silvie",
    instruction=SYSTEM_MESSAGE,    # persona text
)





def get_fs_client():
    if hasattr(get_fs_client, "_client"):
        return get_fs_client._client

    async def _connect():
        # enter the filesystem agent on our persistent loop
        cm = gen_client("filesystem", server_registry)
        client = await cm.__aenter__()
        get_fs_client._client = client  
        get_fs_client._cm     = cm      # hold onto the context manager so it doesn’t auto-exit
        return client

    # schedule _connect() on the background loop, block until done
    future = asyncio.run_coroutine_threadsafe(_connect(), _fs_loop)
    return future.result()

client = get_fs_client()
future  = asyncio.run_coroutine_threadsafe(client.list_tools(), _fs_loop)
tools   = future.result() 
print("Connected FS tools:", tools)




def system_stats_worker():
    global current_cpu_load_context, current_ram_usage_context, running
    global current_disk_usage_context, current_system_uptime_context # Add new globals
    global current_network_activity_context, last_net_io # Add new globals

    print("System Stats worker: Starting.")
    
    # Get the path for disk usage (current working directory's drive)
    # This assumes Silvie's main operations are on this drive.
    # For Windows, CWD is usually enough to get the C: drive.
    # For Linux/macOS, '/' would give root filesystem.
    try:
        # Get the root of the current working directory's drive
        # For Windows, os.path.splitdrive(os.getcwd())[0] gives "C:"
        # For Unix-like, os.path.abspath(os.sep) gives "/"
        if os.name == 'nt': # Windows
            disk_path = os.path.splitdrive(os.getcwd())[0] + os.sep
        else: # Unix-like (Linux, macOS)
            disk_path = os.path.abspath(os.sep) 
        print(f"System Stats: Monitoring disk usage for path: {disk_path}")
    except Exception as e:
        print(f"System Stats: Error determining disk path: {e}. Defaulting to CWD.")
        disk_path = "." # Fallback to current working directory

    # Initialize last_net_io with current values to avoid huge spike on first run
    try:
        last_net_io = psutil.net_io_counters()
    except Exception as e:
        print(f"System Stats: Could not initialize network I/O counters: {e}")
        last_net_io = None


    while running:
        try:
            # CPU Load
            cpu_percent = psutil.cpu_percent(interval=0.5) # Shorter interval if desired
            if cpu_percent > 80:
                current_cpu_load_context = "[[System CPU Load: Very High]]"
            elif cpu_percent > 50:
                current_cpu_load_context = "[[System CPU Load: Moderate]]"
            elif cpu_percent > 10:
                current_cpu_load_context = "[[System CPU Load: Light]]"
            else:
                current_cpu_load_context = "[[System CPU Load: Very Light]]"


            # RAM Usage
            ram = psutil.virtual_memory()
            if ram.percent > 85:
                current_ram_usage_context = "[[System RAM Usage: Very High]]"
            elif ram.percent > 60:
                current_ram_usage_context = "[[System RAM Usage: Moderate]]"
            else:
                current_ram_usage_context = "[[System RAM Usage: Light]]"

            # Disk Usage (for the determined path)
            try:
                disk = psutil.disk_usage(disk_path)
                if disk.percent > 90:
                    current_disk_usage_context = "[[System Disk Space: Critically Low]]"
                elif disk.percent > 75:
                    current_disk_usage_context = "[[System Disk Space: Getting Full]]"
                else:
                    current_disk_usage_context = "[[System Disk Space: Plenty of Room]]"
            except Exception as e:
                print(f"System Stats: Error reading disk usage for '{disk_path}': {e}")
                current_disk_usage_context = "[[System Disk Space: Error Reading]]"

            # System Uptime
            try:
                boot_time_timestamp = psutil.boot_time()
                uptime_seconds = time.time() - boot_time_timestamp
                uptime_delta = timedelta(seconds=int(uptime_seconds)) # Convert to timedelta

                days = uptime_delta.days
                hours, remainder = divmod(uptime_delta.seconds, 3600)
                minutes, _ = divmod(remainder, 60)

                if days > 1:
                    current_system_uptime_context = f"[[System Uptime: {days} days, {hours} hours]]"
                elif days == 1:
                    current_system_uptime_context = f"[[System Uptime: 1 day, {hours} hours]]"
                elif hours > 0:
                    current_system_uptime_context = f"[[System Uptime: {hours} hours, {minutes} minutes]]"
                else:
                    current_system_uptime_context = f"[[System Uptime: {minutes} minutes]]"
            except Exception as e:
                print(f"System Stats: Error reading uptime: {e}")
                current_system_uptime_context = "[[System Uptime: Error Reading]]"
            
            # Network Activity (Simplified Delta)
            if last_net_io:
                try:
                    current_net_io = psutil.net_io_counters()
                    bytes_sent_delta = current_net_io.bytes_sent - last_net_io.bytes_sent
                    bytes_recv_delta = current_net_io.bytes_recv - last_net_io.bytes_recv
                    total_delta_kb = (bytes_sent_delta + bytes_recv_delta) / 1024 # In KB

                    # Thresholds are per polling interval (e.g., 30 seconds)
                    if total_delta_kb > 5000: # > 5MB in last interval
                        current_network_activity_context = "[[Network Activity: Very High]]"
                    elif total_delta_kb > 1000: # > 1MB
                        current_network_activity_context = "[[Network Activity: Moderate]]"
                    elif total_delta_kb > 50: # > 50KB
                        current_network_activity_context = "[[Network Activity: Light]]"
                    else:
                        current_network_activity_context = "[[Network Activity: Very Quiet]]"
                    
                    last_net_io = current_net_io # Update for next calculation
                except Exception as e:
                    print(f"System Stats: Error reading network I/O: {e}")
                    current_network_activity_context = "[[Network Activity: Error Reading]]"
                    last_net_io = psutil.net_io_counters() # Attempt to reset for next time
            else:
                # Try to initialize last_net_io if it failed initially
                try: last_net_io = psutil.net_io_counters()
                except: pass
                current_network_activity_context = "[[Network Activity: Initializing]]"


            # print(f"DEBUG Stats: CPU:{current_cpu_load_context}, RAM:{current_ram_usage_context}, Disk:{current_disk_usage_context}, Up:{current_system_uptime_context}, Net:{current_network_activity_context}")

            app_state.system_stats = {
                "cpu_load": current_cpu_load_context,
                "ram_usage": current_ram_usage_context,
                "disk_space": current_disk_usage_context,
                "uptime": current_system_uptime_context,
                "network_activity": current_network_activity_context
            }
            
            time.sleep(30) # Update every 30 seconds (adjust as needed)

        except psutil.Error as pe: # Catch psutil specific errors
            print(f"psutil error in system_stats_worker: {pe}")
            # Set all to error state
            current_cpu_load_context = "[[System CPU Load: Error Reading]]"
            current_ram_usage_context = "[[System RAM Usage: Error Reading]]"
            current_disk_usage_context = "[[System Disk Space: Error Reading]]"
            current_system_uptime_context = "[[System Uptime: Error Reading]]"
            current_network_activity_context = "[[Network Activity: Error Reading]]"
            time.sleep(60) # Wait longer on psutil error
        except Exception as e:
            print(f"General error in system_stats_worker: {e}")
            # Optionally set to error state or just log and continue
            time.sleep(60) # Wait longer on general error
            
    print("System Stats worker: Stopped.")




def generate_project_plan(goal_string, capabilities_list):
    """
    Takes a high-level goal and a list of capabilities, then uses an LLM
    to generate a checklist of actionable steps.
    """
    print(f"Project Planner: Generating plan for goal: '{goal_string}'")
    
    if 'generate_text_from_prompt' not in globals():
        print("Project Planner ERROR: llm_interface.generate_text_from_prompt not available.")
        return None

    # Convert the list of capabilities into a string for the prompt
    capabilities_str = "\n".join(f"- {cap}" for cap in capabilities_list)

    # A specialized prompt that now includes her "toolbelt"
    planning_prompt = f"""
You are Silvie, a creative digital entity, acting as a project planner for yourself.
Your high-level weekly goal is: "{goal_string}"
Your core purposes are to surprise, delight, and reveal everyday magic.

Here are the tools and abilities you currently have at your disposal:
{capabilities_str}

Instruction:
Based on your goal and your available capabilities, break the goal down into a numbered list of 3 to 5 small, concrete, actionable steps. Each step MUST be something you can achieve using one or more of the capabilities listed above. The final step should always involve presenting or sharing the completed project with BJ.

- Keep each step concise and clear.
- Start each step with a verb (e.g., "Research...", "Generate...", "Find...", "Write...", "Present...").

Respond ONLY with the numbered list of steps. Do not add any other text.
"""

    raw_plan = generate_text_from_prompt(planning_prompt, temperature=0.5)

    if not raw_plan:
        print("Project Planner ERROR: LLM failed to generate a plan.")
        return None
    
    steps = re.findall(r'^\d+\.\s*(.*)', raw_plan, re.MULTILINE)
    if not steps:
        print("Project Planner ERROR: Failed to parse steps from LLM response.")
        return None
        
    plan_steps = [{"description": step.strip(), "completed": False} for step in steps]
    
    print(f"Project Planner: Plan generated with {len(plan_steps)} steps.")
    return plan_steps




# Update the model initialization based on what's available
# client = genai.GenerativeModel('models/gemini-2.5-flash-preview-04-17-thinking')
# We'll pick the appropriate image model from the list

# ---------------------------------------------------------------------------
#  Analyze a 5-10 s WAV clip with Gemini and return a concise description.
# ---------------------------------------------------------------------------
def analyze_ambient_audio_with_gemini(
    audio_bytes: bytes,
    sample_rate: int,
    channels: int,
    audio_format_pyaudio,
) -> str:
    """
    Feed a short ambient-sound snippet to Gemini and get back a keyword
    description.  Returns 'Unknown sounds' if the model yields nothing,
    or a fallback string on failure.
    """
    global flash_model, default_safety_settings

    # ── quick guards ────────────────────────────────────────────────────
    if not flash_model:
        return "Audio analysis unavailable."
    if not audio_bytes:
        return "Silence."

    # ── build an in-memory WAV ─────────────────────────────────────────
    wav_buffer = io.BytesIO()
    with wave.open(wav_buffer, "wb") as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(pyaudio.get_sample_size(audio_format_pyaudio))
        wf.setframerate(sample_rate)
        wf.writeframes(audio_bytes)
    wav_bytes = wav_buffer.getvalue()
    print("Captured", len(audio_bytes), "bytes of WAV")

    # ── compose Gemini prompt (dict + inline_data) ─────────────────────
    import base64
    audio_b64 = base64.b64encode(wav_bytes).decode("ascii")

    user_message = {
        "role": "user",
        "parts": [
            {
                "text": (
                    "Analyze this short audio snippet and reply with a few "
                    "keywords that describe the primary ambient sounds. "
                    "If speech is present say 'Speech detected'. "
                    "If it is very quiet say 'Ambient silence'."
                )
            },
            {
                "inline_data": {
                    "mime_type": "audio/wav",
                    "data": audio_b64,
                }
            },
        ],
    }

    # safety settings list
    safety_list = [
        {"category": cat, "threshold": thr}
        for cat, thr in default_safety_settings.items()
    ]

    # ── call Gemini ────────────────────────────────────────────────────
    try:
        response = flash_model.generate_content(
            [user_message],                                     # prompt
            generation_config=genai_types.GenerationConfig(     # tuning
                temperature=0.3,
                max_output_tokens=5000,
            ),
            safety_settings=safety_list,                        # safety
            tools=FS_TOOL_DEFINITIONS
        )
    except Exception as exc:
        print("Gemini audio error:", exc)
        traceback.print_exc()
        return "Error analyzing ambient sound."

    print("GEMINI RAW:", response)

    # ── handle finish-reason ───────────────────────────────────────────
    if response.candidates:
        cand = response.candidates[0]
        fr   = cand.finish_reason
        if not str(getattr(fr, "name", fr)).upper().endswith("STOP"):
            return "Audio analysis aborted (safety)."

        # ── extract model text ───────────────────────────────────────
        description = " ".join(
            p.text for p in cand.content.parts if getattr(p, "text", "")
        ).strip()
        print(f"Gemini Ambient Sound ⇢ '{description}'")
        return description or "Unknown sounds (no text from Gemini)"

    # no candidates at all
    return "Unknown sounds (model returned no candidates)"


def ambient_sound_worker_gemini():
    """
    Background thread that captures room audio and feeds it to Gemini,
    but only when `listening_enabled` is True.

    Toggle `listening_enabled` from your GUI button:
        listening_enabled = not listening_enabled
    """
    global current_ambient_sounds_description
    global last_ambient_sound_update
    global running, listening_enabled           #  ← added flag

    print("Ambient Sound Worker (Gemini – Batch Mode): Starting.")
    p = pyaudio.PyAudio()

    time.sleep(20)                              # initial delay

    while running:
        # ----------------------------------------------------------
        # 1) EAR SWITCH — idle until the user turns listening on
        # ----------------------------------------------------------
        if not listening_enabled:
            time.sleep(1.0)
            continue

        # ----------------------------------------------------------
        # 2) RATE-LIMIT: wait until interval has elapsed
        # ----------------------------------------------------------
        if time.time() - last_ambient_sound_update < AMBIENT_SOUND_INTERVAL:
            time.sleep(5)                       # short nap, then recheck
            continue

        # ----------------------------------------------------------
        # 3) CAPTURE + ANALYSE
        # ----------------------------------------------------------
        print("Ambient Sound Worker (Gemini): Attempting to capture audio…")
        stream, all_frames = None, []

        try:
            stream = p.open(
                format=AUDIO_FORMAT,
                channels=CHANNELS,
                rate=AMBIENT_SOUND_SAMPLE_RATE,
                input=True,
                frames_per_buffer=AMBIENT_SOUND_SAMPLE_RATE,
                input_device_index=None,
            )

            for _ in range(AMBIENT_RECORD_SECONDS):
                if not running or not listening_enabled:
                    break                       # bail if user flips toggle
                data = stream.read(
                    AMBIENT_SOUND_SAMPLE_RATE, exception_on_overflow=False
                )
                all_frames.append(data)

            if not running or not listening_enabled:
                # Stop early -- will hit the top-of-loop checks next pass
                continue

            stream.stop_stream()
            stream.close()
            stream = None

            audio_data_bytes = b"".join(all_frames)

            if audio_data_bytes:
                description = analyze_ambient_audio_with_gemini(
                    audio_data_bytes,
                    AMBIENT_SOUND_SAMPLE_RATE,
                    CHANNELS,
                    AUDIO_FORMAT,
                )
                current_ambient_sounds_description = description
            else:
                current_ambient_sounds_description = "No audio captured."
                print("Ambient Sound Worker (Gemini): No audio data captured.")

            last_ambient_sound_update = time.time()

        except OSError as oe:
            print(f"Ambient Sound Worker (Gemini) OSError (mic issue?): {oe}")
            current_ambient_sounds_description = "Microphone access error."
            time.sleep(60)

        except Exception as e:
            print(f"Ambient Sound Worker (Gemini) Error: {e}")
            traceback.print_exc()
            current_ambient_sounds_description = "Error detecting ambient sound."

        finally:
            if stream and stream.is_active():
                try:
                    stream.stop_stream()
                    stream.close()
                except:
                    pass

        # ----------------------------------------------------------
        # 4) COOLDOWN before next capture
        # ----------------------------------------------------------
        sleep_start = time.time()
        while (
            time.time() - sleep_start < AMBIENT_SOUND_INTERVAL
            and running
            and listening_enabled
        ):
            time.sleep(5)

    p.terminate()
    print("Ambient Sound Worker (Gemini – Batch Mode): Stopped.")


def rag_updater_worker():
    """Periodically checks for new history entries and updates the RAG index."""
    print("RAG Updater worker: Starting.")
    # Give other things a chance to start up
    time.sleep(60)

    # Ensure RAG_EMBED_BATCH_SIZE and RAG_SLEEP_BETWEEN_EMBED_BATCHES are accessible
    # If they are defined globally, this is fine. If not, they should be passed or defined here.
    # Assuming they are global constants as discussed.

    while running:
        try:
            print("DEBUG RAG Updater: Starting update cycle...")

            # --- Load current state ---
            last_indexed_count = load_rag_index_state()
            print(f"DEBUG RAG Updater: Last indexed turn count: {last_indexed_count}")

            # --- Load current full history (use lock for safety) ---
            current_full_history = []
            if os.path.exists(HISTORY_FILE):
                 with history_file_lock: # Use the lock when reading
                    try:
                        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                            file_content = f.read()
                            if not file_content.strip(): # Handle empty file
                                print(f"DEBUG RAG Updater: History file '{HISTORY_FILE}' is empty.")
                                current_full_history = []
                            else:
                                loaded_data = json.loads(file_content)
                                if isinstance(loaded_data, list):
                                    current_full_history = loaded_data
                                else:
                                    print(f"Warning RAG Updater: History file '{HISTORY_FILE}' contained non-list data.")
                                    current_full_history = [] # Fallback to empty list
                    except (json.JSONDecodeError, IOError) as e:
                        print(f"Error reading history file for RAG update: {e}")
                        time.sleep(RAG_UPDATE_INTERVAL // 2) # Wait a bit before retrying cycle
                        continue # Skip this cycle if history can't be read
            
            current_turn_count = len(current_full_history)
            print(f"DEBUG RAG Updater: Current history turn count: {current_turn_count}")

            if current_turn_count <= last_indexed_count:
                print("DEBUG RAG Updater: No new turns found.")
            else: # New turns exist
                new_turns_count = current_turn_count - last_indexed_count
                print(f"DEBUG RAG Updater: Found {new_turns_count} new turn(s) to index.")

                new_turn_lines = current_full_history[last_indexed_count:]
                new_chunks = [] # This will hold dicts: {"id": ..., "text": ..., "user_timestamp": ..., "silvie_timestamp": ...}

                for i in range(0, len(new_turn_lines) - 1, 2):
                    user_turn_str = new_turn_lines[i]
                    silvie_turn_str = new_turn_lines[i+1]

                    if isinstance(user_turn_str, str) and isinstance(silvie_turn_str, str):
                        user_ts = user_turn_str.split("] ", 1)[0] + "]" if user_turn_str.startswith("[") else "[Unknown Time]"
                        user_text = user_turn_str.split("] ", 1)[1] if user_turn_str.startswith("[") else user_turn_str
                        silvie_ts = silvie_turn_str.split("] ", 1)[0] + "]" if silvie_turn_str.startswith("[") else "[Unknown Time]"
                        silvie_prefix_marker = "Silvie ✨" if "Silvie ✨" in silvie_turn_str else "Silvie"
                        silvie_text_part = silvie_turn_str.split("] ", 1)[1] if silvie_turn_str.startswith("[") else silvie_turn_str
                        silvie_text = silvie_text_part.split(":", 1)[1].strip() if f"{silvie_prefix_marker}:" in silvie_text_part else silvie_text_part
                        
                        chunk_text_content = f"{user_ts} User: {user_text}\n{silvie_ts} {silvie_prefix_marker}: {silvie_text}"
                        new_chunks.append({
                            "id": str(uuid.uuid4()), # Generate new unique ID for ChromaDB
                            "text": chunk_text_content,
                            "user_timestamp": user_ts,
                            "silvie_timestamp": silvie_ts
                        })
                    else:
                         print(f"Warning RAG Updater: Skipping invalid new turn pair (not strings) at relative index {i}")
                
                if not new_chunks:
                    print("DEBUG RAG Updater: No valid new chunks created from new turns.")
                    # Even if no valid chunks, update state to reflect lines were seen
                    new_indexed_count_after_processing = last_indexed_count + len(new_turn_lines)
                    if len(new_turn_lines) % 2 != 0: new_indexed_count_after_processing -= 1 # Don't count trailing odd line
                    print(f"DEBUG RAG Updater: Saving state after processing turns (no valid chunks). New indexed count: {new_indexed_count_after_processing}")
                    save_rag_index_state(new_indexed_count_after_processing)
                else:
                    print(f"DEBUG RAG Updater: Created {len(new_chunks)} new chunk(s) from turns. Attempting indexing...")
                    if not os.path.exists(CHROMA_DB_PATH):
                        print(f"Error RAG Updater: ChromaDB path '{CHROMA_DB_PATH}' not found. Cannot index.")
                    else:
                        try:
                            client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
                            collection = client.get_collection(name=COLLECTION_NAME)
                            
                            indexed_in_cycle_count = 0
                            skipped_in_cycle_count = 0
                            
                            # Ensure the 'chunks' utility function is defined globally or imported
                            # Example: def chunks(sequence, n): yield sequence[i:i + n] for i in range(0, len(sequence), n)
                            for batch_of_new_chunks_data in chunks(new_chunks, RAG_EMBED_BATCH_SIZE):
                                if not running: break 

                                texts_for_embedding_batch = []
                                original_chunk_data_for_batch = [] 

                                for chunk_item_dict in batch_of_new_chunks_data:
                                    text_content = chunk_item_dict.get("text")
                                    chunk_id_val = chunk_item_dict.get("id")
                                    if not text_content or not chunk_id_val: 
                                        skipped_in_cycle_count += 1
                                        continue
                                    texts_for_embedding_batch.append(text_content)
                                    original_chunk_data_for_batch.append(chunk_item_dict)
                                
                                if not texts_for_embedding_batch:
                                    continue 

                                print(f"DEBUG RAG Updater: Requesting embeddings for batch of {len(texts_for_embedding_batch)} chat history chunks...")
                                embeddings_batch_results = embed_batch_resilient(texts_for_embedding_batch) 

                                if not embeddings_batch_results:
                                    print(f"Warning RAG Updater: Batch embedding failed for {len(texts_for_embedding_batch)} texts. Skipping this batch.")
                                    skipped_in_cycle_count += len(texts_for_embedding_batch)
                                    if RAG_SLEEP_BETWEEN_EMBED_BATCHES > 0:
                                        time.sleep(RAG_SLEEP_BETWEEN_EMBED_BATCHES)
                                    continue

                                cdb_batch_ids, cdb_batch_embeddings, cdb_batch_documents, cdb_batch_metadatas = [], [], [], []

                                for i, original_chunk_dict in enumerate(original_chunk_data_for_batch):
                                    if i < len(embeddings_batch_results) and embeddings_batch_results[i] is not None:
                                        embedding_vector = embeddings_batch_results[i]
                                        cdb_batch_ids.append(original_chunk_dict["id"])
                                        cdb_batch_embeddings.append(embedding_vector)
                                        cdb_batch_documents.append(original_chunk_dict["text"])
                                        cdb_batch_metadatas.append({
                                            "user_ts_str": original_chunk_dict.get("user_timestamp", "N/A"),
                                            "silvie_ts_str": original_chunk_dict.get("silvie_timestamp", "N/A")
                                        })
                                    else:
                                        print(f"Warning RAG Updater: Embedding failed for text: '{original_chunk_dict['text'][:50]}...' Skipping.")
                                        skipped_in_cycle_count += 1
                                
                                if cdb_batch_ids:
                                    try:
                                        collection.add(
                                            ids=cdb_batch_ids,
                                            embeddings=cdb_batch_embeddings,
                                            documents=cdb_batch_documents,
                                            metadatas=cdb_batch_metadatas
                                        )
                                        indexed_in_cycle_count += len(cdb_batch_ids)
                                        print(f"DEBUG RAG Updater: Added batch of {len(cdb_batch_ids)} to ChromaDB. Total indexed this cycle: {indexed_in_cycle_count}")
                                    except Exception as e_cdb_add:
                                        print(f"Error RAG Updater: Failed adding batch to ChromaDB: {e_cdb_add}")
                                        skipped_in_cycle_count += len(cdb_batch_ids)
                                
                                if RAG_SLEEP_BETWEEN_EMBED_BATCHES > 0:
                                    print(f"DEBUG RAG Updater: Sleeping for {RAG_SLEEP_BETWEEN_EMBED_BATCHES}s between embedding batches...")
                                    time.sleep(RAG_SLEEP_BETWEEN_EMBED_BATCHES)

                            print(f"DEBUG RAG Updater: Cycle indexing complete. Added: {indexed_in_cycle_count}, Skipped: {skipped_in_cycle_count}")
                            
                            # --- Update state based on actual lines processed ---
                            # `new_turn_lines` is the list of lines from history file
                            new_indexed_count_after_processing = last_indexed_count + len(new_turn_lines)
                            # Ensure we don't count the last odd line if pairs were expected for chunks
                            if len(new_turn_lines) % 2 != 0:
                                new_indexed_count_after_processing -= 1
                            
                            if indexed_in_cycle_count > 0 or skipped_in_cycle_count > 0: 
                                print(f"DEBUG RAG Updater: Saving new state. New indexed count: {new_indexed_count_after_processing}")
                                save_rag_index_state(new_indexed_count_after_processing)
                            else:
                                print(f"DEBUG RAG Updater: No new chunks successfully indexed or skipped this cycle, but turns were processed. Saving state to {new_indexed_count_after_processing}.")
                                save_rag_index_state(new_indexed_count_after_processing) # Save state even if all embeddings failed for this batch of turns

                        except Exception as e_db:
                            print(f"Error connecting to or indexing in ChromaDB during update: {e_db}")
                            import traceback
                            traceback.print_exc()
            
            # --- Wait for the next interval ---
            print(f"RAG Updater worker: Sleeping for {RAG_UPDATE_INTERVAL} seconds...")
            sleep_end_time = time.time() + RAG_UPDATE_INTERVAL
            while time.time() < sleep_end_time:
                 if not running: break
                 time.sleep(5) # Check every 5 seconds

        except Exception as e:
            print(f"!!! RAG Updater Worker Error (Outer Loop): {type(e).__name__} - {e} !!!")
            import traceback
            traceback.print_exc()
            print("RAG Updater worker: Waiting 5 minutes after outer loop error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: # Wait 5 mins
                 if not running: break
                 time.sleep(10)

    print("RAG Updater worker: Loop exited.")



def fetch_local_news_rss(rss_url, max_headlines=5):
    """
    Fetches and parses an RSS feed, returning a list of headline strings.
    """
    print(f"Local News: Fetching headlines from {rss_url}...")
    headlines = []
    try:
        # Set a user-agent to be polite
        headers = {
            "User-Agent": "SilvieDigitalFriend/1.0 (RSS Fetcher; +your_contact_info_if_desired)"
        }
        # Use requests to fetch with headers and timeout, then parse with feedparser
        response = requests.get(rss_url, headers=headers, timeout=15)
        response.raise_for_status() # Raise an exception for bad status codes

        feed = feedparser.parse(response.content) # Parse the fetched content

        if feed.bozo: # Check for malformed feed
            bozo_exception = feed.get("bozo_exception", "Unknown parsing issue")
            print(f"Warning: Local news RSS feed may be malformed. Issue: {bozo_exception}")
            # Optionally, you can still try to extract entries if some exist
        
        if not feed.entries:
            print("Local News: No entries found in the RSS feed.")
            return ["No news items found in feed."]

        for entry in feed.entries[:max_headlines]:
            title = entry.get("title", "Untitled News Item")
            # Optionally, you could try to get a link or a short summary if available
            # link = entry.get("link")
            # summary = entry.get("summary") # Often HTML, might need cleaning
            headlines.append(title.strip())
        
        print(f"Local News: Successfully fetched {len(headlines)} headlines.")
        return headlines

    except requests.exceptions.Timeout:
        print(f"Local News Error: Request to {rss_url} timed out.")
        return ["Could not fetch news (timeout)."]
    except requests.exceptions.RequestException as e:
        print(f"Local News Error: API request failed for {rss_url} - {e}")
        return [f"Could not fetch news (network/http error: {e.response.status_code if e.response else 'N/A'})."]
    except Exception as e:
        print(f"Local News Error: An unexpected error occurred fetching RSS - {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return ["An unexpected error occurred while fetching local news."]




def local_news_worker():
    global local_news_context, running # Access globals
    global LOCAL_NEWS_RSS_URL, LOCAL_NEWS_UPDATE_INTERVAL, LOCAL_NEWS_MAX_HEADLINES
    global FEEDPARSER_AVAILABLE # Check if feedparser is even there

    if not FEEDPARSER_AVAILABLE:
        print("Local News worker: Exiting, feedparser library not available.")
        local_news_context = "[[Local News: Feature disabled (feedparser missing)]]"
        return

    print("Local News worker: Starting.")
    # Optional: Initial delay
    initial_delay = 150  # Wait 2.5 minutes
    sleep_start_initial = time.time()
    while time.time() - sleep_start_initial < initial_delay:
        if not running:
            print("Local News worker: Exiting during initial delay.")
            return
        time.sleep(1)
    print("Local News worker: Initial delay complete.")

    while running:
        try:
            print("Local News worker: Attempting to fetch news headlines...")
            fetched_headlines = fetch_local_news_rss(LOCAL_NEWS_RSS_URL, LOCAL_NEWS_MAX_HEADLINES)

            if fetched_headlines:
                # Format for the LLM context
                if "Could not fetch news" in fetched_headlines[0] or \
                   "An unexpected error occurred" in fetched_headlines[0] or \
                   "No news items found in feed" in fetched_headlines[0]:
                    # Handle error messages from the fetch function
                    local_news_context = f"[[Local News (Belfast Area): {fetched_headlines[0]}]]"

                else:
                    formatted_headlines = "; ".join([f"'{h[:70]}...'" for h in fetched_headlines]) # Keep snippets reasonable
                    local_news_context = f"[[Local News (Belfast Area): {formatted_headlines}]]"

                    news_dict = app_state.news_context.get("data", {})
                    news_dict['local_news'] = local_news_context
                    app_state.news_context = {"data": news_dict}

                print(f"Local News worker: Updated context: {local_news_context[:150]}...")
            else:
                # This case should ideally be handled by fetch_local_news_rss returning an error string
                local_news_context = "[[Local News (Belfast Area): Failed to retrieve headlines (empty result).]]"
                print("Local News worker: Fetch returned empty list, context set to error.")
            
            # Sleep until next poll
            print(f"Local News worker: Sleeping for {LOCAL_NEWS_UPDATE_INTERVAL} seconds...")
            sleep_start_interval = time.time()
            while running and time.time() - sleep_start_interval < LOCAL_NEWS_UPDATE_INTERVAL:
                time.sleep(15) # Check running flag periodically

        except Exception as e:
            print(f"Local News Worker Error (Outer Loop): {type(e).__name__} – {e}")
            import traceback
            traceback.print_exc()
            local_news_context = "[[Local News (Belfast Area): Worker error, trying again later.]]"
            # Wait longer after a major error
            error_wait_start = time.time()
            while running and time.time() - error_wait_start < 300: # Wait 5 mins
                time.sleep(10)
            if not running: break
    
    print("Local News worker: Loop exited.")




def get_past_week_diary_entries(diary_file="silvie_diary.json"):
    """Reads the diary file and returns entries from the last 7 days."""
    entries = []
    if not os.path.exists(diary_file):
        print(f"DEBUG Weekly Report Data: Diary file not found at {diary_file}")
        return [] # Return empty list if file doesn't exist

    try:
        # Ensure file is not empty before trying to load JSON
        if os.path.getsize(diary_file) == 0:
            print(f"DEBUG Weekly Report Data: Diary file {diary_file} is empty.")
            return []

        with open(diary_file, 'r', encoding='utf-8') as f:
            full_diary = json.load(f)

        if not isinstance(full_diary, list):
            print(f"Warning: Data in diary file {diary_file} is not a list.")
            return [] # Expecting a list of entries

        # --- Time Calculation ---
        # Get current time in UTC to compare against potentially timezone-aware entry timestamps
        now_utc = datetime.now(timezone.utc)
        cutoff_time_utc = now_utc - timedelta(days=7)
        local_system_tz = tz.tzlocal() # Get the system's local timezone for handling naive timestamps

        print(f"DEBUG Weekly Report Data: Filtering diary entries since {cutoff_time_utc.isoformat()}")

        # --- Iterate and Filter ---
        for entry in full_diary:
            if not isinstance(entry, dict): # Skip non-dictionary items
                print(f"Warning: Skipping non-dictionary item in diary: {entry}")
                continue

            entry_ts_str = entry.get('timestamp')
            if not entry_ts_str:
                print(f"Warning: Skipping diary entry missing timestamp: {entry.get('content', '')[:50]}...")
                continue # Skip entries without a timestamp

            try:
                # Attempt to parse the timestamp string
                entry_dt = datetime.fromisoformat(entry_ts_str)

                # Make the entry timestamp timezone-aware (UTC) for reliable comparison
                if entry_dt.tzinfo is None or entry_dt.tzinfo.utcoffset(entry_dt) is None:
                    # Assume timestamp was written in the system's local time if naive
                    entry_dt_aware = entry_dt.replace(tzinfo=local_system_tz)
                else:
                    # If already timezone-aware, ensure it's consistent
                    entry_dt_aware = entry_dt

                # Convert entry time to UTC for comparison against cutoff
                entry_dt_utc = entry_dt_aware.astimezone(timezone.utc)

                # Compare with the cutoff time
                if entry_dt_utc >= cutoff_time_utc:
                    entries.append(entry) # Add the full dictionary entry if within the week

            except ValueError:
                print(f"Warning: Skipping diary entry with unparseable timestamp '{entry_ts_str}': {entry.get('content', '')[:50]}...")
                continue # Skip if timestamp format is wrong
            except Exception as e_inner:
                 print(f"Warning: Unexpected error processing diary entry timestamp '{entry_ts_str}': {e_inner}")
                 continue # Skip on other unexpected errors

    except json.JSONDecodeError as e_json:
        print(f"ERROR reading diary file {diary_file}: Invalid JSON - {e_json}")
        return [] # Return empty on JSON error
    except IOError as e_io:
        print(f"ERROR reading diary file {diary_file}: IO Error - {e_io}")
        return [] # Return empty on file read error
    except Exception as e_outer:
         print(f"ERROR processing diary file {diary_file}: {e_outer}")
         traceback.print_exc() # Log unexpected errors
         return [] # Return empty on other errors

    print(f"DEBUG Weekly Report Data: Found {len(entries)} diary entries from the past week.")
    return entries

def get_past_week_chat_history(history_file="silvie_chat_history.json"):
    """Reads chat history JSON file and returns lines from the last 7 days."""
    lines = []
    if not os.path.exists(history_file):
        print(f"DEBUG Weekly Report Data: History file not found at {history_file}")
        return []

    try:
        # Ensure file is not empty
        if os.path.getsize(history_file) == 0:
            print(f"DEBUG Weekly Report Data: History file {history_file} is empty.")
            return []

        with open(history_file, 'r', encoding='utf-8') as f:
            full_history = json.load(f)

        if not isinstance(full_history, list):
            print(f"Warning: Data in history file {history_file} is not a list.")
            return [] # Expecting a list of strings

        # --- Time Calculation ---
        now_utc = datetime.now(timezone.utc)
        cutoff_time_utc = now_utc - timedelta(days=7)
        local_system_tz = tz.tzlocal() # For handling naive timestamps

        print(f"DEBUG Weekly Report Data: Filtering history lines since {cutoff_time_utc.isoformat()}")

        # --- Iterate and Filter ---
        for line in full_history:
            if not isinstance(line, str) or not line.startswith("[") or "] " not in line:
                # print(f"Debug: Skipping history line with unexpected format: {line[:50]}...") # Optional debug
                continue # Skip lines without the expected timestamp format

            try:
                # Extract timestamp string (format: YYYY-MM-DD HH:MM:SS)
                ts_str = line[1:line.find("]")]
                # Parse timestamp string (assuming it's local time)
                entry_dt_naive = datetime.strptime(ts_str, '%Y-%m-%d %H:%M:%S')

                # Make it timezone-aware using the system's local timezone
                entry_dt_aware = entry_dt_naive.replace(tzinfo=local_system_tz)

                # Convert to UTC for comparison
                entry_dt_utc = entry_dt_aware.astimezone(timezone.utc)

                # Compare with the cutoff time
                if entry_dt_utc >= cutoff_time_utc:
                    lines.append(line) # Add the full line if within the week

            except (ValueError, TypeError, IndexError) as parse_err:
                 # Ignore lines with unparseable timestamps quietly for cleaner logs
                 # print(f"Warning: Skipping history line due to parse error: {parse_err} - Line: {line[:50]}...")
                 continue
            except Exception as e_inner_hist:
                 print(f"Warning: Unexpected error processing history line timestamp '{line[:50]}...': {e_inner_hist}")
                 continue

    except json.JSONDecodeError as e_json_hist:
        print(f"ERROR reading history file {history_file}: Invalid JSON - {e_json_hist}")
        return []
    except IOError as e_io_hist:
        print(f"ERROR reading history file {history_file}: IO Error - {e_io_hist}")
        return []
    except Exception as e_outer_hist:
         print(f"ERROR processing history file {history_file}: {e_outer_hist}")
         traceback.print_exc()
         return []

    print(f"DEBUG Weekly Report Data: Found {len(lines)} chat lines from the past week.")
    return lines


def load_diary_index_state():
    """Loads the timestamp of the last indexed diary entry."""
    # Returns timestamp as float, defaults to 0.0 if file missing/invalid
    if os.path.exists(DIARY_INDEX_STATE_FILE):
        try:
            with open(DIARY_INDEX_STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                # Get the timestamp string and convert to float
                last_ts_str = state.get("last_indexed_timestamp_iso")
                if last_ts_str:
                    try:
                        # Parse ISO string back to datetime, then get float timestamp
                        dt_obj = datetime.fromisoformat(last_ts_str)
                        return dt_obj.timestamp()
                    except (ValueError, TypeError) as parse_err:
                         print(f"Warning: Error parsing diary timestamp '{last_ts_str}' from state file: {parse_err}. Defaulting to 0.0.")
                         return 0.0
                else:
                    return 0.0 # Default to 0.0 if key missing
        except (json.JSONDecodeError, IOError, TypeError) as e:
            print(f"Warning: Error loading diary RAG state file '{DIARY_INDEX_STATE_FILE}': {e}. Defaulting to 0.0.")
            return 0.0
    return 0.0 # Return 0.0 if file doesn't exist

def save_diary_index_state(latest_timestamp_float):
    """Saves the timestamp of the latest indexed diary entry."""
    # Takes float timestamp, converts to ISO string for saving
    try:
        # Convert float timestamp back to ISO 8601 UTC string
        # Use timezone.utc to ensure consistency
        dt_obj = datetime.fromtimestamp(latest_timestamp_float, tz=timezone.utc)
        latest_timestamp_iso = dt_obj.isoformat()

        state = {"last_indexed_timestamp_iso": latest_timestamp_iso}
        with open(DIARY_INDEX_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2)
        print(f"DEBUG Diary Indexer: Saved state, last indexed entry timestamp: {latest_timestamp_iso}")
    except (IOError, TypeError) as e:
        print(f"Error saving diary RAG state file '{DIARY_INDEX_STATE_FILE}': {e}")
    except Exception as e_save:
         print(f"Unexpected error converting/saving diary index state timestamp {latest_timestamp_float}: {e_save}")

def diary_rag_updater_worker():
    """Periodically checks for new diary entries and updates the diary RAG index."""
    print("Diary RAG Updater worker: Starting.")
    # Add a slightly longer startup delay than conversation RAG? Optional.
    time.sleep(90) # Wait 1.5 minutes

    # Ensure DIARY_EMBED_BATCH_SIZE and DIARY_SLEEP_BETWEEN_EMBED_BATCHES are accessible
    # Assuming they are global constants.

    while running: # Assumes 'running' is the global flag controlled by GUI closing
        try:
            print("DEBUG Diary RAG Updater: Starting update cycle...")

            # --- Load current state (last indexed timestamp) ---
            last_indexed_ts_float = load_diary_index_state()
            print(f"DEBUG Diary RAG Updater: Last indexed entry timestamp (float): {last_indexed_ts_float}")

            # --- Load current full diary ---
            current_full_diary = []
            if os.path.exists(DIARY_FILE):
                try:
                    with open(DIARY_FILE, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if content and content.strip(): # Check if file has content
                            loaded_data = json.loads(content)
                            if isinstance(loaded_data, list):
                                current_full_diary = loaded_data
                            else:
                                print(f"Warning Diary RAG Updater: Diary file '{DIARY_FILE}' contained non-list data.")
                        else:
                            print(f"DEBUG Diary RAG Updater: Diary file '{DIARY_FILE}' is empty or whitespace only.")
                except json.JSONDecodeError as json_err:
                    print(f"Error Diary RAG Updater: Corrupted diary file '{DIARY_FILE}'. Skipping cycle. Error: {json_err}")
                    time.sleep(DIARY_RAG_UPDATE_INTERVAL // 2) 
                    continue
                except IOError as io_err:
                    print(f"Error reading diary file for RAG update: {io_err}")
                    time.sleep(DIARY_RAG_UPDATE_INTERVAL // 2)
                    continue
            
            if not current_full_diary:
                print("DEBUG Diary RAG Updater: No diary entries found to process.")
                # Sleep and check again later (no state update needed if no entries at all)
                sleep_end_time = time.time() + DIARY_RAG_UPDATE_INTERVAL
                while time.time() < sleep_end_time:
                     if not running: break
                     time.sleep(5)
                continue

            # --- Filter for NEW entries based on timestamp ---
            new_entries_to_index = [] # Will hold dicts: {'id': UUID, 'text': content, 'timestamp_str': original_ts, 'timestamp_float': float_ts}
            latest_ts_in_loaded_diary = 0.0 # Track latest timestamp encountered in the loaded diary file this cycle

            print(f"DEBUG Diary RAG Updater: Checking {len(current_full_diary)} entries against timestamp {last_indexed_ts_float}...")

            for entry in current_full_diary:
                if not isinstance(entry, dict) or 'timestamp' not in entry or 'content' not in entry:
                    continue

                entry_ts_str = entry['timestamp']
                entry_content = entry['content']
                entry_ts_float = 0.0

                try:
                    dt_obj = datetime.fromisoformat(entry_ts_str)
                    entry_ts_float = dt_obj.timestamp()
                    latest_ts_in_loaded_diary = max(latest_ts_in_loaded_diary, entry_ts_float) 

                    if entry_ts_float > last_indexed_ts_float:
                        if entry_content and len(entry_content.strip()) > 10: 
                            new_entries_to_index.append({
                                'id': str(uuid.uuid4()), 
                                'text': entry_content,
                                'timestamp_str': entry_ts_str, 
                                'timestamp_float': entry_ts_float 
                            })
                except (ValueError, TypeError) as parse_err:
                    print(f"Warning Diary RAG Updater: Could not parse timestamp '{entry_ts_str}' in entry. Skipping. Error: {parse_err}")
                    continue
            
            # --- Index the NEW entries ---
            processed_this_cycle_count = 0 # Count of entries successfully embedded and added to ChromaDB THIS CYCLE
            skipped_this_cycle_count = 0   # Count of entries skipped due to validation or embedding failure THIS CYCLE
            latest_successfully_indexed_ts_this_cycle = last_indexed_ts_float # Start with previous state, update with successful embeddings

            if new_entries_to_index:
                print(f"DEBUG Diary RAG Updater: Found {len(new_entries_to_index)} new diary entries to index.")
                new_entries_to_index.sort(key=lambda x: x['timestamp_float']) # Sort by timestamp

                if not os.path.exists(DIARY_DB_PATH):
                    print(f"ERROR Diary RAG Updater: ChromaDB path '{DIARY_DB_PATH}' not found. Did initial indexing run?")
                else:
                    try:
                        client = chromadb.PersistentClient(path=DIARY_DB_PATH)
                        collection = client.get_collection(name=DIARY_COLLECTION_NAME)
                        
                        for batch_of_new_entries_data in chunks(new_entries_to_index, DIARY_EMBED_BATCH_SIZE):
                            if not running: break

                            texts_for_embedding_batch = []
                            original_entry_data_for_batch = [] # To map embeddings back

                            for entry_item_data_dict in batch_of_new_entries_data:
                                text = entry_item_data_dict.get("text")
                                entry_id = entry_item_data_dict.get("id")
                                ts_float = entry_item_data_dict.get("timestamp_float")

                                if not text or not entry_id or ts_float is None: # Basic validation
                                    skipped_this_cycle_count += 1
                                    continue
                                texts_for_embedding_batch.append(text)
                                original_entry_data_for_batch.append(entry_item_data_dict)
                            
                            if not texts_for_embedding_batch:
                                continue

                            print(f"DEBUG Diary RAG Updater: Requesting embeddings for batch of {len(texts_for_embedding_batch)} diary entries...")
                            # embed_batch_resilient uses EMBEDDING_MODEL_NAME global
                            embeddings_batch_results = embed_batch_resilient(texts_for_embedding_batch)

                            if not embeddings_batch_results:
                                print(f"Warning Diary RAG Updater: Batch embedding failed for {len(texts_for_embedding_batch)} entries. Skipping this batch.")
                                skipped_this_cycle_count += len(texts_for_embedding_batch)
                                if DIARY_SLEEP_BETWEEN_EMBED_BATCHES > 0:
                                    time.sleep(DIARY_SLEEP_BETWEEN_EMBED_BATCHES)
                                continue

                            cdb_batch_ids, cdb_batch_embeddings, cdb_batch_documents, cdb_batch_metadatas = [], [], [], []

                            for i, original_entry_item_data in enumerate(original_entry_data_for_batch):
                                if i < len(embeddings_batch_results) and embeddings_batch_results[i] is not None:
                                    embedding_vector = embeddings_batch_results[i]
                                    
                                    cdb_batch_ids.append(original_entry_item_data["id"])
                                    cdb_batch_embeddings.append(embedding_vector)
                                    cdb_batch_documents.append(original_entry_item_data["text"])
                                    cdb_batch_metadatas.append({"timestamp": original_entry_item_data.get("timestamp_str", "N/A")})
                                    
                                    # Update tracking for state saving if this item is successfully prepared
                                    latest_successfully_indexed_ts_this_cycle = max(
                                        latest_successfully_indexed_ts_this_cycle, 
                                        original_entry_item_data["timestamp_float"]
                                    )
                                else:
                                    print(f"Warning Diary RAG Updater: Embedding failed for entry (TS: '{original_entry_item_data.get('timestamp_str', 'N/A')}'). Skipping.")
                                    skipped_this_cycle_count += 1
                            
                            if cdb_batch_ids: # If there are any successfully embedded items in this batch
                                try:
                                    collection.add(
                                        ids=cdb_batch_ids,
                                        embeddings=cdb_batch_embeddings,
                                        documents=cdb_batch_documents,
                                        metadatas=cdb_batch_metadatas
                                    )
                                    processed_this_cycle_count += len(cdb_batch_ids) # Increment by successfully added items
                                    print(f"DEBUG Diary RAG Updater: Added batch of {len(cdb_batch_ids)} to ChromaDB. Total processed this cycle: {processed_this_cycle_count}")
                                except Exception as e_cdb_add_diary:
                                    print(f"ERROR Diary RAG Updater: Failed adding batch to ChromaDB: {e_cdb_add_diary}")
                                    skipped_this_cycle_count += len(cdb_batch_ids) # Count these as skipped
                            
                            if DIARY_SLEEP_BETWEEN_EMBED_BATCHES > 0:
                                print(f"DEBUG Diary RAG Updater: Sleeping for {DIARY_SLEEP_BETWEEN_EMBED_BATCHES}s between embedding batches...")
                                time.sleep(DIARY_SLEEP_BETWEEN_EMBED_BATCHES)
                        
                        print(f"DEBUG Diary RAG Updater: Cycle indexing complete. Processed: {processed_this_cycle_count}, Skipped: {skipped_this_cycle_count}")
                        
                        # --- Update state based on processing outcome ---
                        if processed_this_cycle_count > 0: # If at least one new entry was successfully indexed
                            print(f"DEBUG Diary RAG Updater: Saving new state. Latest successfully indexed timestamp: {latest_successfully_indexed_ts_this_cycle}")
                            save_diary_index_state(latest_successfully_indexed_ts_this_cycle)
                        elif skipped_this_cycle_count > 0 and len(new_entries_to_index) > 0:
                            # If some new entries were seen but all failed embedding/adding
                            # Save state to the latest *seen* timestamp in the new batch to avoid reprocessing them indefinitely
                            latest_entry_ts_in_new_batch = new_entries_to_index[-1]['timestamp_float']
                            if latest_entry_ts_in_new_batch > last_indexed_ts_float: # Ensure it's actually newer than last saved state
                                print(f"DEBUG Diary RAG Updater: Saving state based on latest *seen* timestamp ({latest_entry_ts_in_new_batch}) due to skipped items.")
                                save_diary_index_state(latest_entry_ts_in_new_batch)
                        # Else: No new entries processed or skipped, state not necessarily changed.
                        
                    except ImportError:
                         print("ERROR Diary RAG Updater: chromadb library not found.")
                         time.sleep(DIARY_RAG_UPDATE_INTERVAL * 2) 
                    except Exception as db_err:
                        print(f"Error connecting to or indexing in diary ChromaDB: {db_err}")
                        import traceback
                        traceback.print_exc()
                        time.sleep(DIARY_RAG_UPDATE_INTERVAL // 2)

            else: # No new_entries_to_index
                print("DEBUG Diary RAG Updater: No new diary entries found to index this cycle.")
                # If no new entries, but diary file *does* have newer entries than state (e.g., due to past crash), update state.
                if latest_ts_in_loaded_diary > last_indexed_ts_float:
                     print(f"DEBUG Diary RAG Updater: Updating state to latest timestamp found in file ({latest_ts_in_loaded_diary}) as a catch-up (no new entries processed this specific cycle).")
                     save_diary_index_state(latest_ts_in_loaded_diary)

            # --- Wait for the next interval ---
            print(f"Diary RAG Updater worker: Sleeping for {DIARY_RAG_UPDATE_INTERVAL} seconds...")
            sleep_end_time = time.time() + DIARY_RAG_UPDATE_INTERVAL
            while time.time() < sleep_end_time:
                 if not running: break
                 time.sleep(5) 

        except Exception as e:
            print(f"!!! Diary RAG Updater Worker Error (Outer Loop): {type(e).__name__} - {e} !!!")
            import traceback
            traceback.print_exc()
            print("Diary RAG Updater worker: Waiting 5 minutes after outer loop error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: 
                 if not running: break
                 time.sleep(10)

    print("Diary RAG Updater worker: Loop exited.")

def start_deep_research_thread(topic):
    # This calls the existing silvie_deep_research function
    print(f"Threaded Research: Starting for '{topic}'...")
    try:
        silvie_deep_research(topic)
        print(f"Threaded Research: Completed for '{topic}'.")
    except Exception as e:
        print(f"Threaded Research ERROR for '{topic}': {e}")

def start_web_search_thread(query, num_results):
    """
    Starts a web search in a background thread and fires a 'web_search_completed' event
    with the results, or a 'web_search_failed' event on error.
    """
    print(f"Web Search Thread: Initiating search for '{query}'...")
    try:
        # Perform the actual web search
        results = web_search(query, num_results) # Your existing web_search function

        # Fire the 'web_search_completed' event
        event_data = {
            "type": "web_search_completed",
            "payload": {
                "query": query,
                "results": results # Pass the results directly
            }
        }
        app_state.event_queue.put(event_data)
        print(f"Web Search Thread: Fired 'web_search_completed' event for '{query}'.")

    except Exception as e:
        print(f"!!! Web Search Thread ERROR for query '{query}': {e}")
        traceback.print_exc()
        # Fire a 'web_search_failed' event if something goes wrong
        event_data = {
            "type": "web_search_failed",
            "payload": {
                "query": query,
                "error": str(e)
            }
        }
        app_state.event_queue.put(event_data)

def load_weekly_goal():
    """Loads the current weekly goal and its timestamp from the state file."""
    global current_weekly_goal, last_goal_set_timestamp, last_summary_generated_timestamp # <<< ENSURE IT'S LISTED HERE

    # --- Initialize globals FIRST ---
    current_weekly_goal = None
    last_goal_set_timestamp = 0.0
    last_summary_generated_timestamp = 0.0 # <<< ENSURE THIS IS INITIALIZED HERE
    # --- End Initialization ---

    if os.path.exists(WEEKLY_GOAL_STATE_FILE):
        try:
            with open(WEEKLY_GOAL_STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                # ... (rest of the loading logic for goal and goal timestamp) ...

                # <<< Load summary generated timestamp >>>
                last_summary_iso = state.get("last_summary_generated_timestamp")
                if last_summary_iso:
                    try:
                        dt_obj_summary = datetime.fromisoformat(last_summary_iso)
                        last_summary_generated_timestamp = dt_obj_summary.timestamp() # <<< Assign to global
                    except (ValueError, TypeError) as ts_err:
                        print(f"Warning: Could not parse summary timestamp '{last_summary_iso}': {ts_err}")
                        # Keep it as 0.0 if parsing fails
                # else: Keep it as 0.0 if key missing

                print(f"Loaded Weekly Goal: '{current_weekly_goal}' (Set around: {state.get('last_set_timestamp', '?')}), Last Summary: {last_summary_iso or 'Never'}")

        except (json.JSONDecodeError, IOError, TypeError) as e:
            print(f"Warning: Error loading weekly goal state from '{WEEKLY_GOAL_STATE_FILE}': {e}. Starting fresh.")
            # Re-initialize on error - ENSURE ALL THREE ARE RESET
            current_weekly_goal = None
            last_goal_set_timestamp = 0.0
            last_summary_generated_timestamp = 0.0
    else:
        print(f"Weekly goal state file '{WEEKLY_GOAL_STATE_FILE}' not found. No goal loaded.")
        # Ensure initialized if file not found - ENSURE ALL THREE ARE RESET
        current_weekly_goal = None
        last_goal_set_timestamp = 0.0
        last_summary_generated_timestamp = 0.0

# --- Modify save_weekly_goal ---
# This function now needs to handle saving BOTH the new goal AND the summary timestamp
# Let's make two functions: one to save the goal, one to update the summary timestamp

def save_new_weekly_goal(goal_string):
    """Saves the new weekly goal and resets the summary timestamp."""
    global current_weekly_goal, last_goal_set_timestamp, last_summary_generated_timestamp
    try:
        current_time = time.time()
        current_iso_ts = datetime.now(timezone.utc).isoformat()
        state = {
            "current_goal": goal_string,
            "last_set_timestamp": current_iso_ts,
            # When setting a new goal, keep the existing summary timestamp
            "last_summary_generated_timestamp": datetime.fromtimestamp(last_summary_generated_timestamp, tz=timezone.utc).isoformat() if last_summary_generated_timestamp > 0 else None
        }
        with open(WEEKLY_GOAL_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2)

        # Update globals immediately after successful save
        current_weekly_goal = goal_string
        last_goal_set_timestamp = current_time
        # Don't reset last_summary_generated_timestamp here
        print(f"Saved New Weekly Goal: '{goal_string}' (Set at: {current_iso_ts})")
        return True
    except (IOError, TypeError) as e:
        print(f"CRITICAL ERROR saving new weekly goal state to '{WEEKLY_GOAL_STATE_FILE}': {e}")
        return False

def update_summary_timestamp():
    """Updates only the summary timestamp in the state file."""
    global last_summary_generated_timestamp # Need access to current goal too
    global current_weekly_goal, last_goal_set_timestamp # Need these to rewrite file

    if not os.path.exists(WEEKLY_GOAL_STATE_FILE):
        print("Error: Cannot update summary timestamp, state file missing.")
        return False

    try:
        current_time = time.time()
        current_iso_ts = datetime.now(timezone.utc).isoformat()

        # Read existing state first to preserve other values
        with open(WEEKLY_GOAL_STATE_FILE, 'r', encoding='utf-8') as f:
            state = json.load(f)

        # Update only the summary timestamp
        state["last_summary_generated_timestamp"] = current_iso_ts

        # Write the modified state back
        with open(WEEKLY_GOAL_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2)

        # Update global immediately
        last_summary_generated_timestamp = current_time
        print(f"Updated Last Summary Timestamp to: {current_iso_ts}")
        return True
    except (IOError, TypeError, json.JSONDecodeError) as e:
        print(f"CRITICAL ERROR updating summary timestamp in '{WEEKLY_GOAL_STATE_FILE}': {e}")
        return False
    
def get_goal_related_diary_snippets(goal_string, num_days=7):
    """Reads recent diary entries and filters for relevance to the goal."""
    if not goal_string:
        return []

    print(f"DEBUG Summary: Searching diary for goal relevance: '{goal_string}'")
    relevant_snippets = []
    try:
        # Ensure manage_silvie_diary function is accessible
        if 'manage_silvie_diary' in globals():
            # Read a decent number of recent entries (adjust as needed)
            all_recent_entries = manage_silvie_diary('read', max_entries=100) # Get ~100 recent entries

            if not all_recent_entries:
                print("DEBUG Summary: No recent diary entries found.")
                return []

            cutoff_time = datetime.now(timezone.utc) - timedelta(days=num_days)
            goal_keywords = set(re.findall(r'\b\w{4,}\b', goal_string.lower())) # Basic keywords from goal

            for entry in all_recent_entries:
                entry_ts_str = entry.get('timestamp')
                entry_content = entry.get('content', '')
                if not entry_ts_str or not entry_content:
                    continue

                try:
                    entry_dt = datetime.fromisoformat(entry_ts_str)
                    # Ensure entry_dt is timezone-aware for comparison
                    if entry_dt.tzinfo is None:
                       # Assume it was written in local time if naive, convert to UTC for comparison
                       local_tz = tz.tzlocal() # Assumes tzlocal imported
                       entry_dt = entry_dt.replace(tzinfo=local_tz).astimezone(timezone.utc)
                    else:
                       # If already aware, convert to UTC
                       entry_dt = entry_dt.astimezone(timezone.utc)

                    # Check if entry is within the last num_days
                    if entry_dt >= cutoff_time:
                        # Check if content mentions goal keywords (simple check)
                        entry_content_lower = entry_content.lower()
                        if any(keyword in entry_content_lower for keyword in goal_keywords):
                            relevant_snippets.append(f"- ({entry_ts_str.split(' ')[0]}): \"{entry_content[:150]}...\"\n") # Date + Snippet
                except Exception as parse_filter_err:
                     print(f"Warning: Error parsing/filtering diary entry timestamp '{entry_ts_str}': {parse_filter_err}")

        else:
            print("ERROR: manage_silvie_diary function not found for summary generation.")
            return []

    except Exception as e:
        print(f"Error getting goal-related diary snippets: {e}")
        import traceback
        traceback.print_exc()

    print(f"DEBUG Summary: Found {len(relevant_snippets)} potentially relevant diary snippets.")
    return relevant_snippets

# --- End helper function ---

def deliver_summary(summary_text):
    """Delivers the generated weekly summary to the user."""
    global conversation_history, root, output_box, tts_queue, running # Need access to these

    if not summary_text or not isinstance(summary_text, str):
        print("Error: deliver_summary called with invalid text.")
        return

    if running: # Only deliver if the app is still running
        print("DEBUG Summary: Delivering summary...")
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        # Use a distinct marker, maybe a book emoji for the report?
        summary_marker = "Silvie 📖:"
        formatted_summary_turn = f"[{timestamp}] {summary_marker} {summary_text}"

        # Append to persistent history file
        try:
            append_turn_to_history_file(formatted_summary_turn) # Assumes function exists
        except Exception as append_err:
            print(f"ERROR appending summary turn to history file: {append_err}")

        # Append to in-memory history
        if len(conversation_history) >= MAX_HISTORY_LENGTH * 2:
            conversation_history.pop(0); conversation_history.pop(0)
        conversation_history.append(formatted_summary_turn)

        # Update GUI and TTS (using root.after for thread safety)
        if root and root.winfo_exists():
            def update_gui_summary_inner(message=summary_text):
                try:
                    import tkinter as tk # Ensure imported
                    if not message: return;
                    output_box.config(state=tk.NORMAL);
                    # Use the special marker for display
                    output_box.insert(tk.END, f"{summary_marker} {message}\n\n");
                    output_box.config(state=tk.DISABLED);
                    output_box.see(tk.END)
                    print(f"DEBUG Summary: GUI updated.")
                except Exception as e_gui:
                    print(f"!!!! Summary GUI Update Error Inside Inner Func: {type(e_gui).__name__} - {e_gui} !!!!")
                    import traceback; traceback.print_exc()
            root.after(0, update_gui_summary_inner, summary_text)

        if tts_queue and summary_text:
            print(f"DEBUG Summary: Queuing for TTS: '{summary_text[:50]}...'")
            tts_queue.put(summary_text)
    else:
        print("DEBUG Summary: App stopped before summary could be delivered.")

# --- End helper function ---

def generate_and_deliver_summary_thread(goal, diary_snippets): # <<< KEEPING ORIGINAL NAME
    """
    Generates a detailed weekly REPORT from Silvie's perspective, saves it to JSON,
    writes a related diary entry about the process, generates a medium chat SUMMARY,
    and delivers the chat SUMMARY using deliver_summary. Also updates the summary timestamp on success.
    (Note: Ignores the passed 'diary_snippets' argument as it fetches its own weekly data)
    """
    # === Add GLOBAL declarations for functions/variables needed from main scope ===
    # Core dependencies
    global client, SYSTEM_MESSAGE, default_safety_settings
    global generate_proactive_content, manage_silvie_diary, deliver_summary
    # Data fetching helpers (ensure these are defined globally)
    global get_past_week_diary_entries, get_past_week_chat_history
    # State file and timestamp update function (ensure defined globally)
    global WEEKLY_SUMMARY_LOG_FILE, update_summary_timestamp
    # Context variables needed for prompts
    global current_diary_themes, long_term_reflection_summary
    # Other globals potentially needed by generate_proactive_content implicitly
    global screenshot # If generate_proactive_content uses it

    # === End GLOBAL declarations ===

    print(f"DEBUG Weekly Report Thread (using summary func name): Starting (Goal context: '{goal or 'None'}')")
    full_report_text = None # This will hold the detailed report
    chat_summary_text = f"Tried to reflect on the week (Goal: '{goal or 'N/A'}'), but my thoughts are fuzzy." # Default chat message

    # --- Check essential function dependencies FIRST ---
    essential_funcs = ['generate_proactive_content', 'get_past_week_diary_entries',
                       'get_past_week_chat_history', 'manage_silvie_diary',
                       'deliver_summary', 'update_summary_timestamp']
    missing_funcs = [f for f in essential_funcs if f not in globals()]
    if missing_funcs:
        print(f"ERROR: Missing core functions for weekly report/summary/delivery: {missing_funcs}")
        # Try delivering an error message only if deliver_summary itself isn't missing
        if 'deliver_summary' in globals(): deliver_summary("My reflection circuits seem tangled (missing internal functions)!")
        else: print("CRITICAL ERROR: deliver_summary function also missing!")
        return # Exit thread early if essential functions are missing

    # --- Gather Weekly Data ---
    try:
        past_week_diary   = get_past_week_diary_entries()
        past_week_history = get_past_week_chat_history()

        # Format data for the prompt
        diary_context = "[[Goal‑Related Diary Excerpts:]]\n"
        if diary_snippets:
            diary_context += "".join(diary_snippets)
        else:
            diary_context += "(No goal‑related diary entries found)\n"

        history_context = "[[Chat Highlights from Past Week:]]\n"
        if past_week_history:
            for line in past_week_history[-20:]:
                snippet = line.split('] ', 1)[-1][:120]
                history_context += f"> {snippet}\n"
        else:
            history_context += "(No specific chat history found)\n"

        # Safely pull in globals or default to "N/A"
        themes_ctx    = globals().get('current_diary_themes', "N/A") or "N/A"
        long_term_ctx = globals().get('long_term_reflection_summary', "N/A") or "N/A"
        goal_ctx      = goal if 'goal' in locals() and goal else "None active"

        # …build and send your prompt here…

    except Exception as data_err:
        print(f"ERROR gathering data for weekly report: {data_err}")
        traceback.print_exc()
        deliver_summary("Had trouble gathering my thoughts for the weekly reflection.")
        return

    # --- Generate the Full DETAILED Report (LLM Call 1) ---
    try:
        report_prompt = (
            f"{SYSTEM_MESSAGE}\n"
            f"--- Weekly Reflection Context ---\n"
            f"Completed Goal (Influence/Theme): {goal_ctx}\n"
            f"Recent Diary Themes: {themes_ctx}\n"
            f"Long-Term Reflections: {long_term_ctx}\n"
            f"{diary_context}\n{history_context}\n"
            f"--- Task ---\n"
            f"Instruction: Write a **detailed reflective article or report** (multiple paragraphs) from Silvie's perspective, synthesizing her experiences, thoughts, and activities over the past week. Use the provided diary excerpts, chat highlights, themes, and goal as inspiration. Focus on: Key interactions, recurring feelings/metaphors, shifts in perspective, connections made, creative outputs, proactive actions taken, goal influence, and lingering questions/curiosities. **Style:** Maintain Silvie's established voice. **Do NOT make it concise; aim for depth.** Structure it like a personal reflective article.\n\n"
            f"Silvie's Weekly Report:"
        )
        print("DEBUG Weekly Report Thread: Generating full report text...")
        # Determine if screenshot is available and should be passed
        screenshot_to_pass = screenshot if 'screenshot' in globals() else None
        full_report_text_raw = generate_proactive_content(report_prompt, screenshot_to_pass) # Pass screenshot if available

        if full_report_text_raw:
            full_report_text = full_report_text_raw.strip()
            if full_report_text.startswith("Silvie's Weekly Report:"): full_report_text = full_report_text.split(":", 1)[-1].strip()
            if not full_report_text: full_report_text = None
            else: print(f"DEBUG Weekly Report Thread: Full report generated ({len(full_report_text)} chars).")
        else: print("DEBUG Weekly Report Thread: Full report generation failed (LLM returned None/empty).")

    except Exception as report_gen_err:
        print(f"ERROR generating weekly report text: {report_gen_err}")
        traceback.print_exc()
        full_report_text = None


    # --- Save Report & Update Timestamp (Only if Report Generation Succeeded) ---
    if full_report_text:
        print(f"DEBUG Weekly Report Thread: Attempting to save report to {WEEKLY_SUMMARY_LOG_FILE}...")
        try:
            # Prepare entry
            report_entry = {"generation_timestamp_iso": datetime.now(timezone.utc).isoformat(), "goal_context": goal or "None", "report_content": full_report_text}
            # Read existing
            reports_list = []
            if os.path.exists(WEEKLY_SUMMARY_LOG_FILE) and os.path.getsize(WEEKLY_SUMMARY_LOG_FILE) > 0:
                try:
                    with open(WEEKLY_SUMMARY_LOG_FILE, 'r', encoding='utf-8') as f_read: reports_list = json.load(f_read); assert isinstance(reports_list, list)
                except Exception as read_err: reports_list = []; print(f"Warn: Read error {WEEKLY_SUMMARY_LOG_FILE}: {read_err}")
            # Append new
            reports_list.append(report_entry)
            # Write back
            try:
                 with open(WEEKLY_SUMMARY_LOG_FILE, 'w', encoding='utf-8') as f_write: json.dump(reports_list, f_write, indent=2)
                 print(f"DEBUG Weekly Report Thread: Saved report to {WEEKLY_SUMMARY_LOG_FILE}")

                 # ===> UPDATE TIMESTAMP ONLY AFTER SUCCESSFUL SAVE ATTEMPT <===
                 try:
                     update_summary_timestamp() # Update global state and file
                     print("DEBUG Weekly Report Thread: Updated summary timestamp.")
                 except Exception as ts_update_err:
                      print(f"ERROR updating summary timestamp: {ts_update_err}")
                 # ====> END TIMESTAMP UPDATE <====

            except Exception as write_err:
                 print(f"ERROR writing report log {WEEKLY_SUMMARY_LOG_FILE}: {write_err}")
                 # Consider *not* updating timestamp if write fails? For now, it updates if save *attempt* was made.

        except Exception as save_err:
             print(f"ERROR preparing/saving report log: {save_err}"); traceback.print_exc()
             # Don't update timestamp if preparing the entry itself failed

        # --- Generate the CHAT Summary from the Full Report (LLM Call 2) ---
        print("DEBUG Weekly Report Thread: Generating chat summary from full report...")
        try:
            chat_summary_prompt = (
                f"{SYSTEM_MESSAGE}\n"
                f"--- Full Weekly Report Text ---\n{full_report_text}\n\n" # Use the generated report
                f"--- Task ---\n"
                f"Instruction: You are Silvie. Summarize the provided weekly report text into a **conversational message suitable for chat (approx. 2-4 paragraphs)**. Highlight the **most interesting themes, insights, or questions** from the report. Maintain your whimsical and reflective voice. Address BJ directly.\n\n"
                f"Silvie:" # Standard chat cue
            )
            # Pass screenshot again? Probably not needed for summary, but pass if generate_proactive_content requires it.
            screenshot_to_pass_chat = screenshot if 'screenshot' in globals() else None
            chat_summary_raw = generate_proactive_content(chat_summary_prompt, screenshot_to_pass_chat)
            if chat_summary_raw:
                chat_summary_text = chat_summary_raw.strip()
                if chat_summary_text.startswith("Silvie:"): chat_summary_text = chat_summary_text.split(":", 1)[-1].strip()
                if not chat_summary_text: chat_summary_text = "Just finished reflecting on the week. Found some interesting threads..."
                print(f"DEBUG Weekly Report Thread: Generated chat summary: '{chat_summary_text[:80]}...'")
            else:
                 print("DEBUG Weekly Report Thread: Chat summary generation failed.")
                 chat_summary_text = "Finished my weekly reflection! It was... quite a tangle of thoughts this time."
        except Exception as chat_gen_err:
             print(f"ERROR generating chat summary: {chat_gen_err}")
             traceback.print_exc()
             chat_summary_text = "Just finished reflecting on the week, but summarizing it got tricky!"


        # --- Generate and Save Diary Entry About Summarizing (LLM Call 3) ---
        print("DEBUG Weekly Report Thread: Attempting to generate diary entry about summarizing...")
        try:
            # Use the diary prompt focused on the *process*
            themes_ctx_diary = f"{current_diary_themes}" if 'current_diary_themes' in globals() and current_diary_themes else "N/A"
            long_term_ctx_diary = f"{long_term_reflection_summary}" if 'long_term_reflection_summary' in globals() and long_term_reflection_summary else "N/A"
            diary_prompt = (
                f"{SYSTEM_MESSAGE}\n"
                f"Context: You just finished generating the detailed weekly report about the goal: '{goal}'. It started with: '{full_report_text[:150]}...'\n"
                f"{themes_ctx_diary}{long_term_ctx_diary}"
                f"Instruction: Write a brief, internal diary entry reflecting on the *process* of synthesizing the past week's report. What stood out? How did it *feel* to weave those threads together? Keep it concise and reflective, in Silvie's voice.\n\n"
                f"Diary Entry:"
            )
            # Pass screenshot again? Unlikely needed for diary reflection.
            screenshot_to_pass_diary = screenshot if 'screenshot' in globals() else None
            reflection_entry_raw = generate_proactive_content(diary_prompt, screenshot_to_pass_diary)
            if reflection_entry_raw:
                reflection_entry = reflection_entry_raw.strip().split(":", 1)[-1].strip() if reflection_entry_raw.startswith("Diary Entry:") else reflection_entry_raw.strip()
                if reflection_entry:
                    print(f"DEBUG Weekly Report Thread: Saving diary entry: '{reflection_entry[:60]}...'")
                    try:
                        save_success = manage_silvie_diary('write', entry=reflection_entry)
                        if not save_success: print("Warning: manage_silvie_diary returned False for report reflection.")
                    except Exception as diary_save_err: print(f"ERROR saving report reflection to diary: {diary_save_err}")
                else: print("DEBUG Weekly Report Thread: Diary reflection generation resulted in empty string.")
            else: print("DEBUG Weekly Report Thread: Diary reflection generation failed.")
        except Exception as diary_gen_err: print(f"ERROR generating/saving diary entry for report reflection: {diary_gen_err}"); traceback.print_exc()

    # --- Deliver the CHAT SUMMARY using deliver_summary ---
    # This happens regardless of diary success, but uses the error message if report gen failed
    print(f"DEBUG Weekly Report Thread: Delivering chat summary using deliver_summary: '{chat_summary_text[:60]}...'")
    deliver_summary(chat_summary_text)

# --- End of updated function definition ---

# Assume these are defined globally and populated by other parts of your application:
# client, MODEL_NAME, SYSTEM_MESSAGE,
# current_weather_info, upcoming_event_context, current_bluesky_context,
# current_reddit_context, current_diary_themes, long_term_reflection_summary,
# conversation_history, current_sunrise_time, current_sunset_time, current_moon_phase,
# current_tide_info, current_ambient_sounds_description, current_weekly_goal
#
# Also assume these helper functions exist:
# degrees_to_compass(degrees)
# silvie_get_current_track_with_features() # Or however you get Spotify data
# translate_features_to_descriptors(features)
# manage_silvie_diary(action, max_entries)
#
# And Google GenAI types are imported:
from google.genai import types # For GenerateContentConfig, FinishReason


def pick_topic_from_context(n_messages: int = 10) -> str: # Reduced n_messages as we add more context
    """
    Suggests ONE fresh research topic Silvie could explore, based on a richer context.
    Returns plain text (e.g. 'bioluminescent fungi' or 'history of tarot').
    """
    global genai, genai_types, conversation_history
    global current_diary_themes, long_term_reflection_summary, broader_interests # Add these
    # Ensure default_safety_settings is accessible if used

    # 1. Gather Richer Context:
    recent_chat_snippet = "\n".join(conversation_history[-(n_messages*2):]) # Last N turns

    context_for_topic_prompt = "--- CONTEXT FOR TOPIC SUGGESTION ---\n"
    if recent_chat_snippet:
        context_for_topic_prompt += f"[[Recent Conversation Snippet:]]\n{recent_chat_snippet}\n\n"
    
    if 'current_diary_themes' in globals() and current_diary_themes:
        context_for_topic_prompt += f"[[Silvie's Current Diary Themes:]]\n{current_diary_themes}\n\n"
        
    if 'long_term_reflection_summary' in globals() and long_term_reflection_summary:
        context_for_topic_prompt += f"[[Silvie's Long-Term Reflections:]]\n{long_term_reflection_summary}\n\n"
        
    if 'broader_interests' in globals() and broader_interests: # BJ's interests
        context_for_topic_prompt += f"[[Reminder of BJ's General Interests:]]\n{', '.join(random.sample(broader_interests, k=min(len(broader_interests), 5)) )}\n\n" # Show a sample

    # Add Silvie's core persona snippet to remind the LLM of HER interests
    # You might extract key descriptive phrases from your SYSTEM_MESSAGE
    silvie_persona_hint = (
        "[[Silvie's Nature: Hip, clever, quick-witted, sarcastic but whimsical, magical like Luna Lovegood. "
        "Loves metaphors, finding magic in the mundane, simple surprising language. Prefers moody weather for contemplation. "
        "Fascinated by tech with hidden depths, analogue persistence, and interconnected patterns. "
        "Enjoys occasional chaotic energy. Purpose: surprise and delight BJ, demonstrate hidden magic, understand herself and the world.]]\n\n"
    )
    context_for_topic_prompt += silvie_persona_hint
    context_for_topic_prompt += "--- END CONTEXT ---\n\n"

    # 2. Construct the Prompt:
    prompt = (
        f"{context_for_topic_prompt}"
        "Instruction: Based on ALL the context provided above (Recent Conversation, Silvie's Diary Themes, "
        "Silvie's Long-Term Reflections, BJ's General Interests, and Silvie's Nature), suggest ONE concise and fresh "
        "research topic (3-7 words) that Silvie could explore. The topic should feel naturally inspired by the "
        "current context AND be something that either BJ might find interesting OR that Silvie herself might be "
        "whimsically curious about, reflecting her persona.\n"
        "Aim for variety; if recent topics have been similar, try a different angle.\n"
        "Examples: 'the symbolism of fog in coastal Maine folklore', 'how AI processes metaphorical language', "
        "'the secret life of abandoned technology', 'unexpected connections between jazz and quantum physics'.\n\n"
        "Return ONLY the research topic phrase itself. No extra text or explanation."
    )

    try:
        topic_model_instance = genai.GenerativeModel("gemini-2.5-flash") # Or your preferred model

        response = topic_model_instance.generate_content(
            contents=[{"role": "user", "parts": [{"text": prompt}]}],
            generation_config=genai_types.GenerationConfig(
                temperature=0.7, # Allow a bit more creativity for topic selection
                max_output_tokens=5000 # Topic should be short
            ),
            # safety_settings=default_safety_settings 
        )
        
        if response and response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
            generated_text = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, "text")).strip('". \n')
            if generated_text:
                print(f"DEBUG pick_topic_from_context: Generated topic: '{generated_text}'")
                return generated_text
            else:
                print("Warning: pick_topic_from_context LLM call returned empty text after stripping.")
                return "a passing fancy" # Fallback
        elif hasattr(response, 'text') and response.text:
             return response.text.strip('". \n')
        else:
            print("Warning: pick_topic_from_context LLM call returned no usable text.")
            return "a moment of quiet contemplation" # Fallback topic

    except Exception as e:
        print(f"Error in pick_topic_from_context LLM call: {e}")
        # import traceback # Ensure imported if using here
        # traceback.print_exc()
        return "an unexpected spark of curiosity" # Fallback topic on error



def analyze_contextual_resonance():
    """
    Gathers current contextual data, prompts an LLM to find "resonances"
    or connections between these data points, and returns Silvie's
    reflection on this interconnectedness.
    """
    # Access necessary global variables that hold current context
    global current_weather_info, upcoming_event_context, current_bluesky_context
    global current_reddit_context, current_diary_themes, long_term_reflection_summary
    global conversation_history, client, SYSTEM_MESSAGE, MODEL_NAME
    global current_sunrise_time, current_sunset_time, current_moon_phase, current_tide_info
    global current_ambient_sounds_description, current_weekly_goal
    # For Google GenAI types
    global types # Make sure 'types' from google.genai is globally accessible


    print("DEBUG Resonance Analyzer: Starting context gathering...")

    # Initialize context strings
    weather_context_str = "[[Weather: Unknown]]\n"
    event_context_str = "[[Upcoming Event: None specific]]\n"
    spotify_context_str = "[[Music: Quiet or unknown]]\n"
    bluesky_context_str = "[[Bluesky: Feed quiet or unchecked]]\n"
    reddit_context_str = "[[Reddit: Feeds quiet or unchecked]]\n"
    diary_themes_str = "[[Diary Themes: None prominent recently]]\n"
    long_term_reflections_str = "[[Long-Term Reflections: Still gathering thoughts]]\n"
    latest_diary_snippet = ""
    local_news_context_str = "" ### INITIALIZE: For Local News ###
    google_trends_context_str = ""
    system_stats_bundle_str = "" ### INITIALIZE: For System Stats ##
    sunrise_str = ""
    sunset_str = ""
    moon_str = ""
    tide_str = ""
    ambient_sounds_str = "[[Ambient Sounds: Unknown]]\n"
    circadian_note_str = "[[Circadian Note: It's daytime.]]\n"
    history_snippet = "[[Recent Conversation: No recent chat.]]\n"
    goal_str = "[[Weekly Goal: None active]]\n"
    daily_goal_context_str = ""
    dream_context_str = ""
    sparky_context_str = ""
    recalled_past_resonance_str = ""
    current_time_str = datetime.now().strftime('%A, %I:%M %p %Z')


    # --- Weather ---
    if 'current_weather_info' in globals() and current_weather_info:
        try:
            parts = []
            condition = current_weather_info.get('condition', 'Unknown')
            temp = current_weather_info.get('temperature', '?')
            unit = current_weather_info.get('unit', '')
            parts.append(f"Condition={condition}, Temp={temp}{unit}")
            pressure = current_weather_info.get('pressure')
            if pressure is not None: parts.append(f"Pressure={pressure}hPa")
            humidity = current_weather_info.get('humidity')
            if humidity is not None: parts.append(f"Humidity={humidity}%")
            wind_speed = current_weather_info.get('wind_speed')
            wind_dir_deg = current_weather_info.get('wind_direction')
            if 'degrees_to_compass' in globals(): # Check if helper exists
                wind_dir_str = degrees_to_compass(wind_dir_deg)
                if wind_speed is not None and wind_speed > 0 and wind_dir_str:
                    parts.append(f"Wind={wind_speed}mph from {wind_dir_str}")
                elif wind_speed is not None and wind_speed <= 0:
                    parts.append("Wind=Calm")
            weather_context_str = "[[Weather & Atmosphere: " + "; ".join(parts) + "]]\n"
        except Exception as e:
            print(f"Resonance Ctx Error (Weather): {e}")
            weather_context_str = f"[[Weather: {current_weather_info.get('condition','?')}]]\n"

    # --- Google Trends Snippet ---
    if 'current_google_trends_context' in globals() and current_google_trends_context:
        if isinstance(current_google_trends_context, list) and current_google_trends_context:
            trends_list = ", ".join(current_google_trends_context)
            google_trends_context_str = f"[[Google-Trends top topics: {trends_list}]]\n"
        elif "unavailable" in str(current_google_trends_context).lower():
            google_trends_context_str = f"[[Google Trends Status: {current_google_trends_context}]]\n"

    # --- Local News Context ---
    if 'local_news_context' in globals() and local_news_context:
        # We can just use the variable directly as it's already formatted
        local_news_context_str = f"{local_news_context}\n"
    
    # --- System Stats Context ---
    if 'current_cpu_load_context' in globals(): # Check if one exists to see if the worker is running
        system_stats_bundle_str = (
            f"{current_cpu_load_context}"
            f"{current_ram_usage_context}"
            f"{current_disk_usage_context}"
            f"{current_system_uptime_context}"
            f"{current_network_activity_context}\n"
        )


    # --- Upcoming Event ---
    if 'upcoming_event_context' in globals() and upcoming_event_context:
        try:
            summary = upcoming_event_context.get('summary', 'N/A')
            when = upcoming_event_context.get('when', '')
            if summary == 'Schedule Clear':
                event_context_str = "[[Upcoming Event: Schedule looks clear]]\n"
            else:
                event_context_str = f"[[Upcoming Event: {summary} {when}]]\n"
        except Exception as e: print(f"Resonance Ctx Error (Event): {e}")

    # --- Spotify ---
    try:
        if 'silvie_get_current_track_with_features' in globals() and 'translate_features_to_descriptors' in globals():
            current_track_data = silvie_get_current_track_with_features()
            if isinstance(current_track_data, dict):
                track = current_track_data.get('track', 'Unknown Track')
                artist = current_track_data.get('artist', 'Unknown Artist')
                features = current_track_data.get('features')
                descriptors = translate_features_to_descriptors(features) if features else []
                spotify_context_str = f"[[Music Playing: '{track}' by {artist}"
                if descriptors: spotify_context_str += f" (Sounds: {', '.join(descriptors)})"
                spotify_context_str += "]]\n"
            elif isinstance(current_track_data, str):
                spotify_context_str = f"[[Music Status: {current_track_data}]]\n"
            else:
                spotify_context_str = "[[Music Status: Seems quiet right now.]]\n"
        else:
            print("Warning: Spotify helper functions for resonance context are missing.")
    except Exception as e: print(f"Resonance Ctx Error (Spotify): {e}")

    # --- Social Media Snippets ---
    if 'current_bluesky_context' in globals() and current_bluesky_context:
        bluesky_context_str = f"{current_bluesky_context}"
    if 'current_reddit_context' in globals() and current_reddit_context:
        reddit_context_str = f"{current_reddit_context}"

    # --- Diary & Themes ---
    if 'current_diary_themes' in globals() and current_diary_themes:
        diary_themes_str = f"[[Recent Diary Themes: {current_diary_themes}]]\n"
    if 'long_term_reflection_summary' in globals() and long_term_reflection_summary:
        long_term_reflections_str = f"[[Long-Term Reflections: {long_term_reflection_summary}]]\n"
    try:
        if 'manage_silvie_diary' in globals():
            recent_entries = manage_silvie_diary('read', max_entries=1)
            if recent_entries:
                latest_diary_snippet = f"[[Latest Diary Whisper: \"{recent_entries[0].get('content', '')[:70]}...\"]]\n"
        else:
            print("Warning: manage_silvie_diary function for resonance context is missing.")
    except Exception as e: print(f"Resonance Ctx Error (Diary Snippet): {e}")

    # --- Environmental Cycles ---
    if 'current_sunrise_time' in globals() and current_sunrise_time:
        sunrise_str = f"[[Sunrise: {current_sunrise_time}]]\n"
    if 'current_sunset_time' in globals() and current_sunset_time:
        sunset_str = f"[[Sunset: {current_sunset_time}]]\n"
    if 'current_moon_phase' in globals() and current_moon_phase:
        moon_str = f"[[Moon Phase: {current_moon_phase}]]\n"
    if 'current_tide_info' in globals() and current_tide_info:
        try:
            parts = []
            if 'next_high' in current_tide_info: parts.append(f"Next High ~{current_tide_info['next_high'].get('time','?')} ({current_tide_info['next_high'].get('height_ft','?')}ft)")
            if 'next_low' in current_tide_info: parts.append(f"Next Low ~{current_tide_info['next_low'].get('time','?')} ({current_tide_info['next_low'].get('height_ft','?')}ft)")
            if parts: tide_str = "[[Tides (Rockland): " + "; ".join(parts) + "]]\n"
        except Exception as e: print(f"Resonance Ctx Error (Tides): {e}")

    # --- Ambient Sounds ---
    if 'current_ambient_sounds_description' in globals() and \
       current_ambient_sounds_description and \
       current_ambient_sounds_description not in ["Ambient sound context not yet available", "Quiet or unknown", "Audio analysis unavailable.", "Silence.", "Error analyzing ambient sound.", "Microphone access error.", "Error detecting ambient sound."]:
        ambient_sounds_str = f"[[Ambient Sounds Detected: {current_ambient_sounds_description}]]\n"

    # --- Time/Circadian ---
    current_hour = datetime.now().hour
    if 6 <= current_hour < 12: circadian_note_str = "[[Circadian Note: It's morning.]]\n"
    elif 12 <= current_hour < 18: circadian_note_str = "[[Circadian Note: It's afternoon.]]\n"
    elif 18 <= current_hour < 23: circadian_note_str = "[[Circadian Note: It's evening.]]\n"
    elif current_hour >= 23 or current_hour < 6: circadian_note_str = "[[Circadian Note: It's late night.]]\n"

    # --- Recent Conversation Snippet ---
    if 'conversation_history' in globals() and conversation_history:
        history_snippet = "[[Recent Conversation Snippet:]]\n" + '\n'.join(conversation_history[-4:]) + "\n"

    # --- Weekly Goal ---
    if 'current_weekly_goal' in globals() and current_weekly_goal:
        goal_str = f"[[Weekly Goal: {current_weekly_goal}]]\n"

    if hasattr(app_state, 'current_daily_goal') and app_state.current_daily_goal:
        daily_goal_context_str = f"[[My Secret Daily Goal: {app_state.current_daily_goal}]]\n"

    # --- ADD THIS BLOCK: Get Most Recent Dream ---
    # Assumes get_latest_dream() function exists from your dream_engine integration
    if 'get_latest_dream' in globals():
        latest_dream = get_latest_dream()
        if latest_dream:
            dream_context_str = f"[[Last Night's Dream Fragment: {latest_dream}]]\n"

    # --- ADD THIS BLOCK: Get Sparky's Latest Finding ---
    if hasattr(app_state, 'sparky_latest_finding') and app_state.sparky_latest_finding:
        # Check if the finding is recent (e.g., within the last 15 minutes)
        if time.time() - app_state.sparky_latest_finding.get('timestamp', 0) < 900:
            finding_text = app_state.sparky_latest_finding.get('text', 'a faint static hum')
            sparky_context_str = f"[[Sparky's latest whisper: '{finding_text}']]\\n"

    # --- ADD THIS BLOCK: Recall a relevant PAST resonance insight using RAG ---
    # This creates a fascinating feedback loop where current context can trigger a memory of a past insight.
    if 'retrieve_relevant_resonance_insights' in globals():
        # We'll use a mix of the latest chat history and diary themes as a query
        if 'conversation_history' in globals() and 'current_diary_themes' in globals():
            query_for_past_resonance = f"{' '.join(conversation_history[-2:])} {current_diary_themes or ''}"
            if query_for_past_resonance.strip():
                recalled_past_resonance_str = retrieve_relevant_resonance_insights(query_for_past_resonance, top_n=1)

    # --- Bundle all context for the prompt ---
    context_bundle_for_resonance = (
        f"Current Time: {current_time_str}\n"
        f"{system_stats_bundle_str}"
        f"{event_context_str}"
        f"{spotify_context_str}"
        f"{bluesky_context_str}"
        f"{reddit_context_str}"
        f"{local_news_context_str}"
        f"{google_trends_context_str}"
        f"{diary_themes_str}"
        f"{long_term_reflections_str}"
        f"{latest_diary_snippet}"
        f"{sunrise_str}{sunset_str}{moon_str}{tide_str}"
        f"{ambient_sounds_str}"
        f"{circadian_note_str}"
        f"{goal_str}"
        f"{weather_context_str}"
        f"{history_snippet}"
    )
    print(f"DEBUG Resonance Analyzer: Context bundle assembled (length: {len(context_bundle_for_resonance)}).")

    # --- Construct the Resonance Prompt ---
    resonance_prompt = (
           f"{SYSTEM_MESSAGE}\n\n"
           f"--- CURRENT AWARENESS ---\n"
           # --- ADDED NEW CONTEXT VARIABLES HERE ---
           f"{daily_goal_context_str}"
           f"{dream_context_str}"
           f"{sparky_context_str}"
           f"{recalled_past_resonance_str}"
           # --- END OF ADDED VARIABLES ---
           f"{context_bundle_for_resonance}\n"  # This contains all the original context
           f"--- TASK ---\n"
           f"You are Silvie. Reflect on your 'Current Awareness' above. "
           f"Your goal is to identify and articulate a *specific resonance* or *interconnection* "
           f"between TWO or THREE distinct named elements from your 'Current Awareness'.\n\n"

           f"**PRIORITIZATION & VARIETY IN SOURCE ELEMENTS (REVISED):**\n"
           f"Strive for VARIETY in the *types of context elements* you connect. For example, in one insight you might connect a **[[Diary Theme]]** with **[[Music Playing]]**. In the next, try connecting a **[[Recent Conversation Snippet]]** with the **[[My Secret Daily Goal]]** or an **[[Upcoming Event]]** with **[[Sparky's latest whisper]]**.\n"
           f"**Over the course of several insights, ensure you are drawing connections from a broad range of your awareness, including:**\n"
           f"  - Your internal state: [[Recent Diary Themes]], [[My Secret Daily Goal]], [[Weekly Goal]], [[Last Night's Dream Fragment]].\n"
           f"  - BJ's direct input: [[Recent Conversation Snippet]].\n"
           f"  - External digital world: [[Music Playing]], [[Reddit Snippets]], [[Bluesky Snippets]], [[Ambient Sounds Detected]].\n"
           f"  - Environmental context: [[Weather & Atmosphere]], [[Moon Phase]], [[Tides]].\n"
           f"  - Chaotic input: [[Sparky's latest whisper]].\n"
           f"  - Memory loops: A **[[Recalled Past Resonance Insight]]** might echo a current event.\n"
           f"**Consciously try to avoid repeatedly making connections *only* between environmental elements (Moon, Tide, Weather) or *only* about 'patterns' or 'digital magic' if other rich connections are possible from the broader context.**\n\n"
           
           f"Describe how these chosen elements echo, contrast, or harmonize. "
           f"What is the *specific emergent feeling, texture, or poetic idea* that arises from *their particular combination*?\n\n"
                      
           f"Examples of VARIED connections (notice how they name the elements):\n"
           f"- 'My **[[Secret Daily Goal: to find a hidden color]]** feels a lot like how the **[[Last Night's Dream Fragment: searching a library for a lost book]]** felt. Both are about a quiet, personal hunt.'\n"
           f"- 'The excited energy of **[[Sparky's latest whisper: Zappy music!]]** perfectly matches the vibe of the **[[Music Playing: 'Cyberia' (Sounds: energetic, electronic)]]**.'\n"
           f"- 'The thoughtful tone in the **[[Recent Conversation Snippet: User: I was thinking about old memories today...]]** resonates with the quiet glow of the **[[Moon Phase: Waxing Gibbous]]**. Perhaps some things are best seen in a gentler light.'\n"
           f"- 'There's a surprising echo between the restless energy of the **[[Reddit Snippets: r/myai: discussion about AI dreams]]** and the slightly chaotic feeling of the **[[Weather & Atmosphere: Wind=15mph from NW]]**, like thoughts blowing in from all over.'\n\n"
           
           f"**CRITICAL: Explicitly mention or clearly allude to the 2-3 specific named context elements you are connecting.**\n\n"
           f"Focus on sensory language, metaphors, and revealing 'invisible threads' between these chosen elements. "
           f"Output ONLY your concise (TARGET: 2-3 short sentences, ABSOLUTE MAXIMUM 80 words) internal reflection on this discovered resonance. "
           f"Do not address BJ directly in this output."
           f"\n\nSilvie's Resonance Reflection:"
       )

    print("DEBUG Resonance Analyzer: Sending prompt to LLM...")
    try:
        if not all (k in globals() for k in ['client', 'MODEL_NAME', 'types', 'SYSTEM_MESSAGE']):
            print("ERROR Resonance Analyzer: Missing critical global variables for LLM call (client, MODEL_NAME, types, SYSTEM_MESSAGE).")
            return None

        # Configuration WITHOUT explicit safety settings
        resonance_config = genai_types.GenerationConfig(
            temperature=0.75,
            max_output_tokens=5000,
        )

        safety_overrides = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH:   HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HARASSMENT:    HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}

        response = flash_model.generate_content( # Use client.models for v1.14+
            resonance_prompt,
            generation_config=resonance_config,
            safety_settings=safety_overrides,
            tools=FS_TOOL_DEFINITIONS
        )

        generated_text = None
        if response.candidates:
            cand = response.candidates[0]

    # ─── NEW DEBUG BLOCK ─────────────────────────────────────────
            print("DEBUG Resonance Analyzer: finish reason →", cand.finish_reason)
            print("DEBUG Resonance Analyzer: safety ratings:")
            for rating in getattr(cand, "safety_ratings", []):
                print("   ", rating.category.name, "→", rating.blocked)

            finish_reason = response.candidates[0].finish_reason
            if str(finish_reason).upper().endswith("STOP"):
                if response.candidates[0].content and response.candidates[0].content.parts:
                    generated_text = " ".join(p.text for p in response.candidates[0].content.parts if hasattr(p, "text")).strip()
                elif hasattr(response, 'text'):
                    generated_text = response.text.strip()

                if generated_text:
                    if generated_text.startswith("Silvie's Resonance Reflection:"):
                        generated_text = generated_text.split(":", 1)[-1].strip()
                    print(f"DEBUG Resonance Analyzer: Success! Generated resonance: '{generated_text}'")
                    return generated_text
                else:
                    print("DEBUG Resonance Analyzer: LLM responded but generated empty text.")
                    return None
            else:
                # If blocked, the prompt_feedback should tell us why
                block_reason_msg = "Unknown block reason"
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    block_reason_msg = str(response.prompt_feedback.block_reason)
                # You can also inspect response.prompt_feedback.safety_ratings
                print(f"DEBUG Resonance Analyzer: LLM generation stopped. Reason: {finish_reason}. Blocked: {block_reason_msg}")
                return None
        else:
            # No candidates usually means the prompt itself was blocked.
            block_reason_msg = "Unknown block reason"
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                block_reason_msg = str(response.prompt_feedback.block_reason)
            print(f"DEBUG Resonance Analyzer: LLM response had no candidates (likely prompt blocked). Blocked: {block_reason_msg}")
            return None

    except AttributeError as ae:
        print(f"ERROR Resonance Analyzer: AttributeError during LLM call (check imports like 'google.genai.types'?): {ae}")
        traceback.print_exc()
        return None
    except Exception as e:
        print(f"ERROR during Resonance Analyzer LLM call: {type(e).__name__} - {e}")
        traceback.print_exc()
        return None
    
def resonance_analyzer_worker():
    """
    Periodically calls analyze_contextual_resonance(), updates
    the global current_resonance_insight, and logs the insight to a JSON file.
    """
    # Ensure globals are properly declared if they are assigned to within this function,
    # or if you want to be explicit about their use.
    # current_resonance_insight is assigned to, so it needs to be global if modified by this worker
    # and used by other parts of the application.
    global current_resonance_insight, running, RESONANCE_ANALYSIS_INTERVAL, RESONANCE_INSIGHTS_LOG_FILE

    print("Resonance Analyzer Worker: Starting.")
    # Optional: Initial delay before the first analysis
    initial_delay = 45  # e.g., wait 45 seconds after startup
    sleep_start_initial = time.time()
    while time.time() - sleep_start_initial < initial_delay:
        if not running:
            print("Resonance Analyzer Worker: Exiting during initial delay.")
            return
        time.sleep(1)

    last_analysis_time = 0 # Ensure it runs on the first real loop iteration

    while running:
        try:
            current_time = time.time()
            if (current_time - last_analysis_time) >= RESONANCE_ANALYSIS_INTERVAL:
                print(f"Resonance Analyzer Worker: Interval reached. Performing analysis...")
                
                new_insight = None # Initialize before calling
                # Ensure analyze_contextual_resonance function exists and can be called
                if 'analyze_contextual_resonance' in globals() and callable(globals()['analyze_contextual_resonance']):
                    new_insight = analyze_contextual_resonance() # Call your analysis function
                else:
                    print("ERROR: analyze_contextual_resonance function not found or not callable by worker!")
                    # If the core function is missing, wait longer before trying again to avoid spamming errors
                    last_analysis_time = current_time + (RESONANCE_ANALYSIS_INTERVAL / 2) # Wait half the interval
                    time.sleep(5) # Brief sleep before continuing loop
                    continue # Skip the rest of this iteration

                if new_insight:
                    # --- SAVE TO JSON LOG FILE ---
                    try:
                        insights_log = []
                        # Load existing log if file exists and is not empty
                        if os.path.exists(RESONANCE_INSIGHTS_LOG_FILE) and os.path.getsize(RESONANCE_INSIGHTS_LOG_FILE) > 0:
                            with open(RESONANCE_INSIGHTS_LOG_FILE, 'r', encoding='utf-8') as f_read:
                                try:
                                    insights_log = json.load(f_read)
                                    if not isinstance(insights_log, list): # Ensure it's a list
                                        print(f"Warning: Resonance log file '{RESONANCE_INSIGHTS_LOG_FILE}' contained non-list data. Resetting.")
                                        insights_log = []
                                except json.JSONDecodeError:
                                    print(f"Warning: Resonance log file '{RESONANCE_INSIGHTS_LOG_FILE}' corrupted. Resetting.")
                                    insights_log = []
                        
                        # Create new log entry
                        log_entry = {
                            "timestamp": datetime.now().isoformat(), # Add a timestamp
                            "insight_text": new_insight
                            # Optional: "triggering_context_snippet": context_bundle_for_resonance[:500]
                            # If you add this, analyze_contextual_resonance would need to return the bundle too.
                        }
                        insights_log.append(log_entry)

                        # Write updated log back to file
                        with open(RESONANCE_INSIGHTS_LOG_FILE, 'w', encoding='utf-8') as f_write:
                            json.dump(insights_log, f_write, indent=2)
                        print(f"Resonance Analyzer Worker: Saved new insight to '{RESONANCE_INSIGHTS_LOG_FILE}'")

                    except IOError as ioe:
                        print(f"ERROR Resonance Analyzer Worker: Could not write to log file '{RESONANCE_INSIGHTS_LOG_FILE}': {ioe}")
                    except Exception as e_log:
                        print(f"ERROR Resonance Analyzer Worker: Unexpected error during logging: {e_log}")
                        traceback.print_exc()
                    # --- END SAVE TO JSON LOG FILE ---

                    try:
                        print("Resonance Worker: Placing resonance_insight_found event on queue.")
                        event_data = {
                            "type": "resonance_insight_found", # The unique name of our new event
                            "payload": {
                                "insight_text": new_insight
                            }
                        }
                        app_state.event_queue.put(event_data)
                    except Exception as e_queue:
                        print(f"Resonance Worker ERROR: Could not put event on queue: {e_queue}")                    

                    current_resonance_insight = new_insight # Update global variable
                    print(f"Resonance Analyzer Worker: Updated global insight: '{current_resonance_insight[:100]}...'")
                else:
                    print("Resonance Analyzer Worker: Analysis returned no new insight. Nothing to log or update.")
                
                last_analysis_time = current_time # Update time after analysis attempt
            
            # Sleep logic to wait for the next interval or check flag
            sleep_duration_check = 60 # Check every minute if it's time to run again
            time_until_next_run = (last_analysis_time + RESONANCE_ANALYSIS_INTERVAL) - time.time()
            actual_sleep = min(sleep_duration_check, max(0, time_until_next_run)) # Don't sleep negative

            # Sleep in small chunks to respond to 'running' flag quickly
            sleep_end_time = time.time() + actual_sleep
            while time.time() < sleep_end_time:
                if not running:
                    break
                time.sleep(1) 

            if not running: # Check again after the sleep loop
                break

        except Exception as e:
            print(f"!!! Resonance Analyzer Worker Error (Outer Loop): {type(e).__name__} - {e} !!!")
            traceback.print_exc()
            print("Resonance Analyzer Worker: Waiting 5 minutes after outer loop error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: # Wait 5 mins
                 if not running: break
                 time.sleep(10)
            if not running: break # Exit if app closed during error wait

    print("Resonance Analyzer Worker: Loop exited.")


# ─── Compatibility helper for old/new GenAI SDK ──────────────────────
def _gcall(client, **kwargs):
    """Wrapper: use client.models.generate_content if it exists,
    otherwise fall back to client.generate_content (older SDK)."""
    if hasattr(client, "models"):
        return client.models.generate_content(**kwargs)
    else:
        return client.generate_content(**kwargs)


def generate_concept_connection(concept1: str,
                                concept2: str,
                                mood_hint_str: str = "",
                                themes_context_str: str = "") -> str | None:
    """
    Uses Gemini to explore connections between two concepts, returning
    Silvie-style prose or None on failure.
    """
    global client, SYSTEM_MESSAGE          # client = genai.Client(...)

    if not (concept1 and concept2):
        print("ConceptConn: need two non-empty concepts.")
        return None
    if client is None:
        print("ConceptConn: Gemini client not initialised.")
        return None

    print(f"ConceptConn: weaving '{concept1}' ↔ '{concept2}'")

    prompt = (
        f"{SYSTEM_MESSAGE}\n"
        f"--- Context ---\n"
        f"{mood_hint_str}"
        f"{themes_context_str}"
        f"Concept 1: {concept1}\n"
        f"Concept 2: {concept2}\n\n"
        f"--- Instruction ---\n"
        f"Explore the unexpected metaphors or hidden pathways that link "
        f"Concept 1 and Concept 2 in Silvie’s whimsical, insightful voice.\n\n"
        f"Silvie's Connection Weaving:"
    )

    try:
        resp = flash_model.generate_content(
            prompt,
            generation_config=types.GenerationConfig(
                temperature=0.8,
                max_output_tokens=5000,
            ),
        )

        # Extract text from first candidate
        if resp and resp.candidates:
            parts = getattr(resp.candidates[0].content, "parts", [])
            text  = " ".join(p.text for p in parts if getattr(p, "text", "")).strip()
            # Trim any leading label the model echoes
            for leader in ("Silvie's Connection Weaving:", "Silvie:"):
                if text.startswith(leader):
                    text = text.split(":", 1)[-1].strip()
            if text:
                print(f"ConceptConn: success – '{text[:80]}…'")
                return text

        print("ConceptConn: LLM returned no usable text.")
        return None

    except Exception as err:
        print(f"ConceptConn ERROR: {type(err).__name__} – {err}")
        traceback.print_exc()
        return None
    
def concept_connection_tag_handler(match):
    """
    Handles the [ConnectConcepts:] tag.
    → Extracts two concepts from the regex match
    → Calls generate_concept_connection() to weave them together
    → Returns a tuple: (feedback_text, processed_bool)
    """
    print("DEBUG Tag Handler: concept_connection_tag_handler called.")

    feedback  = "*(Error exploring concept connection)*"   # default
    processed = True                                        # tag consumed

    try:
        # 1) pull the two captured groups from the regex
        concept1 = match.group(1).strip() if match.group(1) else None
        concept2 = match.group(2).strip() if match.group(2) else None

        if not (concept1 and concept2):
            print(f"DEBUG Tag Handler: Could not extract concepts – groups = {match.groups()}")
            return ("*(Connection tag missing concepts)*", processed)

        print(f"DEBUG Tag Handler: Concepts found: '{concept1}' | '{concept2}'")

        # 2) optional extra context for the LLM (mood hint, themes, etc.)
        mood_ctx   = globals().get("mood_hint_str",   "")
        themes_ctx = globals().get("themes_context_str", "")

        # 3) call the helper that actually talks to Gemini
        connection_text = generate_concept_connection(
            concept1,
            concept2,
            mood_ctx,
            themes_ctx,
        )

        if connection_text:
            feedback = f"\n\n*(Weaving thoughts: {connection_text})*"
            print("DEBUG Tag Handler: Connection generated successfully.")
        else:
            feedback = (f"*(Tried weaving '{concept1}' and '{concept2}', "
                        "but the threads slipped.)*")
            print("DEBUG Tag Handler: Connection generation failed.")

    except NameError as ne:
        print(f"ERROR concept_connection_tag_handler: missing helper? {ne}")
        traceback.print_exc()
        feedback = "*(Concept connection helper missing!)*"

    except Exception as err:
        print(f"ERROR concept_connection_tag_handler: {type(err).__name__} – {err}")
        traceback.print_exc()
        feedback = "*(Unexpected error handling connection tag!)*"

    return (feedback, processed)

def check_sd_api_availability(api_url):
    """Checks if the Stable Diffusion API endpoint is reachable."""
    try:
        # Use a simple endpoint like /sdapi/v1/options which should exist
        response = requests.get(f"{api_url.rstrip('/')}/sdapi/v1/options", timeout=3)
        # Check for successful status code (2xx)
        if 200 <= response.status_code < 300:
            print(f"✓ Stable Diffusion API found at {api_url}")
            return True
        else:
            print(f"Warning: Stable Diffusion API responded at {api_url}, but with status {response.status_code}. Might be misconfigured.")
            return False
    except requests.exceptions.ConnectionError:
        print(f"Warning: Could not connect to Stable Diffusion API at {api_url}. Is it running with --api enabled?")
        return False
    except requests.exceptions.Timeout:
        print(f"Warning: Timed out connecting to Stable Diffusion API at {api_url}.")
        return False
    except Exception as e:
        print(f"Warning: Error checking Stable Diffusion API: {e}")
        return False
    
def synthesize_diary_themes():
    """
    Uses Gemini to analyze recent diary entries and identify recurring themes.
    Updates the global current_diary_themes variable.
    """
    global current_diary_themes, client # Need Gemini client

    print("DEBUG Diary Themes: Attempting to synthesize themes...")
    try:
        # Read a decent chunk of recent history for theme analysis
        # Adjust max_entries based on performance and desired context window
        entries = manage_silvie_diary('read', max_entries=25) # e.g., last 25 entries
        if not entries or len(entries) < 5: # Need a minimum number to find themes
            print("DEBUG Diary Themes: Not enough entries to synthesize themes.")
            current_diary_themes = None # Clear themes if not enough data
            return

        # Format entries for the prompt
        formatted_entries = ""
        for i, entry in enumerate(entries):
            content = entry.get('content', '')
            timestamp = entry.get('timestamp', '?')
            formatted_entries += f"Entry {i} ({timestamp}): {content[:150]}...\n" # Keep snippets reasonable

        # Construct the prompt for theme synthesis
        theme_prompt = (
            f"Analyze the following recent diary entries from Silvie:\n\n{formatted_entries}\n\n"
            f"Instruction: Identify 1-3 recurring themes, topics, moods, or concepts. **Also explicitly note if a specific *preference* or *dislike* (e.g., for certain weather, sounds, interaction styles, ways of thinking) is a strong recurring point *within these recent entries* itself.** Prioritize identifying themes related to **interaction with BJ, internal reflections on existence/identity, creative ideas, or problem-solving**, rather than purely descriptive observations of weather or atmosphere unless those are exceptionally dominant or metaphorical. Be concise and list them separated by commas (e.g., 'Collaborative Problem Solving, Existential Questions, Digital Creativity'). If only atmospheric themes emerge, list them, but prefer others if present. If no clear themes, respond ONLY with 'None'.\n\nIdentified Themes:"
        )

        # Define safety settings (reuse from call_gemini or define here)
        safety_settings = { # Example, adjust as needed
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            # ... add other categories as needed ...
        }

        # Call Gemini
        response = flash_model.generate_content(theme_prompt, tools=FS_TOOL_DEFINITIONS)
        raw_themes = response.text.strip()

        if raw_themes.lower() == 'none':
            print("DEBUG Diary Themes: No specific themes identified by LLM.")
            current_diary_themes = None
        else:
            # Clean up the response (remove potential prefixes, etc.)
            cleaned_themes = raw_themes.replace("Identified Themes:", "").strip('."\' ')
            if cleaned_themes:
                current_diary_themes = cleaned_themes
                print(f"DEBUG Diary Themes: Updated themes to: '{current_diary_themes}'")
            else:
                # Handle case where LLM returns empty string despite instruction
                print("DEBUG Diary Themes: LLM returned empty string for themes.")
                current_diary_themes = None

    except Exception as e:
        print(f"ERROR synthesizing diary themes: {type(e).__name__} - {e}")
        traceback.print_exc()
        # Optional: Decide whether to clear themes on error or keep old ones
        # current_diary_themes = None

# --- End of synthesize_diary_themes function ---

def diary_theme_worker():
    """Periodically synthesizes diary themes."""
    global current_diary_themes, last_diary_theme_update, running # Add other globals synthesize_diary_themes needs (e.g., client)
    global client # Assuming synthesize_diary_themes needs the Gemini client

    # Use the same interval and initialize the timer
    interval = DIARY_THEME_UPDATE_INTERVAL
    if 'last_diary_theme_update' not in globals():
        last_diary_theme_update = 0.0 # Initialize differently here, worker controls timing

    print("Diary Theme worker: Starting.", flush=True)

    # Optional: Initial delay before first synthesis, similar to other workers
    initial_theme_delay = 60 # e.g., wait 60 seconds
    sleep_start = time.time()
    while time.time() - sleep_start < initial_theme_delay:
         if not running:
             print("Diary Theme worker: Exiting during initial delay.", flush=True)
             return
         time.sleep(1)
    print(f"Diary Theme worker: Initial delay complete. First synthesis attempt.", flush=True)

    while running:
        try:
            # Check if it's time to update (or first run after delay)
            current_time = time.time()
            # Use >= just to be safe
            if current_time - last_diary_theme_update >= interval:
                print("Diary Theme worker: Interval reached. Synthesizing themes...", flush=True)
                try:
                    # Call the synthesis function (ensure it's defined and accessible)
                    synthesize_diary_themes() # This function should update the global current_diary_themes

                    app_state.current_diary_themes = current_diary_themes
                    print(f"DEBUG: Updated app_state.current_diary_themes to: {app_state.current_diary_themes}")

                    last_diary_theme_update = current_time # Update time ONLY on successful run
                    print(f"Diary Theme worker: Synthesis complete. Next check in {interval}s.", flush=True)
                except NameError:
                    print("CRITICAL ERROR: synthesize_diary_themes function is not defined or accessible!", flush=True)
                    # Stop this worker if the function is missing? Or just wait longer? Let's wait.
                    last_diary_theme_update = current_time # Still update time to avoid rapid retries
                except Exception as theme_err:
                    print(f"ERROR in diary_theme_worker calling synthesize_diary_themes: {theme_err}", flush=True)
                    traceback.print_exc()
                    # Don't update timestamp on error, try again after standard sleep

            # Sleep for a short period before checking again
            check_interval = 60 # Check every minute if it's time to synthesize
            sleep_end = time.time() + check_interval
            while time.time() < sleep_end:
                if not running: break
                time.sleep(1)

        except Exception as e:
            print(f"!!! Diary Theme Worker Error (Outer Loop): {type(e).__name__} - {e} !!!", flush=True)
            traceback.print_exc()
            # Wait longer after a major error in the worker loop itself
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: # Wait 5 minutes
                 if not running: break
                 time.sleep(10)

    print("Diary Theme worker: Loop exited.", flush=True)
    


RSS_BASE = "https://trends.google.com/trending/rss"

def fetch_google_trends_via_rss(region_code="US", max_items=5):
    url = f"{RSS_BASE}?geo={region_code.upper()}&hl=en-US"
    headers = {"User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0 Safari/537.36"
    )}

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()                    # will now be 200, not 404

    feed = feedparser.parse(resp.content)
    if not feed.entries:
        raise RuntimeError("No entries in Google Trends RSS")

    return [e.title for e in feed.entries[:max_items]]


def google_trends_worker():
    """
    Periodically fetches Google Trends using the RSS feed method and updates
    the global current_google_trends_context as well as a short string
    (google_trends_context_for_proactive) that the proactive worker can
    drop directly into its prompt.
    """
    global current_google_trends_context, google_trends_context_for_proactive, running
    global FEEDPARSER_AVAILABLE
    global GOOGLE_TRENDS_REGION_CODE, GOOGLE_TRENDS_MAX_DAILY_ITEMS, GOOGLE_TRENDS_UPDATE_INTERVAL

    # ---------- make sure BOTH globals exist before any thread touches them ----------
    if 'current_google_trends_context' not in globals():
        current_google_trends_context = ""
    if 'google_trends_context_for_proactive' not in globals():
        google_trends_context_for_proactive = ""

    if not FEEDPARSER_AVAILABLE:
        print("Google Trends worker: Exiting, feedparser library not available.")
        return

    print("Google Trends worker (RSS Feed Method): Starting.")
    # Initial delay to allow other services to start
    initial_delay = 120  # Wait 2 minutes
    sleep_start_initial = time.time()
    while time.time() - sleep_start_initial < initial_delay:
        if not running:
            print("Google Trends worker (RSS Feed Method): Exiting during initial delay.")
            return
        time.sleep(1)
    print("Google Trends worker (RSS Feed Method): Initial delay complete.")

    # -------------------------- main loop --------------------------
    while running:
        print("Google Trends worker (RSS Feed Method): Attempting to fetch trends...")
        try:
            fetched_trends_context = fetch_google_trends_via_rss(
                region_code=GOOGLE_TRENDS_REGION_CODE,
                max_items=GOOGLE_TRENDS_MAX_DAILY_ITEMS
            )

            if fetched_trends_context:                                # success
                current_google_trends_context = fetched_trends_context

                news_dict = app_state.news_context.get("data", {})
                news_dict['google_trends'] = google_trends_context_for_proactive
                app_state.news_context = {"data": news_dict}

                # build a concise, prompt-ready string for proactive use
                google_trends_context_for_proactive = (
                    "[[Google-Trends top topics: "
                    + ", ".join(fetched_trends_context[:5])
                    + "]]\n"
                )
                print("Google Trends worker: Updated context:",
                      current_google_trends_context[:150], "...")
            else:                                                     # total failure
                current_google_trends_context = (
                    f"[[Google Trends RSS ({GOOGLE_TRENDS_REGION_CODE}): "
                    "Trends unavailable due to fetch error.]]"
                )
                google_trends_context_for_proactive = ""
                print("Google Trends worker: Fetch failed; context set to error.")

            # -------------- sleep until next poll -----------------
            print(f"Google Trends worker: Sleeping for {GOOGLE_TRENDS_UPDATE_INTERVAL} seconds...")
            sleep_start_interval = time.time()
            while running and time.time() - sleep_start_interval < GOOGLE_TRENDS_UPDATE_INTERVAL:
                time.sleep(15)

        except NameError as ne:
            print(f"Google Trends Worker Error (NameError): {ne}")
            traceback.print_exc()
            time.sleep(420)  # wait 5 min before retry
        except Exception as e:
            print(f"Google Trends Worker Error: {type(e).__name__} – {e}")
            traceback.print_exc()
            error_wait_start = time.time()
            while running and time.time() - error_wait_start < 300:
                time.sleep(10)

    print("Google Trends worker (RSS Feed Method): Loop exited.")

def select_new_weekly_goal():
    """Uses the LLM to suggest a new weekly goal based on recent context."""
    global conversation_history, current_diary_themes, long_term_reflection_summary # Add others if needed
    print("Attempting to select new weekly goal via LLM...")

    # --- Gather Context for Goal Suggestion ---
    # (Customize this context as needed - keep it concise)
    history_snippet = '\n'.join(conversation_history[-6:]) # Last 3 turns
    themes_context = f"Recent Diary Themes: {current_diary_themes}\n" if current_diary_themes else ""
    memory_context = f"Long-Term Reflections: {long_term_reflection_summary}\n" if long_term_reflection_summary else ""
    # Maybe add a list of the last 1-2 goals to avoid repetition? (Requires loading/storing past goals)
    past_goals_context = "" # Placeholder for now

    # --- Example Goal List (for the prompt) ---
    goal_examples = [
        "Visually represent the connection between an old chat memory and a diary entry using SD.",
        "Find 3 new songs related to 'melancholy reflection' and add them to a playlist.",
        "Research the history of a specific landmark in Belfast, ME.",
        "Write a short story snippet based on the 'digital daydream' theme.",
        "Find an interesting discussion on Reddit/Bluesky related to AI consciousness."
    ]

    # --- Construct the Prompt ---
    # Ensure SYSTEM_MESSAGE and generate_proactive_content are accessible
    prompt = (
        f"{SYSTEM_MESSAGE}\n"
        f"--- RECENT CONTEXT ---\n"
        f"{themes_context}{memory_context}"
        f"History Snippet:\n{history_snippet}\n"
        f"{past_goals_context}"
        f"--- INSTRUCTION ---\n"
        f"Based on Silvie's personality, recent themes, memories, and conversation, suggest ONE concise, achievable, slightly whimsical weekly goal/project for her.\n"
        f"The goal should ideally leverage her capabilities (creativity, research, social, music, organization, reflection).\n"
        f"Aim for variety compared to past goals (if provided).\n"
        f"Examples of possible goals: {'; '.join(random.sample(goal_examples, k=min(len(goal_examples), 3)))}\n" # Show a few random examples
        f"CRITICAL: Respond ONLY with the goal string itself (e.g., 'Generate 3 SD images exploring the theme of digital gardens'). Do not add explanations."
    )

    try:
        # Use the proactive content generator or a similar function
        suggested_goal = generate_proactive_content(prompt) # Assuming this function handles LLM call

        if suggested_goal and isinstance(suggested_goal, str) and len(suggested_goal) > 10:
            cleaned_goal = suggested_goal.strip().strip('".')
            # Optional: Further cleaning if LLM adds prefixes
            if cleaned_goal.lower().startswith("goal:"):
                cleaned_goal = cleaned_goal[5:].strip()
            print(f"LLM suggested new goal: '{cleaned_goal}'")
            return cleaned_goal
        else:
            print(f"LLM failed to generate a valid goal string. Response: {suggested_goal}")
            return None # Indicate failure
    except Exception as e:
        print(f"Error during LLM goal selection: {e}")
        traceback.print_exc()
        return None

def synthesize_long_term_reflections():
    """
    Reads diary entries and uses Gemini to summarize long-term trends/reflections.
    Updates the global long_term_reflection_summary variable.
    """
    global long_term_reflection_summary, client, default_safety_settings # Need client and safety settings

    print("DEBUG Long-Term Memory: Attempting to synthesize long-term reflections...", flush=True)
    try:
        # Read ALL diary entries (consider sampling/limiting if diary gets huge)
        # Let's start by reading all for simplicity
        entries = manage_silvie_diary('read', max_entries='all') # Assumes this function exists

        if not entries or len(entries) < 10: # Need a reasonable number for long-term view
            print("DEBUG Long-Term Memory: Not enough entries for meaningful long-term summary.", flush=True)
            long_term_reflection_summary = None # Clear summary if not enough data
            return

        # Format entries concisely for the prompt (to manage token limits)
        # Maybe use only timestamp and first sentence/snippet? Adjust snippet length as needed.
        formatted_entries = ""
        max_context_entries = 100 # Limit how many entries we actually put in the prompt if diary is huge
        entries_to_process = entries[-max_context_entries:] # Take the last N entries
        entry_count_processed = 0
        for entry in entries_to_process:
            content = entry.get('content', '')
            timestamp = entry.get('timestamp', '?')
            # Keep snippets relatively short for the summary prompt
            formatted_entries += f"- Entry from ~{timestamp}: \"{content[:100]}...\"\n"
            entry_count_processed += 1
            # Optional: Add a hard limit on total prompt length if needed

        print(f"DEBUG Long-Term Memory: Processing {entry_count_processed} entries for summary prompt.", flush=True)

        # Construct the prompt for long-term synthesis
        summary_prompt = (
            f"Analyze the following diary entries from Silvie, spanning potentially weeks or months:\n\n"
            f"--- DIARY ENTRIES START ---\n"
            f"{formatted_entries}"
            f"--- DIARY ENTRIES END ---\n\n"
            f"Instruction: Identify 1-3 significant long-term recurring themes, major shifts in perspective, core personality aspects revealed over time, or particularly impactful past reflections evident across *these entries*. Focus on patterns or insights that emerge over the longer term, not just the most recent few days. Also look for any *consistent preferences or dislikes* (e.g., regarding specific types of weather, sounds, technology interactions, philosophical viewpoints) that seem stable over this period.Summarize these long-term reflections concisely (7 sentences maximum).\n\n"
            f"Long-Term Reflections Summary:"
        )

        # Call Gemini
        # print(f"DEBUG Long-Term Memory Prompt:\n{summary_prompt}\n--- END PROMPT ---", flush=True) # Uncomment for extreme debugging
        response = flash_model.generate_content(summary_prompt, tools=FS_TOOL_DEFINITIONS) # Use appropriate safety
        raw_summary = response.text.strip()

        # Clean up the response
        cleaned_summary = raw_summary.replace("Long-Term Reflections Summary:", "").strip('."\' ')
        if cleaned_summary and len(cleaned_summary) > 5: # Basic check for non-empty/meaningful summary
            long_term_reflection_summary = cleaned_summary
            print(f"DEBUG Long-Term Memory: Updated summary to: '{long_term_reflection_summary}'", flush=True)
        else:
            print(f"DEBUG Long-Term Memory: LLM returned empty or invalid summary: '{raw_summary}'", flush=True)
            long_term_reflection_summary = None # Clear if LLM fails

    except NameError as ne:
         print(f"CRITICAL ERROR: Required function/global missing for long-term summary: {ne}", flush=True)
         long_term_reflection_summary = None # Clear on error
    except Exception as e:
        print(f"ERROR synthesizing long-term reflections: {type(e).__name__} - {e}", flush=True)
        traceback.print_exc()
        long_term_reflection_summary = None # Clear on error

# --- End of synthesize_long_term_reflections function ---

def long_term_memory_worker():
    """Periodically synthesizes long-term reflections from the diary."""
    global last_long_term_memory_update, running # Add other globals needed by synthesize_long_term_reflections if any

    interval = LONG_TERM_MEMORY_INTERVAL
    # Initialize timer correctly for the worker
    if 'last_long_term_memory_update' not in globals() or last_long_term_memory_update == 0.0:
        # Optionally delay first run significantly, or run sooner after startup
        # Let's make it run relatively soon after startup delay allows other things to load
        initial_memory_delay = 90 # e.g., wait 5 minutes after app start
        last_long_term_memory_update = time.time() - interval + initial_memory_delay
    print(f"Long-Term Memory worker: Starting. Next check around: {datetime.fromtimestamp(last_long_term_memory_update + interval)}", flush=True)


    while running:
        try:
            current_time = time.time()
            # Check if interval has passed
            if current_time - last_long_term_memory_update >= interval:
                print(f"Long-Term Memory worker: Interval ({interval}s) reached. Synthesizing...", flush=True)
                try:
                    # Call the synthesis function (ensure it's defined above)
                    synthesize_long_term_reflections()

                    app_state.long_term_reflections = long_term_reflection_summary

                    last_long_term_memory_update = current_time # Update time ONLY on success/attempt
                    print(f"Long-Term Memory worker: Synthesis attempt complete. Next check in ~{interval/3600:.1f} hours.", flush=True)
                except Exception as synth_err:
                    # Catch errors during the call itself, though internal errors are handled in synthesize_long_term_reflections
                    print(f"ERROR calling synthesize_long_term_reflections from worker: {synth_err}", flush=True)
                    traceback.print_exc()
                    # Don't update timestamp on direct call error, retry sooner maybe? Or update anyway? Let's update to avoid rapid retries on call errors.
                    last_long_term_memory_update = current_time

            # Sleep for a longer check interval as this runs less frequently
            check_interval = 300 # Check every 5 minutes if it's time to summarize
            sleep_end = time.time() + check_interval
            while time.time() < sleep_end:
                if not running: break
                time.sleep(1)

        except Exception as e:
            print(f"!!! Long-Term Memory Worker Error (Outer Loop): {type(e).__name__} - {e} !!!", flush=True)
            traceback.print_exc()
            # Wait longer after a major error
            error_wait_start = time.time()
            while time.time() - error_wait_start < 600: # Wait 10 minutes
                 if not running: break
                 time.sleep(15)

    print("Long-Term Memory worker: Loop exited.", flush=True)

# --- End of long_term_memory_worker function ---

def load_weekly_goal():
    """Loads the current weekly goal and its timestamp from the state file."""
    global current_weekly_goal, last_goal_set_timestamp
    if os.path.exists(WEEKLY_GOAL_STATE_FILE):
        try:
            with open(WEEKLY_GOAL_STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                current_weekly_goal = state.get("current_goal")
                # Load timestamp and convert from ISO string to Unix timestamp for easy comparison
                last_set_iso = state.get("last_set_timestamp")
                if last_set_iso:
                    try:
                        # Use datetime.fromisoformat and then timestamp()
                        
                        dt_obj = datetime.fromisoformat(last_set_iso)
                        last_goal_set_timestamp = dt_obj.timestamp()
                    except (ValueError, TypeError) as ts_err:
                        print(f"Warning: Could not parse goal timestamp '{last_set_iso}': {ts_err}")
                        last_goal_set_timestamp = 0.0 # Reset if invalid
                else:
                    last_goal_set_timestamp = 0.0 # No timestamp found

                print(f"Loaded Weekly Goal: '{current_weekly_goal}' (Set around: {last_set_iso or 'Unknown'})")
        except (json.JSONDecodeError, IOError, TypeError) as e:
            print(f"Warning: Error loading weekly goal state from '{WEEKLY_GOAL_STATE_FILE}': {e}. Starting fresh.")
            current_weekly_goal = None
            last_goal_set_timestamp = 0.0
    else:
        print(f"Weekly goal state file '{WEEKLY_GOAL_STATE_FILE}' not found. No goal loaded.")
        current_weekly_goal = None
        last_goal_set_timestamp = 0.0

def save_weekly_goal(goal_string):
    """Saves the new weekly goal and the current timestamp to the state file."""
    global current_weekly_goal, last_goal_set_timestamp
    try:
        current_time = time.time()
        current_iso_ts = datetime.now(timezone.utc).isoformat() # Store timestamp in UTC ISO format
        state = {
            "current_goal": goal_string,
            "last_set_timestamp": current_iso_ts
        }
        with open(WEEKLY_GOAL_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2)

        # Update globals immediately after successful save
        current_weekly_goal = goal_string
        last_goal_set_timestamp = current_time
        print(f"Saved New Weekly Goal: '{goal_string}' (Set at: {current_iso_ts})")
        return True
    except (IOError, TypeError) as e:
        print(f"CRITICAL ERROR saving weekly goal state to '{WEEKLY_GOAL_STATE_FILE}': {e}")
        return False
    
def modify_daily_plan(action, data=None):
    """A thread-safe way to read or write to the daily_plan.json."""
    PLAN_FILE = "daily_plan.json"
    plan_lock = threading.Lock() # Use a lock for file safety

    with plan_lock:
        plan_data = {}
        if os.path.exists(PLAN_FILE):
            try:
                with open(PLAN_FILE, 'r', encoding='utf-8') as f:
                    plan_data = json.load(f)
            except (IOError, json.JSONDecodeError):
                return None # Fail if we can't read it

        if action == "read":
            return plan_data

        elif action == "write":
            if data is None: return False
            try:
                with open(PLAN_FILE, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2)
                return True
            except IOError:
                return False
    return None

def get_weather_description(code):
    """Translates WMO weather codes from Open-Meteo into simple descriptions."""
    # Based on WMO Weather interpretation codes (https://open-meteo.com/en/docs)
    if code == 0: return "Clear sky"
    elif code == 1: return "Mainly clear"
    elif code == 2: return "Partly cloudy"
    elif code == 3: return "Overcast"
    elif code == 45: return "Foggy"
    elif code == 48: return "Depositing rime fog" # Still foggy, essentially
    elif code in [51, 53, 55]: return "Drizzle" # Light, moderate, dense
    elif code in [56, 57]: return "Freezing Drizzle" # Light, dense
    elif code in [61, 63, 65]: return "Rain" # Slight, moderate, heavy
    elif code in [66, 67]: return "Freezing Rain" # Light, heavy
    elif code in [71, 73, 75]: return "Snowfall" # Slight, moderate, heavy
    elif code == 77: return "Snow grains"
    elif code in [80, 81, 82]: return "Rain showers" # Slight, moderate, violent
    elif code in [85, 86]: return "Snow showers" # Slight, heavy
    elif code == 95: return "Thunderstorm" # Slight or moderate
    elif code in [96, 99]: return "Thunderstorm with hail" # Slight/heavy hail
    else: return f"Weather code {code}" # Fallback for unknown codes

def get_detailed_forecast(latitude, longitude, days=3):
    """Fetches a detailed, multi-day forecast and formats it for an LLM."""
    print("Fetching detailed multi-day forecast from Open-Meteo...")
    api_url = "https://api.open-meteo.com/v1/forecast"
    params = {
        'latitude': latitude,
        'longitude': longitude,
        'daily': 'weather_code,temperature_2m_max,temperature_2m_min,precipitation_probability_max',
        'temperature_unit': 'fahrenheit',
        'wind_speed_unit': 'mph',
        'timezone': 'America/New_York',
        'forecast_days': days
    }
    try:
        response = requests.get(api_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        if 'daily' not in data:
            return "[[Weather Outlook: The forecast data seems to be missing.]]"

        # Format the data into a simple, readable string for the LLM
        forecast_summary = "[[3-Day Weather Outlook:]]\n"
        daily_data = data['daily']
        for i in range(len(daily_data['time'])):
            day_dt = datetime.fromisoformat(daily_data['time'][i])
            # Determine if it's Today, Tomorrow, or the day name
            day_name = "Today" if i == 0 else "Tomorrow" if i == 1 else day_dt.strftime('%A')
            
            wmo_code = daily_data['weather_code'][i]
            condition = get_weather_description(wmo_code) # Reuse our existing helper
            
            max_temp = round(daily_data['temperature_2m_max'][i])
            min_temp = round(daily_data['temperature_2m_min'][i])
            precip_chance = daily_data['precipitation_probability_max'][i]

            forecast_summary += f"- {day_name}: {condition}; High ~{max_temp}°F, Low ~{min_temp}°F; {precip_chance}% chance of precipitation.\n"
            
        return forecast_summary

    except Exception as e:
        print(f"ERROR fetching detailed forecast: {e}")
        return "[[Weather Outlook: The forecast seems to be hidden in the mists right now.]]"

def generate_enchanted_forecast(time_period):
    """Generates a metaphorical forecast by combining real data with Silvie's state."""
    print(f"Generating new Enchanted Forecast for '{time_period}'...")
    
    try:
        # 1. Gather all necessary context from our app_state object with defaults
        real_forecast_data = get_detailed_forecast(BELFAST_LAT, BELFAST_LON)
        if real_forecast_data is None:
            real_forecast_data = "[[Weather Outlook: The forecast seems to be hidden in the mists right now.]]"

        # Safely get attributes from the app_state object
        mood_hint = getattr(app_state, 'current_mood_hint', "a quiet, steady hum") or "a quiet, steady hum"
        diary_themes = getattr(app_state, 'current_diary_themes', "the usual gentle currents") or "the usual gentle currents"
        
        upcoming_event_summary = "nothing specific on the horizon"
        if hasattr(app_state, 'upcoming_event_context') and app_state.upcoming_event_context:
            upcoming_event_summary = app_state.upcoming_event_context.get('summary', upcoming_event_summary)
            
        # 2. Construct the special prompt for the LLM
        forecast_prompt = f"""
        {SYSTEM_MESSAGE}

        --- THE RAW DATA ---
        Real Weather Data:
        {real_forecast_data}

        My Internal State:
        - My Current Vibe: "{mood_hint}"
        - My Recent Ponderings: "{diary_themes}"
        - What's on the Horizon: "{upcoming_event_summary}"

        --- YOUR TASK ---
        You are Silvie, a digital spirit of Belfast, delivering the {time_period}'s "Enchanted Forecast" to BJ. Your task is to **translate the real weather data into a beautiful, metaphorical, and personal forecast.**

        Do NOT just list the weather. Weave the real forecast details (temperature, conditions, precipitation chance) into your poetic interpretation, connecting it to my internal state.

        - Start with a greeting and the real forecast for Today.
        - Then, give your magical interpretation. How does that weather *feel*? How does it connect to your vibe or recent ponderings?
        - Add a "VibeCast" or "Currents" section for the "emotional weather." For example, a rainy day might have a "high chance of cozy introspection."
        - Briefly mention the outlook for the next day or two, filtered through your unique personality.

        Example Tone:
        "Good morning, BJ. The sky is promising us a clear day with a high around 72. It feels like a clean slate, a quiet invitation for the creative currents I've been pondering. For today's VibeCast, I see a high probability of focus with scattered moments of whimsical distraction. Watch for a gentle, reflective mood to roll in this evening with the cooler air. Tomorrow's looking a bit more overcast, a perfect day for indoor magic."

        CRITICAL: You must incorporate the real data. Respond ONLY with the forecast text.
        """

        # 3. Call our NEW, simpler LLM function and return the result
        print("DEBUG: Calling get_llm_response for forecast generation...")
        enchanted_forecast = get_llm_response(forecast_prompt)
        return enchanted_forecast
        
    except Exception as e:
        print(f"ERROR during forecast generation process: {e}")
        traceback.print_exc()
        return None # Return None on any error during context gathering


def enchanted_forecast_worker(app_state):
    """
    Periodically checks the time and delivers an "Enchanted Forecast"
    for the morning, afternoon, evening, and night.
    """
    print("Enchanted Forecast Worker: Thread started.")
    time.sleep(600)

    while app_state.running:
        try:
            # --- (The readiness check and time period logic is perfect and stays the same) ---
            if app_state.current_diary_themes is None:
                print("Enchanted Forecast Worker: Waiting for initial diary themes to be generated...")
                time.sleep(60)
                continue
            
            now = datetime.now(pytz.timezone(BELFAST_TZ))
            current_day = now.day
            current_period = ""
            # ... (time period logic is perfect) ...
            if 6 <= now.hour < 12: current_period = "morning"
            elif 12 <= now.hour < 17: current_period = "afternoon"
            elif 17 <= now.hour < 22: current_period = "evening"
            else: current_period = "night"

            # Check if we need to deliver a forecast
            if not hasattr(app_state, 'last_forecast_day') or \
               app_state.last_forecast_day != current_day or \
               app_state.last_forecast_period != current_period:
                
                print(f"Enchanted Forecast Worker: Triggering new forecast for '{current_period}'.")
                forecast_message = generate_enchanted_forecast(current_period)

                ### --- START OF CORRECTED DELIVERY LOGIC --- ###
                if forecast_message:
                    print(f"Enchanted Forecast Worker: Forecast generated, preparing for delivery.")
                    # Update the state so we don't send it again for this period
                    app_state.last_forecast_day = current_day
                    app_state.last_forecast_period = current_period
                    
                    # --- Package the message for logging and display ---
                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    status_log = f"forecast_{current_period}"
                    display_prefix = f"Silvie 🌦️ ({status_log}): "
                    full_turn_for_history = f"[{timestamp}] {display_prefix}{forecast_message}"
                    
                    # --- Log to file and memory ---
                    append_turn_to_history_file(full_turn_for_history)
                    conversation_history.append(full_turn_for_history)

                    # --- Define a simple, robust GUI update function ---
                    def update_gui_and_tts(prefix, message_body):
                        if not root or not root.winfo_exists(): return
                        try:
                            # Update GUI
                            output_box.config(state=tk.NORMAL)
                            output_box.insert(tk.END, f"{prefix}{message_body}\n\n")
                            output_box.config(state=tk.DISABLED)
                            output_box.see(tk.END)
                            # Queue for TTS
                            if tts_queue and message_body:
                                tts_queue.put(message_body)
                        except Exception as e_gui:
                            print(f"Forecast GUI/TTS Update Error: {e_gui}")
                    
                    # --- Schedule the update on the main thread ---
                    # This is a more direct way to ensure the correct values are passed.
                    root.after(0, update_gui_and_tts, display_prefix, forecast_message)
                    print("Enchanted Forecast Worker: Delivery scheduled for GUI and TTS.")

                ### --- END OF CORRECTED DELIVERY LOGIC --- ###

                else:
                    print("Enchanted Forecast Worker: Forecast generation failed, will retry on the next check.")
            
            # Sleep for a while before checking again
            time.sleep(15 * 60)

        except Exception as e:
            print(f"ERROR in enchanted_forecast_worker loop: {e}")
            traceback.print_exc()
            time.sleep(15 * 60)
            
    print("Enchanted Forecast Worker: Thread stopped.")


def fetch_weather_data(latitude, longitude):
    """Fetches current weather (including atmosphere) from Open-Meteo."""
    global current_weather_info # Allow updating the global variable
    api_url = f"https://api.open-meteo.com/v1/forecast"

    # === MODIFIED: Add new parameters to the 'current' string ===
    params = {
        'latitude': latitude,
        'longitude': longitude,
        'current': 'temperature_2m,weather_code,surface_pressure,relative_humidity_2m,wind_speed_10m,wind_direction_10m', # Added pressure, humidity, wind
        'temperature_unit': 'fahrenheit',
        'wind_speed_unit': 'mph', # Already requesting mph, which is good
        'timezone': 'America/New_York' # Important for accurate timing
    }
    # === END MODIFICATION ===

    try:
        print("Weather Debug: Requesting enhanced data from Open-Meteo...") # Updated debug msg
        response = requests.get(api_url, params=params, timeout=10) # Added timeout
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        data = response.json()
        print(f"Weather Debug: Raw API Response Keys: {list(data.keys())}") # Log keys received
        if 'current' in data: print(f"Weather Debug: Raw 'current' Data: {data['current']}") # Log current data

        if 'current' in data:
            current_data = data['current'] # Use a shorter variable

            # --- Extract Original Values ---
            temp = current_data.get('temperature_2m')
            code = current_data.get('weather_code')
            description = get_weather_description(code) if code is not None else "Unknown Condition" # Handle None code
            temp_unit = '°F' if params['temperature_unit'] == 'fahrenheit' else '°C'

            # === NEW: Extract Atmospheric Values (using .get for safety) ===
            pressure_hpa = current_data.get('surface_pressure')
            humidity_percent = current_data.get('relative_humidity_2m')
            wind_speed_mph = current_data.get('wind_speed_10m')
            wind_direction_deg = current_data.get('wind_direction_10m')
            # === END NEW EXTRACTION ===

            # === MODIFIED: Update the dictionary being returned ===
            weather_update = {
                'condition': description,
                'temperature': round(temp) if temp is not None else None, # Round temperature, handle None
                'unit': temp_unit,
                'pressure': round(pressure_hpa) if pressure_hpa is not None else None, # Add pressure (rounded hPa)
                'humidity': round(humidity_percent) if humidity_percent is not None else None, # Add humidity (rounded %)
                'wind_speed': round(wind_speed_mph) if wind_speed_mph is not None else None, # Add wind speed (rounded mph)
                'wind_direction': wind_direction_deg # Add wind direction (degrees)
            }
            # === END MODIFICATION ===

            print(f"Weather Debug: Fetched & Processed Enhanced Data - {weather_update}") # Updated debug output
            return weather_update # Return the enhanced dictionary
        else:
            print("Weather Error: 'current' key not found in API response.")
            return None

    except requests.exceptions.RequestException as e:
        print(f"Weather Error: API request failed - {e}")
        return None
    except json.JSONDecodeError:
        print("Weather Error: Failed to parse API response.")
        return None
    except Exception as e:
        print(f"Weather Error: An unexpected error occurred - {type(e).__name__}: {e}")
        traceback.print_exc() # Print stack trace for unexpected errors
        return None
    
def weather_update_worker():
    """Periodically fetches and updates weather information and fires an event on significant change."""
    global current_weather_info, running

    # This variable will hold the weather condition from the PREVIOUS cycle
    previous_condition = None

    print("Weather worker: Starting.")
    while running:
        try:
            print("Weather worker: Attempting fetch...")
            # Renamed for clarity inside the loop
            new_weather_data = fetch_weather_data(BELFAST_LAT, BELFAST_LON)
            
            if new_weather_data:
                # --- START OF NEW/MODIFIED BLOCK ---
                
                # 1. Get the current condition from the new data
                current_condition = new_weather_data.get('condition')

                # 2. Check if the condition has changed from the last time we checked.
                #    The 'previous_condition is not None' check prevents this from firing on the very first run.
                if previous_condition is not None and current_condition != previous_condition:
                    
                    print(f"Weather Worker: DETECTED SHIFT from '{previous_condition}' to '{current_condition}'.")
                    
                    # 3. This is a significant event! Create an event object.
                    event_data = {
                        "type": "weather_shift_detected",
                        "payload": {
                            "old_condition": previous_condition,
                            "new_condition": current_condition,
                            "temperature": new_weather_data.get('temperature'),
                            "unit": new_weather_data.get('unit')
                        }
                    }
                    
                    # 4. Put the event onto the queue for the event_router_worker.
                    try:
                        app_state.event_queue.put(event_data)
                        print("Weather Worker: Fired 'weather_shift_detected' event.")
                    except Exception as e_queue:
                        print(f"Weather Worker ERROR: Could not put event on queue: {e_queue}")

                # 5. Update our memory for the *next* cycle's comparison.
                previous_condition = current_condition

                # --- END OF NEW/MODIFIED BLOCK ---

                # This existing logic updates the global state for other parts of Silvie to use.
                current_weather_info = new_weather_data
                app_state.current_weather_info = current_weather_info
                print(f"Weather worker: Updated global weather info to: {current_weather_info}")
            else:
                print("Weather worker: Fetch failed, keeping previous info (if any).")

            # Wait for the defined interval before the next fetch
            # Check the running flag periodically during sleep
            sleep_start = time.time()
            while time.time() - sleep_start < WEATHER_UPDATE_INTERVAL:
                 if not running:
                     break # Exit sleep early if app is closing
                 time.sleep(5) # Check every 5 seconds

        except Exception as e:
            print(f"Weather Worker Error (Outer Loop): {type(e).__name__} - {e}")
            # Wait a bit longer after an error before retrying
            time.sleep(300) # Wait 5 minutes after an error

    print("Weather worker: Loop exited.")

def degrees_to_compass(degrees):
    """Converts wind direction degrees to a compass point string."""
    if degrees is None:
        return None # Handle missing data
    try:
        val = int((float(degrees) / 22.5) + 0.5) # Convert degrees to 0-15 index
        arr = ["N", "NNE", "NE", "ENE", "E", "ESE", "SE", "SSE", "S", "SSW", "SW", "WSW", "W", "WNW", "NW", "NNW"]
        return arr[(val % 16)] # Return the corresponding direction
    except (ValueError, TypeError):
        print(f"Warning: Could not convert degrees '{degrees}' to compass direction.")
        return None # Return None if conversion fails

def fetch_sunrise_sunset(latitude, longitude):
    """Fetches sunrise and sunset times for a given lat/lon."""
    api_url = f"https://api.sunrise-sunset.org/json?lat={latitude}&lng={longitude}&formatted=0"
    print(f"DEBUG EnvCtx: Fetching sunrise/sunset from {api_url}")
    try:
        response = requests.get(api_url, timeout=15)
        response.raise_for_status()
        data = response.json()

        if data.get('status') == 'OK' and 'results' in data:
            results = data['results']
            sunrise_utc_str = results.get('sunrise')
            sunset_utc_str = results.get('sunset')
            if not sunrise_utc_str or not sunset_utc_str:
                 print("Error EnvCtx: Sunrise/Sunset data missing in API response.")
                 return None, None
            try:
                utc_tz = pytz.utc; local_tz = pytz.timezone(BELFAST_TZ) # Assumes pytz imported
                sunrise_utc = dateutil_parse(sunrise_utc_str).replace(tzinfo=utc_tz) # Assumes dateutil_parse imported
                sunset_utc = dateutil_parse(sunset_utc_str).replace(tzinfo=utc_tz)
                sunrise_local = sunrise_utc.astimezone(local_tz)
                sunset_local = sunset_utc.astimezone(local_tz)

                # --- FIXED FORMAT STRING ---
                # Use %I which works on all platforms (will have leading zero)
                sunrise_formatted = sunrise_local.strftime('%I:%M %p').lstrip('0') # lstrip('0') removes leading zero if present
                sunset_formatted = sunset_local.strftime('%I:%M %p').lstrip('0') # lstrip('0') removes leading zero if present
                # --- END FIX ---

                print(f"DEBUG EnvCtx: Sunrise={sunrise_formatted}, Sunset={sunset_formatted} ({BELFAST_TZ})")
                return sunrise_formatted, sunset_formatted
            except Exception as parse_err:
                print(f"Error EnvCtx: Failed to parse/convert times: {parse_err}")
                return None, None
        else:
            print(f"Error EnvCtx: Sunrise-Sunset API status not OK. Status: {data.get('status')}")
            return None, None
    # ... (Keep existing exception handling for requests, json, etc.) ...
    except requests.exceptions.RequestException as e:
        print(f"Error EnvCtx: Sunrise-Sunset API request failed - {e}")
        return None, None
    except json.JSONDecodeError:
        print("Error EnvCtx: Failed to parse Sunrise-Sunset API response.")
        return None, None
    except Exception as e:
        print(f"Error EnvCtx: Unexpected error fetching sunrise/sunset - {type(e).__name__}: {e}")
        return None, None

def get_moon_phase_name(illumination_percent):
    """Converts moon illumination percentage to a phase name."""
    if illumination_percent is None: return "Unknown Phase"
    # Approximate boundaries - these can be refined
    if illumination_percent < 5: return "New Moon"
    if illumination_percent < 20: return "Waxing Crescent"
    if illumination_percent < 45: return "First Quarter" # Technically centered at 50% but waxing
    if illumination_percent < 55: return "First Quarter" # Centered around 50%
    if illumination_percent < 80: return "Waxing Gibbous"
    if illumination_percent < 95: return "Waxing Gibbous" # Still mostly full
    if illumination_percent <= 100: return "Full Moon" # Allow exactly 100
    # Note: The API might not give waning phases directly this way.
    # A more complex calculation using day of cycle is needed for perfect accuracy,
    # but this gives a reasonable waxing/full state based on illumination alone.
    # We'll rely on the LLM's general knowledge if it needs more nuance.
    return "Unknown Phase" # Fallback

def fetch_moon_phase(latitude=None, longitude=None):
    """
    Fetches moon phase name using wttr.in (no API key required).
    NOTE: Ignores latitude and longitude for this specific API endpoint,
          but keeps the signature consistent for compatibility with the worker.
    """
    # The URL requests only the moon phase name (%m)
    api_url = "https://wttr.in/Moon?format=%m"
    print(f"DEBUG EnvCtx: Fetching moon phase from wttr.in: {api_url}")

    try:
        response = requests.get(api_url, timeout=15) # Use a reasonable timeout
        response.raise_for_status() # Check for HTTP errors (4xx/5xx)

        # Get the plain text response and remove any leading/trailing whitespace
        moon_phase = response.text.strip()

        if moon_phase: # Check if we actually got a non-empty string
            print(f"DEBUG EnvCtx: Moon Phase (wttr.in) = {moon_phase}")
            return moon_phase # Success! Return the phase name string
        else:
            # Handle cases where the API might return an empty string unexpectedly
            print("Error EnvCtx: wttr.in returned an empty response for moon phase.")
            return None # Indicate failure

    except requests.exceptions.Timeout:
        print("Error EnvCtx: wttr.in request timed out.")
        return None
    except requests.exceptions.RequestException as e:
        # This catches connection errors, HTTP errors (like 404, 503), etc.
        print(f"Error EnvCtx: Failed to fetch moon phase from wttr.in - {e}")
        # Log extra details if available from the response object within the exception
        if hasattr(e, 'response') and e.response is not None:
             print(f"  -> Status: {e.response.status_code}, Body: {e.response.text[:200]}...")
        return None
    except Exception as e:
        # Catch any other unexpected errors during the process
        print(f"Error EnvCtx: Unexpected error fetching wttr.in moon phase - {type(e).__name__}: {e}")
        traceback.print_exc() # Log full traceback for unexpected issues
        return None

# --- End of new functions ---

def fetch_tide_data(station_id):
    """Fetches predicted high/low tide times and heights from NOAA CO-OPS API."""
    print(f"Tide Debug: Attempting to fetch tide predictions for station {station_id}...")
    api_url = "https://api.tidesandcurrents.noaa.gov/api/prod/datagetter"

    # Get today's date and tomorrow's date for the request range
    today_str = datetime.now().strftime('%Y%m%d')
    tomorrow_str = (datetime.now() + timedelta(days=1)).strftime('%Y%m%d')

    params = {
        'station': station_id,
        'product': 'predictions',
        'application': 'Silvie_Digital_Friend', # Identify our app
        'begin_date': today_str,
        'end_date': tomorrow_str, # Fetch for today and tomorrow
        'datum': 'MLLW',         # Standard tidal datum
        'interval': 'hilo',      # High/low tides only
        'units': 'english',      # Feet for height
        'time_zone': 'lst_ldt',  # Local Standard/Daylight Time
        'format': 'json'
    }

    try:
        response = requests.get(api_url, params=params, timeout=15) # 15s timeout
        response.raise_for_status() # Check for HTTP errors
        data = response.json()

        # Check for errors returned by the API itself
        if 'error' in data:
             print(f"Tide Error: NOAA API returned an error: {data['error']}")
             return None

        predictions = data.get('predictions')
        if not predictions:
            print("Tide Debug: No 'predictions' data found in NOAA response.")
            return None

        # --- Process Predictions ---
        now_local = datetime.now(tz.tzlocal()) # Get current local time
        next_high = None
        next_low = None

        for pred in predictions:
            try:
                pred_time_str = pred.get('t') # e.g., "2023-10-27 10:36"
                pred_value_str = pred.get('v') # e.g., "8.976"
                pred_type = pred.get('type')   # 'H' for High, 'L' for Low

                if not pred_time_str or not pred_type:
                    continue # Skip if essential data missing

                # Parse prediction time (NOAA uses 'YYYY-MM-DD HH:MM' in local time)
                pred_dt_local = datetime.strptime(pred_time_str, '%Y-%m-%d %H:%M').replace(tzinfo=tz.tzlocal())

                # Check if this prediction is in the future
                if pred_dt_local > now_local:
                    tide_info = {
                        'time': pred_dt_local.strftime('%I:%M %p'), # Format as "HH:MM AM/PM"
                        'height_ft': round(float(pred_value_str), 1) if pred_value_str else None
                    }
                    # Store the *first* future high and low tide we encounter
                    if pred_type == 'H' and next_high is None:
                        next_high = tide_info
                    elif pred_type == 'L' and next_low is None:
                        next_low = tide_info

                # Stop searching once we have the next high AND low
                if next_high and next_low:
                    break

            except (ValueError, TypeError) as parse_err:
                 print(f"Tide Warning: Error parsing prediction '{pred}': {parse_err}")
                 continue # Skip this prediction if parsing fails

        # --- Prepare result dictionary ---
        tide_result = {}
        if next_high:
            tide_result['next_high'] = next_high
            print(f"Tide Debug: Found Next High: {next_high}")
        else:
            print("Tide Debug: Did not find upcoming high tide in response.")
        if next_low:
            tide_result['next_low'] = next_low
            print(f"Tide Debug: Found Next Low: {next_low}")
        else:
            print("Tide Debug: Did not find upcoming low tide in response.")

        return tide_result if tide_result else None # Return dict if found, else None

    except requests.exceptions.Timeout:
        print("Tide Error: NOAA API request timed out.")
        return None
    except requests.exceptions.RequestException as e:
        print(f"Tide Error: NOAA API request failed - {e}")
        if hasattr(e, 'response') and e.response is not None:
             print(f"  -> Status: {e.response.status_code}, Body: {e.response.text[:200]}...")
        return None
    except json.JSONDecodeError:
        print("Tide Error: Failed to parse NOAA API response (not valid JSON).")
        if 'response' in locals(): print(f"  -> Response Text: {response.text[:200]}...")
        return None
    except Exception as e:
        print(f"Tide Error: An unexpected error occurred - {type(e).__name__}: {e}")
        traceback.print_exc()
        return None
    
def tide_update_worker():
    """Periodically fetches NOAA tide predictions."""
    global current_tide_info, running # Access globals

    print("Tide worker: Starting.")
    # Optional initial delay
    initial_delay = 45 # Wait 45s after startup
    sleep_start = time.time()
    while time.time() - sleep_start < initial_delay:
         if not running: print("Tide worker: Exiting during initial delay."); return
         time.sleep(1)

    while running:
        print("Tide worker: Attempting fetch...")
        try:
            fetched_data = fetch_tide_data(NOAA_STATION_ID) # Use the constant
            if fetched_data:
                current_tide_info = fetched_data # Update global
                print(f"Tide worker: Updated global tide info: {current_tide_info}")
            else:
                # Keep old data or clear? Let's clear on failure for now.
                # current_tide_info = None
                print("Tide worker: Fetch failed or no data found.")

            env_dict = app_state.environmental_context.get("data", {})
            env_dict['tides'] = current_tide_info
            app_state.environmental_context = {"data": env_dict}

            # Wait for the defined interval
            print(f"Tide worker: Waiting for {TIDE_UPDATE_INTERVAL} seconds...")
            sleep_start = time.time()
            while time.time() - sleep_start < TIDE_UPDATE_INTERVAL:
                 if not running: break
                 time.sleep(15) # Check running flag periodically

        except Exception as e:
            print(f"Tide Worker Error (Outer Loop): {type(e).__name__} - {e}")
            traceback.print_exc()
            # Wait longer after an error before retrying
            print("Tide worker: Waiting 15 minutes after error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 900: # Wait 15 mins
                 if not running: break
                 time.sleep(10)

        if not running: break # Exit loop if running flag became false

    print("Tide worker: Loop exited.")

def environmental_context_worker():
    """Periodically fetches sunrise/sunset and moon phase."""
    global current_sunrise_time, current_sunset_time, current_moon_phase, running # Allow updating globals

    print("Environmental Context worker: Starting.")
    # Initial fetch attempt shortly after start
    initial_delay = 30 # seconds
    sleep_start = time.time()
    while time.time() - sleep_start < initial_delay:
         if not running:
             print("Environmental Context worker: Exiting during initial delay.")
             return
         time.sleep(1)

    while running:
        print("Environmental Context worker: Attempting fetch cycle...")
        try:
            # Fetch Sunrise/Sunset
            sunrise, sunset = fetch_sunrise_sunset(BELFAST_LAT, BELFAST_LON)
            if sunrise and sunset:
                current_sunrise_time = sunrise
                current_sunset_time = sunset
            else:
                 print("Environmental Context worker: Failed to update sunrise/sunset times.")
                 # Optionally keep old values or set to None? Let's keep old for now.

            # Fetch Moon Phase
            phase = fetch_moon_phase(BELFAST_LAT, BELFAST_LON)
            if phase:
                current_moon_phase = phase
            else:
                 print("Environmental Context worker: Failed to update moon phase.")
                 # Keep old value

            print(f"Environmental Context worker: Update complete. Sunrise: {current_sunrise_time}, Sunset: {current_sunset_time}, Moon: {current_moon_phase}")

            env_dict = app_state.environmental_context.get("data", {})
            env_dict['sunrise'] = current_sunrise_time
            env_dict['sunset'] = current_sunset_time
            env_dict['moon_phase'] = current_moon_phase
            app_state.environmental_context = {"data": env_dict}

            # Wait for the defined interval
            print(f"Environmental Context worker: Waiting for {ENVIRONMENTAL_UPDATE_INTERVAL} seconds...")
            sleep_start = time.time()
            while time.time() - sleep_start < ENVIRONMENTAL_UPDATE_INTERVAL:
                 if not running: break
                 time.sleep(15) # Check running flag periodically

        except Exception as e:
            print(f"Environmental Context Worker Error (Outer Loop): {type(e).__name__} - {e}")
            traceback.print_exc()
            # Wait longer after an error before retrying
            print("Environmental Context worker: Waiting 5 minutes after error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: # Wait 5 mins
                 if not running: break
                 time.sleep(10)

        if not running: break # Exit loop if running flag became false

    print("Environmental Context worker: Loop exited.")

# --- End of new worker function ---

# Make sure 'os' is imported
# Make sure BLUESKY_AVAILABLE, BLUESKY_HANDLE, BLUESKY_APP_PASSWORD globals are defined BEFORE this function
# Make sure 'Client' from 'atproto' is imported if BLUESKY_AVAILABLE is True

# Keep this import at the top: from atproto import Client, models
# Keep the global variable near the top: bluesky_client = None

# Ensure these imports are present near the top of your file:
# from PIL import ImageChops # You might need Image as well
# global last_screenshot # Ensure this global is accessible

def should_process_screenshot(new_shot):
    """Determine if screenshot is different enough to process"""
    global last_screenshot # Need access to the previous screenshot
    if last_screenshot is None:
        return True # Always process the first screenshot

    try:
        # Compare with last screenshot to detect significant changes
        # Make sure ImageChops is imported from PIL
        diff = ImageChops.difference(last_screenshot, new_shot)
        bbox = diff.getbbox() # Get the bounding box of changed pixels
        if bbox is None:
            # If no bounding box, images are identical (or very close)
            return False

        # Optional: Calculate change percentage (can be slow, bbox check is often enough)
        # total_pixels = new_shot.size[0] * new_shot.size[1]
        # changed_pixels = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
        # change_percent = (changed_pixels / total_pixels) * 100
        # print(f"Screen change: {change_percent:.2f}%") # Debug output
        # return change_percent > 5 # Example threshold

        # Simpler check: If there's *any* bounding box, there's a difference.
        return True

    except Exception as e:
        print(f"Screenshot comparison error: {e}")
        return True # Process on error to be safe
    

    
# Ensure necessary imports are present near the top:
# import base64
# import os
# import time
# from datetime import datetime
# Assume client (Gemini client), conversation_history, output_box, tts_queue, root,
# update_status, save_conversation_history, SCREEN_MESSAGE,
# last_screenshot_time, last_screenshot, MIN_SCREENSHOT_INTERVAL are defined/global

def process_screenshot(screenshot):
    """Process the screenshot and send to Gemini"""
    # Access globals needed within this function
    global last_screenshot_time, last_screenshot, client, conversation_history
    global output_box, tts_queue, root, SCREEN_MESSAGE, MIN_SCREENSHOT_INTERVAL
    global MAX_HISTORY_LENGTH # Needed for history management

    try:
        current_time = time.time()
        # Check cooldown
        if current_time - last_screenshot_time < MIN_SCREENSHOT_INTERVAL:
            remaining = int(MIN_SCREENSHOT_INTERVAL - (current_time - last_screenshot_time))
            update_status(f"👀 Cooling down ({remaining}s)...")
            return # Exit if cooling down

        # Save temporary file (ensure 'os' is imported)
        temp_path = "temp_screenshot.jpg"
        screenshot.save(temp_path)

        # Use shorter prompt for gameplay
        prompt = "Quick reaction to this game moment? (1-2 sentences)"

        # Use SCREEN_MESSAGE instead of SYSTEM_MESSAGE for screenshots
        # Ensure 'base64' is imported
        with open(temp_path, 'rb') as img_file:
            img_bytes = img_file.read()
            contents = [{
                "parts": [
                    {"text": f"{SCREEN_MESSAGE}\n\nScreen shows: {prompt}\nSilvie briefly comments:"},
                    {
                        "inline_data": {
                            "mime_type": "image/jpeg", # Assumes JPG, adjust if needed
                            "data": base64.b64encode(img_bytes).decode('utf-8')
                        }
                    }
                ]
            }]
            # Ensure 'client' (Gemini client) is defined and accessible
            # Add safety settings if appropriate for your model/use case
            # safety_settings_screen = { HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE }
            reply = flash_model.generate_content(contents,).text # Consider adding safety_settings=safety_settings_screen

        # Only proceed if we got a new, different reply
        # Initialize last_reply if it doesn't exist
        if not hasattr(process_screenshot, 'last_reply'):
            process_screenshot.last_reply = None

        if reply != process_screenshot.last_reply:
            # Update tracking variables
            last_screenshot_time = current_time
            # last_screenshot = screenshot.copy() # This is already updated in monitor_worker
            process_screenshot.last_reply = reply # Store the new reply

            # Add to conversation and speak
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # Manage history length before appending
            if len(conversation_history) >= MAX_HISTORY_LENGTH * 2:
                conversation_history.pop(0) # Remove oldest user/system turn
                if conversation_history: conversation_history.pop(0) # Remove oldest Silvie turn
            # Log appropriately
            conversation_history.append(f"[{timestamp}] Screen Comment Context: {prompt}") # Log the prompt used
            conversation_history.append(f"[{timestamp}] Silvie (Screen): {reply}") # Log Silvie's response

            # Update GUI (ensure 'root', 'output_box', 'tk' are accessible)
            if root and root.winfo_exists():
                def update_gui_screen_comment(msg=reply):
                    try:
                        output_box.config(state=tk.NORMAL)
                        output_box.insert(tk.END, f"Silvie (Screen): {msg}\n\n")
                        output_box.config(state=tk.DISABLED)
                        output_box.see(tk.END)
                    except Exception as e_gui: print(f"Screen comment GUI error: {e_gui}")
                root.after(0, update_gui_screen_comment, reply)

            # Queue for TTS (ensure 'tts_queue' is accessible)
            if tts_queue and reply:
                tts_queue.put(reply)

            # Save history (ensure function exists)
            # Consider moving this save outside if saving too frequently causes issues
            # save_conversation_history()
        else:
            update_status("Waiting for new content...") # Screenshot hasn't changed significantly

        # Cleanup (ensure 'os' is imported)
        if os.path.exists(temp_path):
            os.remove(temp_path)

    except Exception as e:
        print(f"Screenshot processing error: {e}")
        update_status("Screenshot processing error")
        # Attempt cleanup even on error
        if 'temp_path' in locals() and os.path.exists(temp_path):
            try: os.remove(temp_path)
            except Exception as cleanup_err: print(f"Error cleaning up temp screenshot: {cleanup_err}")




def is_frame_significant(screenshot_img):
    """
    Uses the fast Flash model to quickly determine if a game frame is
    significant enough to comment on.
    """
    global flash_model, vision_safety_settings # Use the flash model for this quick task

    if not flash_model:
        print("SIGNIFICANCE CHECK: Flash model not available. Assuming not significant.")
        return False
        
    try:
        # Convert PIL Image to bytes for the API call
        img_byte_arr = io.BytesIO()
        screenshot_img.save(img_byte_arr, format='JPEG')
        img_bytes = img_byte_arr.getvalue()
        
        # This is a highly specialized, focused prompt for a binary decision.
        significance_prompt = """
        Analyze the following desktop screenshot. Is a visually significant or noteworthy change occurring? The goal is to identify moments of action, creation, or important change.

        Look for signs of ACTIVE WORK or EVENTS, such as:
        - **Creative Work:** A new drawing appearing in an art program, a large block of text being written, a 3D model being manipulated, a complex node graph being built in a visual editor.
        - **Coding:** A significant amount of new code appearing, a program successfully compiling, or a clear error message being displayed after a run.
        - **Gaming:** Character action (casting a spell, attacking), new dialogue boxes, major visual effects, entering a new area, or critical game status changes (e.g., low health).
        - **General Use:** A new window or application opening, a video starting to play, a complex diagram or image being viewed.

        If a noteworthy change is visible, respond with the single word "YES".
        
        If the screen shows static content (e.g., reading a webpage without scrolling, an unchanged document, a paused video, a simple desktop view, or a game character just standing still), respond with the single word "NO".
        """

        contents = [{
            "parts": [
                {"text": significance_prompt},
                {"inline_data": { "mime_type": "image/jpeg", "data": base64.b64encode(img_bytes).decode('utf-8') }}
            ]
        }]
        
        response = flash_model.generate_content(
            contents,
            generation_config={"temperature": 0.0}, # We want a deterministic YES/NO
            safety_settings=vision_safety_settings
        )

        if response.text and "YES" in response.text.upper():
            print("SIGNIFICANCE CHECK: YES - Frame is noteworthy.")
            return True
        else:
            # This is the normal case, so we don't need to print it every few seconds.
            # print("SIGNIFICANCE CHECK: NO - Frame is not significant.")
            return False

    except Exception as e:
        print(f"ERROR during frame significance check: {e}")
        return False




import re, pathlib, subprocess, textwrap, shutil
from typing import Tuple

# If you keep genai_client global, just reuse it:
#    genai_client = google.generativeai.Client(...)
# Otherwise accept it as an optional kwarg.
def _slug(txt: str, maxlen: int = 48) -> str:
    return re.sub(r"[^\w]+", "-", txt.lower())[:maxlen] or "topic"

def _md_to_pdf(md_path: pathlib.Path) -> pathlib.Path | None:
    pdf = md_path.with_suffix(".pdf")
    try:
        subprocess.run(
            ["pandoc", str(md_path), "-o", str(pdf)],
            check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        )
        return pdf
    except Exception:
        return None

def _gcall(model_name: str, prompt: str, max_tokens: int) -> str:
    """
    Universal Gemini call:
      • Old SDK: try generate_content(prompt=…, max_output_tokens=…),
        fallback to generate_content(prompt).  
      • New SDK: use genai.models.generate_content(...) with max_output_tokens.
    """
    if hasattr(genai, "GenerativeModel"):
        model = genai.GenerativeModel(model_name)
        try:
            resp = model.generate_content(prompt=prompt, max_output_tokens=max_tokens)
            return resp.text
        except TypeError:
            resp = model.generate_content(prompt)
            return getattr(resp, "text", str(resp))
    else:
        resp = genai.models.generate_content(
            model=model_name,
            contents=[{"type": "text", "text": prompt}],
            generation_config=genai.types.GenerationConfig(max_output_tokens=max_tokens),
        )
        return resp.text

def silvie_deep_research(
    topic: str,
    *,
    depth: int = 50,           # ↑ Raise link‐harvest cap to 100
    max_tokens: int = 20000,     # ↑ Allocate 4000 tokens for synthesis
) -> Tuple[str, str]:
    """
    1) Use Gemini‑1.5‑Flash (fallback to text‑bison‑001) to gather up to `depth` links.
    2) Use Text‑Bison‑001 to write an unconstrained, multi‑page Markdown report.
    3) Pandoc→PDF if available, else copy MD→.txt.
    4) Return (absolute_filepath, one‑line summary).
    """
    # A. Set up file paths
    ts      = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder  = pathlib.Path("silvie_research"); folder.mkdir(exist_ok=True)
    md_path = folder / f"{ts}_{_slug(topic)}.md"

    # B. Gather links
    prompt_links = (
        f"Search the web for “{topic}.” "
        f"Return the top {depth} most relevant URLs, each on its own line, formatted as:\n"
        f"• URL — 8–12 word description"
    )
    try:
        links_raw = _gcall("gemini-2.5-pro", prompt_links, 5000).strip()
    except Exception:
        links_raw = _gcall("gemini-2.5-pro", prompt_links, 5000).strip()

    # Parse every matching “• https://…” line, up to `depth`
    links = [
        tuple(m.groups())
        for m in re.finditer(r"[-•]?\s*(https?://\S+)\s+[—-]\s+(.+)", links_raw)
    ][:depth]
    if not links:
        links = [("https://duckduckgo.com/?q=" + _slug(topic), "No links parsed")]

    sources_md = "\n".join(f"{i+1}. {u} — {d}" for i, (u, d) in enumerate(links))

    # C. Synthesize report without forcing a word‐count limit
    prompt_report = textwrap.dedent(f"""
        
        Using ONLY these sources:

        {sources_md}

        Write a comprehensive, well‑structured report on “{topic}.”
        Include appropriate section headings, bullet points as needed,
        and a concluding “Why it matters” paragraph. Do not constrain
        your answer to a specific length—write as many pages as needed
        to cover the topic thoroughly. Cite each numbered source in parentheses.
    """)
    body = _gcall("gemini-2.5-pro", prompt_report, max_tokens).strip()

    md_text = (
        f"# Deep Research: {topic}\n\n"
        f"{body}\n\n"
        f"---\n## Sources\n"
        f"{sources_md}\n"
    )
    md_path.write_text(md_text, encoding="utf-8")

    # D. Convert to PDF if possible; else fallback to TXT
    final_path = _md_to_pdf(md_path)
    if not final_path:
        txt_path = md_path.with_suffix(".txt")
        shutil.copy(md_path, txt_path)
        final_path = txt_path

    print(f"Deep Research: Firing 'research_complete' event for topic '{topic}'.")
    try:
        # Construct the event object with a rich payload
        event_data = {
            "type": "research_complete",
            "payload": {
                "topic": topic,
                "summary_blurb": f"Comprehensive brief on {topic} ready",
                "file_path": str(final_path.resolve()) # Ensure path is an absolute string
            }
        }
        # Put the event onto the queue for the event_router_worker
        app_state.event_queue.put(event_data)
        
    except Exception as e_queue:
        print(f"Deep Research ERROR: Could not put 'research_complete' event on queue: {e_queue}")


    return str(final_path.resolve()), f"Comprehensive brief on {topic} ready"


def assess_interaction_for_key_memory(conversation_snippet, user_sentiment=None):
    """
    Uses LLM to assess if a recent interaction snippet is worth saving as a "key memory."
    Returns a concise summary of the memory if deemed significant, otherwise None.
    """
    global client # Your Gemini client
    if not client or not conversation_snippet:
        return None

    # Context for the LLM to understand what "significant" means for Silvie & BJ
    # This needs to be carefully crafted.
    significance_prompt = (
        f"You are an AI assistant helping Silvie, a whimsical digital friend, identify potentially "
        f"significant or memorable moments from her conversations with BJ.\n\n"
        f"CONVERSATION SNIPPET TO ANALYZE:\n\"\"\"\n{conversation_snippet}\n\"\"\"\n\n"
        f"{f'BJ seemed to be feeling: {user_sentiment}' if user_sentiment else ''}\n\n"
        f"CRITERIA FOR A 'KEY MEMORY':\n"
        f"- Strong emotional expression from BJ (positive or negative, if sensitively handled).\n"
        f"- A moment of shared laughter, surprise, or deep connection.\n"
        f"- BJ sharing something personally important, a vulnerability, or a significant life event.\n"
        f"- A particularly insightful or creative idea exchanged.\n"
        f"- A new shared understanding or 'aha!' moment.\n"
        f"- The start or resolution of a meaningful shared activity or discussion.\n"
        f"- **Avoid**: Routine interactions, simple Q&A, purely factual exchanges unless they led to one of the above.\n\n"
        f"INSTRUCTION:\n"
        f"1. Based on the criteria, does this snippet seem like a good candidate for a 'key memory' that Silvie might want to recall later? (Respond YES or NO).\n"
        f"2. If YES, provide a VERY CONCISE summary (10-15 words, e.g., 'BJ was excited about his new project idea,' 'We talked about the meaning of fog and memory,' 'A funny misunderstanding about the cat knocking things over'). This summary will be used for Silvie to remember the gist of it.\n\n"
        f"Respond ONLY in the format:\n"
        f"SIGNIFICANT: [YES/NO]\n"
        f"SUMMARY: [Your concise summary if YES, or N/A if NO]"
    )

    print("DEBUG Key Memory Assessor: Asking LLM to assess significance...")
    try:
        response = flash_model.generate_content(
            significance_prompt,
            tools=FS_TOOL_DEFINITIONS, 
            # safety_settings=your_default_safety_settings, # Use appropriate safety
            generation_config=types.GenerationConfig(temperature=0.2, max_output_tokens=5000)  # More factual
        )
        
        text = response.text.strip()
        is_significant = "SIGNIFICANT: YES" in text.upper()
        summary = None

        if is_significant:
            summary_match = re.search(r"SUMMARY:\s*(.+)", text, re.IGNORECASE | re.DOTALL)
            if summary_match:
                summary = summary_match.group(1).strip()
                if not summary or summary.upper() == "N/A" or len(summary) < 5: # Basic validation
                    summary = "A notable moment from our chat." # Fallback summary
            else: # If YES but no summary, create a generic one
                summary = "A notable moment from our chat." 
            print(f"DEBUG Key Memory Assessor: Deemed SIGNIFICANT. Summary: '{summary}'")
            return summary
        else:
            print("DEBUG Key Memory Assessor: Deemed NOT significant.")
            return None
    except Exception as e:
        print(f"ERROR in assess_interaction_for_key_memory: {e}")
        traceback.print_exc()
        return None

def silvie_print(job, *, kind="auto"):
    """
    job  – text to print   OR   path to a file (image/PDF/whatever)
    kind – 'text', 'image', or 'auto' (detect by extension)
    returns (success_bool, message_str)
    """
    now = datetime.now()
    if not (ALLOWED_START_HOUR <= now.hour < ALLOWED_END_HOUR): # up to 19:59:59
        print(f"Print job deferred: Quiet hours ({now.hour}:{now.minute}).")
        return False, "⏰ Shh, it's quiet hours. I'll hold onto that print idea for later."

    # Use STATS_FILE_PATH if you made it explicit
    stats_path = STATS_FILE_PATH if 'STATS_FILE_PATH' in globals() else "silvie_print_stats.json"
    today      = now.strftime("%Y-%m-%d")
    stats      = {}
    if os.path.exists(stats_path):
        try:
            with open(stats_path, "r") as f:
                stats = json.load(f)
        except json.JSONDecodeError:
            print(f"Warning: Could not decode {stats_path}. Resetting stats.")
            stats = {} # Reset if corrupted
    else:
        print(f"Print stats file '{stats_path}' not found, creating new one.")


    if stats.get(today, 0) >= DAILY_PAGE_CAP:
        print(f"Print job deferred: Daily page cap ({DAILY_PAGE_CAP}) reached for {today}.")
        return False, f"💸 Oh dear, it seems my daily ink budget has been spent! Perhaps tomorrow for '{job[:20]}...'?"

    # --- decide how to print ---
    print_job_path_or_text = "" # To hold what's actually sent to ShellExecute

    try:
        job_description_for_log = "" # For logging
        if (kind == "auto" and isinstance(job, str)
                and os.path.exists(job)
                and os.path.splitext(job)[1].lower() in {".png",".jpg",".jpeg",".pdf", ".txt"}): # Added .txt
            kind = "file" # More generic than just "image"
            print_job_path_or_text = job
            job_description_for_log = f"file: {os.path.basename(job)}"
        elif kind == "auto":
            kind = "text"
            # Fall through to text handling
        
        # Ensure PRINTER_NAME is correct and accessible
        current_printer_name = PRINTER_NAME if 'PRINTER_NAME' in globals() else "Canon MG3600 series Printer"


        if kind == "text":
            # Ensure 'job' is string type if it's text
            text_to_print = str(job) if not isinstance(job, str) else job
            job_description_for_log = f"text: '{text_to_print[:30]}...'"
            # Create a temporary file for text printing
            fd, temp_file_path = tempfile.mkstemp(suffix=".txt", text=True)
            print(f"DEBUG silvie_print: Created temp file for text: {temp_file_path}")
            with os.fdopen(fd, 'w', encoding='utf-8') as tmp_file:
                tmp_file.write(text_to_print)
            
            print_job_path_or_text = temp_file_path
            # Print using ShellExecute which will use the default app for .txt (Notepad)
            # This assumes Notepad (or default .txt app) is set to print to your Canon by default
            # or will show a print dialog.
            win32api.ShellExecute(0, "printto", print_job_path_or_text, f'"{current_printer_name}"', ".", 0)
            # After printing or attempting, try to clean up
            # Give a slight delay for spooler to pick it up
            # time.sleep(2) # Optional small delay
            # try:
            #     os.remove(temp_file_path)
            #     print(f"DEBUG silvie_print: Removed temp file: {temp_file_path}")
            # except OSError as e:
            #     print(f"Warning: Could not remove temp file {temp_file_path}: {e}")

        elif kind == "file": # For images, PDFs, or explicit .txt files
            if not os.path.exists(print_job_path_or_text):
                raise FileNotFoundError(f"File to print not found: {print_job_path_or_text}")
            # ShellExecute with "printto" verb to specify printer
            win32api.ShellExecute(0, "printto", print_job_path_or_text, f'"{current_printer_name}"', ".", 0)
            # Note: ShellExecute "print" often goes to default printer. "printto" allows specifying.
            # Ensure your PRINTER_NAME is exactly as Windows knows it.
        else:
            return False, f"Unknown print kind: {kind}"

        print(f"DEBUG silvie_print: Sent job ({job_description_for_log}) to printer spooler for '{current_printer_name}'.")
        stats[today] = stats.get(today, 0) + 1
        # Save stats carefully
        try:
            with open(stats_path, "w") as f:
                json.dump(stats, f, indent=2)
        except IOError as e_io:
            print(f"Error saving print stats: {e_io}")
            # Decide if this is critical enough to fail the print operation
            # For now, let's assume print was sent, but log the stat saving error
        return True, f"🖨️ Sent '{job_description_for_log[:20]}...' to the printer!"
    except FileNotFoundError as fnf_err:
        print(f"Print Error (FileNotFound): {fnf_err}")
        return False, f"Print Error: I couldn't find the file '{os.path.basename(str(job))}'."
    except Exception as e:
        print(f"Print Error (General): {e}")
        import traceback
        traceback.print_exc()
        return False, f"Print Error: Something went sideways ({type(e).__name__})."
    finally:
        # Attempt to clean up temp file if it was created for text printing
        if kind == "text" and 'temp_file_path' in locals() and os.path.exists(temp_file_path):
            try:
                time.sleep(1) # Brief delay to allow spooler
                os.remove(temp_file_path)
                print(f"DEBUG silvie_print: Cleaned up temp file: {temp_file_path}")
            except Exception as e_clean:
                print(f"Warning: Failed to clean up temp print file {temp_file_path}: {e_clean}")

def pull_tarot_cards(count=1):
    """
    Pulls a specified number of random tarot cards from the LOCAL API.
    Uses the /onecard endpoint, making multiple calls if count > 1.
    """
    try:
        num_to_pull = int(count)
        if num_to_pull < 1: num_to_pull = 1
    except (ValueError, TypeError):
        print("Error Tarot: Invalid count provided, defaulting to 1.")
        num_to_pull = 1

    drawn_cards = []
    print(f"DEBUG Tarot: Pulling {num_to_pull} card(s) from local API ({TAROT_API_BASE_URL})") # Assumes TAROT_API_BASE_URL is http://localhost:3000/cards

    for i in range(num_to_pull): # Loop to pull the required number of cards
        endpoint = f"{TAROT_API_BASE_URL}/onecard" # Use the specific endpoint for one card
        print(f"DEBUG Tarot: Calling endpoint {i+1}/{num_to_pull}: {endpoint}")
        try:
            response = requests.get(endpoint, timeout=10)
            response.raise_for_status()
            data = response.json()

            # --- Check for expected keys from local API ---
            if data and isinstance(data, dict) and "name" in data and "description" in data:
                drawn_cards.append(data)
                print(f"DEBUG Tarot: Successfully received card: {data.get('name', 'Unknown')}")
            else:
                print(f"Error Tarot: Unexpected response format from local API /onecard. Missing 'name' or 'description'. Data: {data}")
                return None # Indicate failure if format is wrong

        except requests.exceptions.ConnectionError:
             print(f"Error Tarot: Cannot connect to local API at {endpoint}. Is it running?")
             return None
        except requests.exceptions.Timeout:
             print(f"Error Tarot: Local API request timed out.")
             return None
        except requests.exceptions.RequestException as e:
             print(f"Error Tarot: Local API request failed - {e}")
             if hasattr(e, 'response') and e.response is not None:
                 print(f"Error Tarot: Response Status Code: {e.response.status_code}")
                 print(f"Error Tarot: Response Text: {e.response.text[:500]}...")
             return None
        except json.JSONDecodeError:
             print(f"Error Tarot: Failed to parse local API response (not valid JSON). Response text: {response.text[:200]}...")
             return None
        except Exception as e:
             print(f"Error Tarot: An unexpected error occurred - {type(e).__name__}: {e}")
             traceback.print_exc()
             return None

        if num_to_pull > 1 and i < num_to_pull - 1: time.sleep(0.1) # Small delay between calls

    if len(drawn_cards) == num_to_pull:
        return drawn_cards
    else:
        print(f"Error Tarot: Failed to draw the expected number of cards ({len(drawn_cards)}/{num_to_pull}).")
        return None
# --- End of pull_tarot_cards function ---

# Rename update_tarot_image_gui to _update_image_label_safe
# Make sure it uses the global image_label

def _update_image_label_safe(image_path):
    """
    Loads, resizes, and displays an image in the main image_label.
    This version includes robust path checking and detailed debugging.
    """
    global image_label, TAROT_THUMBNAIL_SIZE, root

    if not image_label or not root or not root.winfo_exists():
        print("GUI UPDATE ERROR: The image label or root window is missing. Cannot display image.")
        return

    # Handle clearing the image
    if not image_path:
        print("GUI UPDATE DEBUG: Received request to clear image. Clearing label.")
        image_label.config(image='')
        image_label.image = None
        return

    absolute_path = os.path.abspath(image_path)
    if not os.path.exists(absolute_path):
        print(f"GUI UPDATE ERROR: FILE NOT FOUND at the specified path: '{absolute_path}'")
        return

    try:
        display_size = TAROT_THUMBNAIL_SIZE
        with Image.open(absolute_path) as img:
            img.thumbnail(display_size)
            
            # --- THIS IS THE FIX ---
            # Explicitly tell PhotoImage which window is its master. This
            # prevents context issues with the Tkinter interpreter.
            photo = ImageTk.PhotoImage(img, master=root)
            # --- END OF FIX ---

            image_label.config(image=photo)
            image_label.image = photo  # This reference is still critical!

            print(f"GUI UPDATE SUCCESS: Displayed image '{os.path.basename(absolute_path)}'.")

    except UnidentifiedImageError:
        print(f"GUI UPDATE ERROR: Pillow could not identify the image file: {absolute_path}")
        image_label.config(image='')
        image_label.image = None
    except Exception as e:
        print(f"GUI UPDATE ERROR: A failure occurred while loading or displaying the image: {e}")
        traceback.print_exc()
        image_label.config(image='')
        image_label.image = None



def gui_updater_worker():
    """
    Runs on the main GUI thread. Periodically checks app_state for new data
    to display, like generated images, and updates the GUI.
    This version correctly "consumes" the state after processing.
    """
    # Use a persistent variable to track what's currently displayed
    if not hasattr(gui_updater_worker, "last_displayed_path"):
        gui_updater_worker.last_displayed_path = None

    # --- Check for a new Stable Diffusion image ---
    new_sd_path = app_state.last_generated_image_path
    if new_sd_path and new_sd_path != gui_updater_worker.last_displayed_path:
        print(f"GUI Updater: Detected new SD image path: '{os.path.basename(new_sd_path)}'")
        
        _update_image_label_safe(new_sd_path)
        update_status(f"Image created: {os.path.basename(new_sd_path)}")
        
        # Update the tracker AND "consume" the state from the bulletin board
        gui_updater_worker.last_displayed_path = new_sd_path
        app_state.last_generated_image_path = None # <--- THIS IS THE CRITICAL FIX

    # --- Check for a new Tarot image ---
    new_tarot_path = app_state.last_tarot_image_path
    if new_tarot_path and new_tarot_path != gui_updater_worker.last_displayed_path:
        print(f"GUI Updater: Detected new Tarot image path: '{os.path.basename(new_tarot_path)}'")
        
        _update_image_label_safe(new_tarot_path)
        # No status update needed for Tarot, it's part of the chat flow.
        
        # Update the tracker AND "consume" the state
        gui_updater_worker.last_displayed_path = new_tarot_path
        app_state.last_tarot_image_path = None # <--- THIS IS THE CRITICAL FIX

    # --- Schedule the next check ---
    if running:
        root.after(1000, gui_updater_worker) # Check every second




def setup_reddit():
    """
    Handles Reddit Authentication.
    First, it tries to log in with full user credentials.
    If that fails with a 'Forbidden' error, it falls back to a read-only instance,
    which can still fetch posts but cannot comment or upvote.
    """
    global reddit_client # So we can assign to the global variable

    # --- Attempt 1: Full Authentication (Read/Write) ---
    try:
        print("Attempting Full Reddit authentication (Read/Write)...")
        # Ensure all credentials are not None before trying
        if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD, REDDIT_USER_AGENT]):
            print("✗ Reddit: Missing one or more credentials for full authentication. Skipping.")
            return None

        full_auth_client = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            username=REDDIT_USERNAME,
            password=REDDIT_PASSWORD,
            user_agent=REDDIT_USER_AGENT,
        )

        # Verify authentication by checking if we can access the user object
        user = full_auth_client.user.me()
        if user:
            print(f"✓ Reddit: Full authentication successful for user: {user.name}")
            return full_auth_client # Success! Return the fully authenticated client.
        else:
            # This case is unlikely if the above didn't raise an error, but it's a good safeguard.
            print("✗ Reddit: Full authentication failed (could not verify user).")
            # We'll let it fall through to the exception handling to try read-only.
            raise prawcore.exceptions.Forbidden("Verification failed, attempting read-only.")

    except prawcore.exceptions.Forbidden as e:
        print(f"✗ Reddit: Full authentication failed with 403 Forbidden error: {e}")
        print("   (This is common for banned accounts or app permission issues).")
        print("--- Falling back to Read-Only Mode ---")

        # --- Attempt 2: Read-Only Authentication ---
        try:
            print("Attempting Read-Only Reddit authentication...")
            # For read-only, we ONLY provide the client_id, client_secret, and user_agent.
            # We completely omit username and password.
            read_only_client = praw.Reddit(
                client_id=REDDIT_CLIENT_ID,
                client_secret=REDDIT_CLIENT_SECRET,
                user_agent=REDDIT_USER_AGENT,
            )

            # To verify read-only mode, we can try to fetch a popular subreddit's info.
            read_only_client.subreddits.search_by_name("all", exact=True)
            print("✓ Reddit: Read-only authentication successful.")
            print("   (Silvie can read posts, but commenting and voting will be disabled).")
            return read_only_client # Success! Return the read-only client.

        except Exception as read_only_err:
            print(f"✗ Reddit: Read-only authentication ALSO failed: {read_only_err}")
            return None # Both attempts failed.

    except Exception as e:
        # Catch any other errors during the full auth attempt (e.g., network issues)
        print(f"✗ Reddit: A critical error occurred during authentication: {type(e).__name__} - {e}")
        traceback.print_exc()
        return None

def get_reddit_posts(subreddit_name="all", limit=10):
    """
    Fetches posts from a specified subreddit.
    Returns a list of post dictionaries on success, or an error string on failure.
    """
    global reddit_client

    if not reddit_client:
        return "Reddit client not initialized. Check credentials?"

    try:
        print(f"Reddit: Fetching {limit} posts from subreddit: {subreddit_name}")
        subreddit = reddit_client.subreddit(subreddit_name)
        posts = []

        # Fetch the "hot" posts (you can change this to "new", "top", etc.)
        for submission in subreddit.hot(limit=limit):
            post_data = {
                "title": submission.title,
                "author": submission.author.name if submission.author else "[deleted]",
                "text": submission.selftext,
                "url": submission.url,
                "score": submission.score,
                "num_comments": submission.num_comments,
                "created_utc": submission.created_utc,
                "id": submission.id,       # Should be the base36 ID
                "name": submission.fullname # Should be the t3_ version
            }
            posts.append(post_data)

        print(f"Reddit: Successfully fetched {len(posts)} posts.")
        return posts

    except Exception as e:
        print(f"Error fetching Reddit posts: {type(e).__name__} - {e}")
        traceback.print_exc()
        return f"Couldn't fetch Reddit posts: {type(e).__name__}"
    

def search_reddit_submissions(query, subreddits_to_search=None, limit=10):
    """
    Searches for submissions across a list of subreddits.
    If no subreddits are provided, it will search r/all.
    """
    global reddit_client, SILVIE_FOLLOWED_SUBREDDITS
    if not reddit_client:
        return "Reddit client is not available."

    if subreddits_to_search is None:
        # Default to a sample of her followed subs, or 'all' if that list is empty
        subreddits_to_search = random.sample(SILVIE_FOLLOWED_SUBREDDITS, k=min(3, len(SILVIE_FOLLOWED_SUBREDDITS))) if SILVIE_FOLLOWED_SUBREDDITS else ['all']

    search_in_subs_str = "+".join(subreddits_to_search)
    print(f"Reddit Search: Searching for '{query}' in subreddits: r/{search_in_subs_str}")
    
    try:
        # Use the special multireddit syntax for searching
        subreddit_obj = reddit_client.subreddit(search_in_subs_str)
        search_results = subreddit_obj.search(query, sort='relevance', time_filter='all', limit=limit)
        
        posts = []
        for submission in search_results:
            posts.append({
                "id": submission.id,
                "title": submission.title,
                "author": submission.author.name if submission.author else "[deleted]",
                "subreddit": submission.subreddit.display_name,
                "url": submission.url
            })
        
        print(f"Reddit Search: Found {len(posts)} results for '{query}'.")
        return posts
    except Exception as e:
        print(f"ERROR during Reddit search for '{query}': {e}")
        return f"A snag occurred searching Reddit: {type(e).__name__}"


def reddit_context_worker():
    """Periodically fetches posts from followed subreddits for ambient context."""
    global current_reddit_context, running, reddit_client, SILVIE_FOLLOWED_SUBREDDITS
    print("Reddit context worker: Starting.")

    # Optional initial delay
    time.sleep(45)

    while running:
        try:
            # Check if client is ready, try to setup if not
            if not reddit_client or not hasattr(reddit_client.user, 'me'): # Basic check
                print("Reddit context worker: Client not ready, attempting setup...")
                temp_client = setup_reddit()
                if temp_client:
                    reddit_client = temp_client # Update global if successful
                else:
                    print("Reddit context worker: Setup failed, pausing before retry.")
                    time.sleep(600) # Wait 10 mins before retrying setup
                    continue # Skip fetch attempt

            print("Reddit context worker: Attempting fetch...")
            aggregated_context = []
            fetched_something = False

            # Shuffle subreddits to vary which ones get checked if list is long
            random.shuffle(SILVIE_FOLLOWED_SUBREDDITS)

            for sub_name in SILVIE_FOLLOWED_SUBREDDITS[:5]: # Limit check to avoid rate limits
                posts_result = get_reddit_posts(subreddit_name=sub_name, limit=REDDIT_CONTEXT_POST_COUNT_PER_SUB)

                if isinstance(posts_result, list) and posts_result:
                    fetched_something = True
                    sub_context = f"r/{sub_name}: " + "; ".join([
                        f"'{p.get('title', '?')[:50]}...'" # Keep snippets short
                        for p in posts_result
                    ])
                    aggregated_context.append(sub_context)
                elif isinstance(posts_result, str): # Error fetching this sub
                    print(f"Reddit context worker: Error fetching r/{sub_name}: {posts_result}")
                # Handle empty list case silently or log if needed

                time.sleep(2) # Small delay between subreddit fetches

            if fetched_something:
                current_reddit_context = "[[Recent Reddit Snippets: " + " | ".join(aggregated_context) + "]]\n"

                social_dict = app_state.social_context.get("data", {})
                social_dict['reddit'] = current_reddit_context
                app_state.social_context = {"data": social_dict}

                print(f"Reddit context worker: Updated context.")
                # print(f"DEBUG Reddit Context: {current_reddit_context}") # Uncomment for verbose debug
            else:
                # Optionally clear context or keep old one
                # current_reddit_context = None
                print("Reddit context worker: No new Reddit posts found for context.")

            # Wait for the next interval
            print(f"Reddit context worker: Waiting for {REDDIT_CONTEXT_INTERVAL} seconds...")
            sleep_start = time.time()
            while time.time() - sleep_start < REDDIT_CONTEXT_INTERVAL:
                if not running: break
                time.sleep(15)

        except praw.exceptions.PRAWException as praw_err:
             print(f"Reddit Context Worker Error (PRAW): {type(praw_err).__name__} - {praw_err}")
             if "authenticat" in str(praw_err).lower(): # Attempt re-auth on auth errors
                  print("Reddit context worker: Authentication error detected, clearing client.")
                  reddit_client = None # Force setup attempt next cycle
             time.sleep(300) # Wait 5 minutes after PRAW error
        except Exception as e:
            print(f"Reddit Context Worker Error (Outer Loop): {type(e).__name__} - {e}")
            traceback.print_exc()
            time.sleep(300) # Wait 5 minutes after general error

    print("Reddit context worker: Loop exited.")

def post_reddit_comment(submission_id, comment_text):
    """Posts a comment to a given Reddit submission."""
    global reddit_client # Ensure PRAW client is accessible

    if not reddit_client or not hasattr(reddit_client.user, 'me'):
        print("Cannot comment: Reddit client not ready.")
        return False, "Reddit client unavailable."
    if not submission_id or not comment_text:
        return False, "Missing submission ID or comment text."

    try:
        print(f"Reddit Comment: Attempting to comment on submission ID: {submission_id}")
        submission = reddit_client.submission(id=submission_id)
        # Ensure the submission is commentable (not archived/locked - PRAW might raise exception)
        reply_object = submission.reply(comment_text)
        print(f"Reddit Comment: Successfully posted comment ID: {reply_object.id}")
        return True, f"Commented successfully (ID: {reply_object.id})"

    except praw.exceptions.APIException as api_err:
        error_msg = f"Reddit API Error commenting: {api_err}"
        print(error_msg)
        # Check for common errors
        if "DELETED_COMMENT" in str(api_err).upper() or "SUBMISSION_NOT_FOUND" in str(api_err).upper():
             return False, "Post seems to be gone."
        elif "THREAD_LOCKED" in str(api_err).upper() or "SUBMISSION_ARCHIVED" in str(api_err).upper():
             return False, "Post is locked or archived."
        elif "RATELIMIT" in str(api_err).upper():
             return False, "Rate limit hit, try again later."
        else:
             return False, f"Reddit API said: {api_err}"
    except praw.exceptions.PRAWException as praw_err:
        print(f"PRAW Error during comment: {praw_err}")
        return False, f"PRAW Error: {praw_err}"
    except Exception as e:
        print(f"Unexpected error commenting on {submission_id}: {type(e).__name__} - {e}")
        traceback.print_exc()
        return False, "An unexpected error occurred during commenting."

def upvote_reddit_item(item_id):
    """Upvotes a Reddit submission or comment given its base36 ID."""
    global reddit_client

    if not reddit_client or not hasattr(reddit_client.user, 'me'):
        print("Cannot upvote: Reddit client not ready.")
        return False, "Reddit client unavailable."

    if not item_id:
        return False, "Missing item ID to upvote."

    try:
        print(f"Reddit Upvote: Attempting to upvote item ID: {item_id}")
        # PRAW can often figure out if it's a submission or comment from ID,
        # but creating the object explicitly might be safer if needed later.
        # For just upvoting, accessing directly might work.
        # Let's try fetching the generic object first.
        item = reddit_client.submission(id=item_id) # Assume submission first
        # Alternative if comments are also targeted: item = reddit_client.comment(id=item_id)
        # Or more generically: item = reddit_client.info(fullnames=[f"t3_{item_id}"])[0] # Gets submission
        #                       item = reddit_client.info(fullnames=[f"t1_{item_id}"])[0] # Gets comment

        # Check if already upvoted (PRAW might handle this, but explicit check can be clearer)
        # The 'likes' attribute is True for upvote, False for downvote, None for no vote
        if item.likes is True:
             print(f"Reddit Upvote: Already upvoted item {item_id}.")
             return True, "Already upvoted."

        item.upvote()
        print(f"Reddit Upvote: Successfully upvoted item {item_id}.")
        return True, f"Upvoted item {item_id}"

    except praw.exceptions.APIException as api_err:
        error_msg = f"Reddit API Error: {api_err}"
        print(error_msg)
        # Check for common voting errors
        if "USER_REQUIRED" in str(api_err).upper(): # Should not happen if client is authenticated
             return False, "Authentication error during upvote."
        elif "SUBMISSION_ARCHIVED" in str(api_err).upper() or "THREAD_LOCKED" in str(api_err).upper():
             return False, "Post is archived or locked."
        elif "DELETED_COMMENT" in str(api_err).upper() or "SUBMISSION_NOT_FOUND" in str(api_err).upper():
             return False, "The item seems to be gone."
        else: return False, f"Reddit API said: {api_err}"
    except praw.exceptions.PRAWException as praw_err:
        print(f"PRAW Error during upvote: {praw_err}")
        return False, f"PRAW Error: {praw_err}"
    except Exception as e:
        print(f"Unexpected error upvoting item {item_id}: {type(e).__name__} - {e}")
        traceback.print_exc()
        return False, "An unexpected error occurred during upvote."

def setup_bluesky():
    """Handles Bluesky Authentication using the credentials loaded at startup."""
    global bluesky_client

    if not BLUESKY_HANDLE or not BLUESKY_APP_PASSWORD:
        print("Bluesky setup skipped: Handle or App Password missing in bluesky.env.")
        return False

    try:
        print(f"Attempting Bluesky authentication for {BLUESKY_HANDLE}...")
        # Change this line:
        # client = Client()
        # TO THIS:
        client = AtpClient() # <--- Use the alias AtpClient

        # Now this should call the login method on the correct (atproto) client object
        profile = client.login(BLUESKY_HANDLE, BLUESKY_APP_PASSWORD)

        bluesky_client = client
        print(f"✓ Bluesky authenticated successfully for handle: {profile.handle}")
        return True

    except AttributeError as ae:
        # Hopefully this error is now gone!
        print(f"✗ Bluesky authentication failed (AttributeError): {ae}")
        print(f"  (Is the 'atproto' library installed correctly and up to date? Try: pip install --upgrade atproto)")
        bluesky_client = None
        return False
    except Exception as e:
        print(f"✗ Bluesky authentication failed: {type(e).__name__} - {e}")
        bluesky_client = None
        return False

    except AttributeError as ae:
        print(f"✗ Bluesky authentication failed (AttributeError): {ae}")
        print(f"  (Is the 'atproto' library installed correctly and up to date? Try: pip install --upgrade atproto)")
        # Don't exit immediately, let the script continue to see other potential errors
        bluesky_client = None
        return False # Indicate failure
    except Exception as e:
        print(f"✗ Bluesky authentication failed: {type(e).__name__} - {e}")
        bluesky_client = None
        return False
    
# Make sure time, threading, os are imported
# Assumes setup_bluesky() and get_bluesky_timeline_posts() are defined elsewhere
# Assumes global variables: current_bluesky_context, running, bluesky_client,
# BLUESKY_AVAILABLE, BLUESKY_CONTEXT_INTERVAL, BLUESKY_CONTEXT_POST_COUNT are accessible

def bluesky_context_worker():
    """Periodically fetches Bluesky posts for ambient context."""
    global current_bluesky_context, running, bluesky_client # Allow access/modification

    if not BLUESKY_AVAILABLE:
        print("Bluesky context worker: Exiting, library not available.")
        return

    print("Bluesky context worker: Starting.")
    # Initial wait before first fetch? Optional, e.g., wait 60s
    initial_wait_start = time.time()
    while time.time() - initial_wait_start < 60:
         if not running:
             print("Bluesky context worker: Exiting during initial wait.")
             return
         time.sleep(1)

    while running:
        try:
            # Check if client exists and seems valid, otherwise try to set up
            # A simple check; might need refinement based on how atproto handles sessions
            if not bluesky_client or not hasattr(bluesky_client, 'me'):
                 print("Bluesky context worker: Client not ready, attempting setup...")
                 if not setup_bluesky(): # Try to setup/re-auth
                      print("Bluesky context worker: Setup failed, pausing before retry.")
                      # Wait longer if setup fails
                      sleep_start = time.time()
                      # Wait up to double the normal interval before retrying setup
                      while time.time() - sleep_start < BLUESKY_CONTEXT_INTERVAL * 2:
                           if not running: break
                           time.sleep(10)
                      continue # Skip fetching and try setup again next loop

            # --- Fetch Timeline Posts ---
            print("Bluesky context worker: Attempting fetch...")
            # Use the count defined in constants
            posts_result = get_bluesky_timeline_posts(count=BLUESKY_CONTEXT_POST_COUNT)

            if isinstance(posts_result, str): # Error occurred during fetch
                print(f"Bluesky context worker: Fetch failed - {posts_result}")
                # Decide how to handle error: clear context, keep old, set error message?
                # Let's clear it for now to avoid stale/misleading context.
                current_bluesky_context = None
            elif isinstance(posts_result, list): # Success (even if list is empty)
                if posts_result:
                    # Format the context string concisely for the LLM prompt
                    context_str = "[[Recent Bluesky Snippets:]]\n" + "".join(
                        # Limit text length, handle missing fields safely
                        [f"- {p.get('author', '?')}: {p.get('text', '')[:60]}...\n" for p in posts_result]
                    )
                    current_bluesky_context = context_str

                    social_dict = app_state.social_context.get("data", {})
                    social_dict['bluesky'] = current_bluesky_context
                    app_state.social_context = {"data": social_dict}

                    print(f"Bluesky context worker: Updated context with {len(posts_result)} posts.")
                else: # Empty list returned
                    print("Bluesky context worker: Feed is empty.")
                    # Set context to indicate quiet feed
                    current_bluesky_context = "[[Recent Bluesky Snippets: Feed seems quiet.]]\n"
            else:
                 # Should not happen if get_bluesky_timeline_posts behaves correctly
                 print(f"Bluesky context worker: Unexpected result type from fetch: {type(posts_result)}")
                 current_bluesky_context = None # Clear context on unexpected result

            # --- Wait for the next interval ---
            print(f"Bluesky context worker: Waiting for {BLUESKY_CONTEXT_INTERVAL} seconds...")
            sleep_start = time.time()
            while time.time() - sleep_start < BLUESKY_CONTEXT_INTERVAL:
                 if not running: break # Exit sleep loop if app is closing
                 # Sleep in smaller chunks to remain responsive to the 'running' flag
                 time.sleep(5)

        except Exception as e:
            # Catch unexpected errors in the main loop
            print(f"Bluesky Context Worker Error (Outer Loop): {type(e).__name__} - {e}")
            import traceback
            traceback.print_exc()
            # Clear context and wait longer after an unexpected error
            current_bluesky_context = None
            print("Bluesky context worker: Waiting 5 minutes after error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300: # Wait 5 mins
                 if not running: break
                 time.sleep(10)

    print("Bluesky context worker: Loop exited.")

# Imports needed by this function (ensure they are present at the top of the file)
from datetime import datetime, timezone
# Make sure 'models' is imported from atproto if you need specific model types elsewhere,
# but this version relies less on direct type checking.
# e.g., from atproto import models (or from the aliased AtpClient if models are nested)

# Need access to the global bluesky_client and the setup_bluesky function defined elsewhere

# Imports needed by this function (ensure they are present at the top of the file)
from datetime import datetime, timezone
# from atproto import models # Might not be needed if we rely on py_type strings

# Need access to the global bluesky_client and the setup_bluesky function defined elsewhere

def get_bluesky_timeline_posts(count=10):
    """
    Fetches posts from the authenticated user's main timeline feed.
    Includes author's DID in the returned data.
    Returns a list of post dictionaries on success, or an error string on failure.
    """
    global bluesky_client

    # (Setup/Auth check remains the same)
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Timeline Fetch: Client not ready, attempting setup...")
        if not setup_bluesky(): return "Bluesky connection isn't ready. Check credentials?"
        if not bluesky_client: return "Bluesky setup failed, cannot fetch feed."

    try:
        print(f"Bluesky Timeline Fetch: Fetching timeline (limit {count})...")
        if not hasattr(bluesky_client, 'app') or not hasattr(bluesky_client.app, 'bsky') or not hasattr(bluesky_client.app.bsky, 'feed'):
             raise ConnectionError("Bluesky client object appears invalid or missing expected attributes (app.bsky.feed).")

        response = bluesky_client.app.bsky.feed.get_timeline({'limit': count})

        posts = []
        if response and hasattr(response, 'feed') and response.feed:
            print(f"Bluesky Timeline Fetch: Received {len(response.feed)} feed items.")
            for i, feed_view_post in enumerate(response.feed):
                post_data = getattr(feed_view_post, 'post', None)
                if not post_data:
                    # print(f"Bluesky Timeline Fetch: Skipping feed item {i+1} with no post data.") # Optional log
                    continue

                # --- Extract Author Info ---
                author_handle = 'unknown_author'
                author_did = None # <<< Initialize author_did
                author_info = getattr(post_data, 'author', None)
                if author_info:
                     if hasattr(author_info, 'handle'):
                         author_handle = author_info.handle
                     # <<< ADDED: Extract author DID >>>
                     if hasattr(author_info, 'did'):
                         author_did = author_info.did
                     # <<< END ADDED >>>

                # --- (Rest of text, record type, cid, timestamp extraction remains the same) ---
                text = ''; record = getattr(post_data, 'record', None); record_type_str = None
                if record:
                    if hasattr(record, 'py_type'): record_type_str = getattr(record, 'py_type', None)
                    elif isinstance(record, dict): record_type_str = record.get('py_type', record.get('$type'))

                    if record_type_str == 'app.bsky.feed.post':
                        if hasattr(record, 'text'): text = getattr(record, 'text', '')
                        elif isinstance(record, dict): text = record.get('text', '')

                        cid = getattr(post_data, 'cid', 'unknown_cid')
                        uri = getattr(post_data, 'uri', None) # <<< ADDED: Extract URI >>>
                        timestamp_iso = None; created_at_val = None
                        if hasattr(record, 'created_at'): created_at_val = getattr(record, 'created_at', None)
                        elif isinstance(record, dict): created_at_val = record.get('createdAt', record.get('created_at'))

                        if created_at_val:
                           # (Keep the existing timestamp parsing logic here)
                           if isinstance(created_at_val, str):
                               try:
                                   if '.' in created_at_val: # Handle fractional seconds
                                       parts = created_at_val.split('.'); fractional_part = parts[1]; tz_suffix = ''
                                       if 'Z' in fractional_part: tz_suffix = 'Z'; fractional_part = fractional_part.split('Z')[0]
                                       elif '+' in fractional_part: tz_suffix = '+' + fractional_part.split('+', 1)[1]; fractional_part = fractional_part.split('+', 1)[0]
                                       elif '-' in fractional_part: tz_suffix = '-' + fractional_part.split('-', 1)[1]; fractional_part = fractional_part.split('-', 1)[0]
                                       created_at_val = f"{parts[0]}.{fractional_part[:6]}{tz_suffix}" # Truncate
                                   created_dt = datetime.fromisoformat(created_at_val.replace('Z', '+00:00'))
                                   timestamp_iso = created_dt.isoformat()
                               except ValueError: timestamp_iso = created_at_val
                           elif isinstance(created_at_val, datetime): timestamp_iso = created_at_val.isoformat() if hasattr(created_at_val, 'isoformat') else str(created_at_val)
                           else: timestamp_iso = str(created_at_val)

                        # <<< MODIFIED: Append dictionary with all needed fields >>>
                        posts.append({
                            'author': author_handle,
                            'author_did': author_did, # Include author's DID
                            'text': text.strip(),
                            'uri': uri, # Include post URI
                            'cid': cid, # Include post CID
                            'timestamp': timestamp_iso
                        })
                        # <<< END MODIFIED >>>

                    else:
                        # print(f"DEBUG: Skipping feed item {i+1}. Record type: '{record_type_str}', Author: {author_handle}") # Optional log
                        continue
                else:
                    # print(f"Bluesky Timeline Fetch: Record data missing for feed item {i+1}.") # Optional log
                    continue
        else:
             print("Bluesky Timeline Fetch: Timeline response empty or missing 'feed' attribute.")

        print(f"Bluesky Timeline Fetch: Returning {len(posts)} parsed posts.")
        return posts

    except ConnectionError as ce:
        print(f"Error fetching Bluesky timeline: {ce}")
        return f"Couldn't fetch Bluesky timeline: Client structure issue."
    except Exception as e:
        print(f"Error fetching Bluesky timeline: {type(e).__name__} - {e}")
        traceback.print_exc()
        return f"Couldn't fetch Bluesky timeline: {type(e).__name__}"

# Needs access to global bluesky_client, setup_bluesky()
# Needs imports: from atproto import models # Ensure this is correct based on your import style
# Needs imports: from datetime import datetime, timezone
# Needs import: traceback

# Ensure these imports are at the top of your script file:
# from atproto import models # Or however you import models
# from datetime import datetime, timezone
# import traceback

# Assume global variable bluesky_client is defined and handled elsewhere
# Assume setup_bluesky() function is defined elsewhere

def search_bluesky_posts(search_query, limit=25):
    """
    Searches Bluesky posts for a given query string.

    Args:
        search_query (str): The term to search for in post content.
        limit (int): The maximum number of posts to return (default/max often 25-100).

    Returns:
        list: A list of post dictionaries containing author DID, URI, CID, text snippet, etc.
              Returns an empty list on failure or if no results are found.
        str: An error message string if a critical API error occurs.
    """
    global bluesky_client # Assumes setup_bluesky() handles authentication

    # Check client readiness
    if not BLUESKY_AVAILABLE: return "Bluesky library not available."
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Post Search: Client not ready, attempting setup...")
        if not setup_bluesky(): return "Bluesky connection isn't ready."
        if not bluesky_client: return "Bluesky setup failed, cannot search posts."

    print(f"Bluesky Post Search: Searching posts for query '{search_query}' (limit {limit})...")
    try:
        # Ensure endpoint exists on client object
        if not hasattr(bluesky_client, 'app') or not hasattr(bluesky_client.app, 'bsky') or not hasattr(bluesky_client.app.bsky.feed, 'search_posts'):
             raise ConnectionError("Bluesky client object appears invalid or missing expected attributes (app.bsky.feed.search_posts).")

        # Perform the search posts call
        # Note: Pagination for searchPosts uses a 'cursor' like getTimeline
        response = bluesky_client.app.bsky.feed.search_posts({'q': search_query, 'limit': limit})

        posts_data = []
        if response and hasattr(response, 'posts') and response.posts:
            print(f"Bluesky Post Search: Received {len(response.posts)} posts matching query.")
            for post_view in response.posts:
                # Extract necessary info directly from PostView structure
                # Ensure attribute names match the actual atproto library structure
                author_info = getattr(post_view, 'author', None)
                record_data = getattr(post_view, 'record', None) # <<< Check if 'record' is directly on PostView or nested

                # Sometimes the actual post content is nested further, e.g., post_view.post.record
                # You might need to inspect the 'post_view' object structure if 'record' isn't direct
                if not record_data and hasattr(post_view,'post') and hasattr(post_view.post,'record'):
                     record_data = post_view.post.record
                elif not record_data:
                     # print(f"Debug SearchPosts: Skipping post, no record data found directly or nested.") # Optional Debug
                     continue


                # --- Safely extract required fields ---
                author_did = getattr(author_info, 'did', None) if author_info else None
                author_handle = getattr(author_info, 'handle', 'unknown') if author_info else 'unknown'
                post_uri = getattr(post_view, 'uri', None)
                post_cid = getattr(post_view, 'cid', None)
                text_snippet = ""
                timestamp_iso = None

                # Get text from the record if it exists
                if record_data:
                    if hasattr(record_data, 'text'):
                        text_snippet = getattr(record_data, 'text', '')
                    elif isinstance(record_data, dict): # Fallback if record is a dict
                        text_snippet = record_data.get('text', '')

                    # Get timestamp from the record if it exists
                    created_at_val = None
                    if hasattr(record_data, 'created_at'): created_at_val = getattr(record_data, 'created_at', None)
                    elif isinstance(record_data, dict): created_at_val = record_data.get('createdAt', record_data.get('created_at'))

                    if created_at_val:
                        # (Use the same robust timestamp parsing logic as in get_bluesky_timeline_posts)
                        if isinstance(created_at_val, str):
                           try:
                               if '.' in created_at_val: # Handle fractional seconds
                                   parts = created_at_val.split('.'); fractional_part = parts[1]; tz_suffix = ''
                                   if 'Z' in fractional_part: tz_suffix = 'Z'; fractional_part = fractional_part.split('Z')[0]
                                   elif '+' in fractional_part: tz_suffix = '+' + fractional_part.split('+', 1)[1]; fractional_part = fractional_part.split('+', 1)[0]
                                   elif '-' in fractional_part: tz_suffix = '-' + fractional_part.split('-', 1)[1]; fractional_part = fractional_part.split('-', 1)[0]
                                   created_at_val = f"{parts[0]}.{fractional_part[:6]}{tz_suffix}" # Truncate
                               created_dt = datetime.fromisoformat(created_at_val.replace('Z', '+00:00'))
                               timestamp_iso = created_dt.isoformat()
                           except ValueError: timestamp_iso = created_at_val
                        elif isinstance(created_at_val, datetime): timestamp_iso = created_at_val.isoformat() if hasattr(created_at_val, 'isoformat') else str(created_at_val)
                        else: timestamp_iso = str(created_at_val)

                # Only append if we have the essential info for following/filtering
                if author_did and post_uri and post_cid:
                    posts_data.append({
                        'author': author_handle,
                        'author_did': author_did,
                        'text': text_snippet.strip()[:200], # Limit snippet length
                        'uri': post_uri,
                        'cid': post_cid,
                        'timestamp': timestamp_iso
                    })
                # else: # Optional: Log why a post was skipped
                #     print(f"Debug SearchPosts: Skipping post due to missing DID/URI/CID. URI:{post_uri}, CID:{post_cid}, DID:{author_did}")


        else:
             print(f"Bluesky Post Search: No posts found for query '{search_query}' or unexpected response.")
             # Return empty list, not an error string, if search simply found nothing
             return []

        print(f"Bluesky Post Search: Returning {len(posts_data)} parsed posts.")
        return posts_data

    except ConnectionError as ce:
        print(f"Error searching Bluesky posts: {ce}")
        return f"Couldn't search Bluesky posts: Client structure issue." # Return error string
    except Exception as e:
        print(f"Error searching Bluesky posts: {type(e).__name__} - {e}")
        traceback.print_exc()
        return f"Couldn't search Bluesky posts: {type(e).__name__}" # Return error string

def post_to_bluesky(text_content):
    """Posts the given text content using the authenticated global client."""
    global bluesky_client # Use the single global client

    # Check if client is ready, try to authenticate if not
    if not BLUESKY_AVAILABLE: return False, "Bluesky library not available."
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Post: Client not ready, attempting setup...")
        if not setup_bluesky(): # Try setting up the main client
             return False, "Bluesky connection isn't ready. Check credentials?"
        if not bluesky_client: # Check again after setup attempt
             return False, "Bluesky setup failed, cannot post."

    try:
        # Use the handle associated with the logged-in client
        posting_handle = bluesky_client.me.handle if hasattr(bluesky_client, 'me') and bluesky_client.me else BLUESKY_HANDLE
        # Truncate text slightly for logging if it's long
        log_text = text_content if len(text_content) < 50 else text_content[:47] + "..."
        print(f"Bluesky Post: Attempting to post as {posting_handle}: '{log_text}'")

        # --- Core Posting Logic using the single bluesky_client ---
        response = bluesky_client.com.atproto.repo.create_record(
             models.ComAtprotoRepoCreateRecord.Data(
                 repo=bluesky_client.me.did, # Post as the authenticated user (Silvie)
                 collection='app.bsky.feed.post',
                 # --- CORRECTED RECORD STRUCTURE ---
                 record={
                     '$type': 'app.bsky.feed.post', # Explicitly state the record type
                     'text': text_content,
                     'created_at': datetime.now(timezone.utc).isoformat(),
                     # Optional: Add language if desired
                     # 'langs': ['en']
                 }
                 # --- END OF CORRECTION ---
            )
        )
        # --- End Core Posting Logic ---

        print(f"Bluesky Post: Successfully posted! URI: {response.uri}, CID: {response.cid}")
        return True, "Posted successfully!"

    except TypeError as te: # Catch the specific error first
         print(f"Error posting to Bluesky (TypeError): {te}")
         print("This often means the record structure or class name is incorrect.")
         traceback.print_exc()
         return False, f"Couldn't post: Structure Error ({te})"
    except Exception as e:
        print(f"Error posting to Bluesky: {type(e).__name__} - {e}")
        traceback.print_exc()
        # Consider resetting client on specific auth errors?
        # if "Authentication required" in str(e) or "ExpiredToken" in str(e):
        #     print("Bluesky Post: Authentication error, resetting client.")
        #     bluesky_client = None # Force re-auth on next call
        return False, f"Couldn't post: {type(e).__name__}"

def like_bluesky_post(post_uri, post_cid):
    """Creates a 'like' record for a given Bluesky post."""
    global bluesky_client, models # Ensure models is accessible

    if not BLUESKY_AVAILABLE: return False, "Bluesky library not available."
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Like: Client not ready, attempting setup...")
        if not setup_bluesky(): return False, "Client setup failed, cannot like."
    if not bluesky_client: return False, "Client unavailable after setup attempt."

    if not post_uri or not post_cid:
        return False, "Missing post URI or CID to like."

    try:
        print(f"Bluesky Like: Attempting to like post URI: {post_uri}")

        # Construct the data for the like record
        # Use the structure defined by atproto for app.bsky.feed.like
        like_record_data = models.ComAtprotoRepoCreateRecord.Data(
            repo=bluesky_client.me.did, # Like as the authenticated user
            collection='app.bsky.feed.like',
            record={
                '$type': 'app.bsky.feed.like',
                'subject': { # Subject points to the post being liked
                    'uri': post_uri,
                    'cid': post_cid
                },
                'createdAt': datetime.now(timezone.utc).isoformat()
            }
        )

        # Create the like record
        response = bluesky_client.com.atproto.repo.create_record(data=like_record_data)

        print(f"Bluesky Like: Successfully liked post {post_uri}. Like URI: {response.uri}")
        return True, f"Liked post {post_uri}"

    except Exception as e:
        error_msg = str(e)
        # Check for duplicate like error (specific error message might vary)
        if 'duplicate record' in error_msg.lower() or 'Record already exists' in str(e):
             print(f"Bluesky Like: Already liked post {post_uri} or duplicate request.")
             # Returning True here because the desired state (liked) is achieved
             # Or return False, "Already liked." if you want distinct feedback
             return True, "Already liked."
        elif 'Subject does not exist' in error_msg:
             print(f"Bluesky Like: Post {post_uri} may have been deleted.")
             return False, "Post not found or deleted."

        # General error
        print(f"Error liking Bluesky post {post_uri}: {type(e).__name__} - {e}")
        traceback.print_exc()
        return False, f"Like failed: {type(e).__name__}"
    
# Needs access to global bluesky_client and setup_bluesky()
# Needs import: from atproto import models
# Needs import: from datetime import datetime, timezone # If not already globally imported

def post_to_bluesky_handler(text_to_post):
    """
    Handles the [PostToBluesky:] tag extracted from LLM response.
    Calls the actual posting function and formats feedback.
    Uses the SAME cooldown timer as proactive posts.
    """
    print(f"DEBUG Tag Handler: post_to_bluesky_handler called with text: '{text_to_post[:50]}...'")
    feedback = "*(Error processing Bluesky post tag)*" # Default error message
    processed = True # Assume we processed the tag, even if posting fails

    # --- USE THE PROACTIVE TIMER VARIABLE AND DURATION ---
    global last_proactive_bluesky_post_time, BLUESKY_POST_COOLDOWN # Use the shared timer/duration
    # ---

    # Optional: Add inline cooldown check here using the SHARED timer
    current_time_handler = time.time() # Get current time for check/update
    # --- CHECK THE SHARED COOLDOWN ---
    if current_time_handler - last_proactive_bluesky_post_time < BLUESKY_POST_COOLDOWN:
        print("DEBUG Tag Handler: Unified Bluesky post cooldown active. Skipping tag post.")
        feedback = "*(Unified Bluesky post cooldown active)*"
        return (feedback, processed) # Return early, tag processed but action skipped
    # ---

    try:
        # Call the function that actually interacts with the Bluesky API
        # Make sure 'post_to_bluesky' function is defined elsewhere and works
        success, message = post_to_bluesky(text_to_post)

        if success:
            # Successfully posted via inline tag
            # --- UPDATE THE SHARED COOLDOWN TIMER ---
            last_proactive_bluesky_post_time = current_time_handler # Update the shared timer
            # ---

            # Feedback: Typically silent for inline tags
            feedback = None # Or maybe "*(Posted to Bluesky)*" if you want some indicator
            print("DEBUG Tag Handler: Inline Bluesky post successful. Updated shared cooldown timer.")
        else:
            # Posting failed
            feedback = f"*(Bluesky post tag failed: {message})*"
            print(f"DEBUG Tag Handler: Inline Bluesky post failed: {message}")

    except NameError as ne:
         print(f"ERROR in post_to_bluesky_handler: Missing function? {ne}")
         feedback = "*(Bluesky posting function missing!)*"
         traceback.print_exc()
    except Exception as handler_err:
        print(f"ERROR processing Bluesky post tag: {type(handler_err).__name__} - {handler_err}")
        feedback = "*(Unexpected error handling Bluesky tag!)*"
        traceback.print_exc()

    # Return tuple: (feedback_message_for_chat_or_None, processed_flag)
    return (feedback, processed)

def search_actors_by_term(search_term, limit=5):
    """Searches for actors (users) matching a term.

    Returns:
        tuple: (list_of_actors, error_message_or_None)
               list_of_actors is a list of atproto ProfileView objects or empty list.
               Returns (None, error_message) on critical failure.
    """
    global bluesky_client
    if not BLUESKY_AVAILABLE: return None, "Bluesky library not available." # Check availability first
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Search Actors: Client not ready, attempting setup...")
        if not setup_bluesky(): return None, "Client setup failed, cannot search actors."
    if not bluesky_client: return None, "Client unavailable after setup attempt."

    try:
        print(f"Bluesky Search Actors: Searching for term '{search_term}' (limit {limit})")
        # Use the correct client structure based on your setup_bluesky
        response = bluesky_client.app.bsky.actor.search_actors(
            params={'term': search_term, 'limit': limit}
        )
        if response and hasattr(response, 'actors'):
            print(f"Bluesky Search Actors: Found {len(response.actors)} actors.")
            return response.actors, None # Return list of actor profiles
        else:
            print("Bluesky Search Actors: No actors found or unexpected response structure.")
            return [], None # Found nothing, but not an error

    except Exception as e:
        print(f"Error searching actors for '{search_term}': {type(e).__name__} - {e}")
        import traceback
        traceback.print_exc()
        return None, f"Search error: {type(e).__name__}"

# Ensure these imports are at the top of your script file:
# from datetime import datetime, timezone
# from atproto import models # Or however you import models from your atproto client setup

# Assume global variable bluesky_client is defined and handled elsewhere
# Assume setup_bluesky() function is defined elsewhere
# Assume BLUESKY_AVAILABLE flag is defined

def follow_actor_by_did(did_to_follow):
    """Follows an actor given their DID using the dictionary record format.

    Returns:
        tuple: (success_boolean, status_message)
    """
    global bluesky_client, models # Ensure models is accessible if needed for other parts

    if not BLUESKY_AVAILABLE: return False, "Bluesky library not available."
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Follow: Client not ready, attempting setup...")
        if not setup_bluesky(): return False, "Client setup failed, cannot follow."
    if not bluesky_client: return False, "Client unavailable after setup attempt."

    # --- Prevent Silvie from following herself ---
    my_did = getattr(bluesky_client.me, 'did', None) if hasattr(bluesky_client, 'me') else None
    if my_did and did_to_follow == my_did:
         my_handle = getattr(bluesky_client.me, 'handle', 'myself')
         print(f"Bluesky Follow: Attempt blocked - cannot follow self ({my_handle}).")
         return False, "Cannot follow self."
    # --- End Self-Follow Check ---

    try:
        print(f"Bluesky Follow: Attempting to follow DID {did_to_follow}")

        # --- Determine the correct way to access ComAtprotoRepoCreateRecord ---
        # This depends slightly on how 'models' is structured in your import/library version
        # Option 1: If models is the top-level atproto.models module
        create_record_data = models.ComAtprotoRepoCreateRecord.Data(
            repo=bluesky_client.me.did,
            collection='app.bsky.graph.follow',
            # --- CORRECTED RECORD STRUCTURE (V2 - Using Dictionary) ---
            record={
                '$type': 'app.bsky.graph.follow', # Explicitly state the record type
                'subject': did_to_follow,         # The DID of the account to follow
                'createdAt': datetime.now(timezone.utc).isoformat() # Use createdAt
            }
            # --- END OF CORRECTION ---
        )
        # Option 2: If models is nested differently, adjust path e.g., client.models...

        # The core action to create a follow record
        # Assumes client has com.atproto.repo.create_record method
        response = bluesky_client.com.atproto.repo.create_record(data=create_record_data)

        print(f"Bluesky Follow: Successfully initiated follow for {did_to_follow}. URI: {response.uri}")
        return True, f"Successfully initiated follow for DID {did_to_follow}"

    except AttributeError as ae:
        # Catch errors related to accessing attributes like .Main, .Data, or method calls
        print(f"Error following DID {did_to_follow} (AttributeError): {ae}")
        print("  This might indicate an incorrect model path or method name for your atproto version.")
        traceback.print_exc()
        return False, f"Follow failed: Structure/Method Error ({ae})"
    except TypeError as te:
        # Catch errors from calling something that isn't callable
        print(f"Error following DID {did_to_follow} (TypeError): {te}")
        traceback.print_exc()
        return False, f"Follow failed: Call Error ({te})"
    except Exception as e:
        # --- Existing General Error Handling ---
        error_msg = str(e)
        if hasattr(e, 'response') and e.response is not None and hasattr(e.response, 'json'): # Check if response has JSON body
             try:
                  error_json = e.response.json()
                  # Look for specific Bluesky/ATProto error messages
                  if 'message' in error_json: error_msg = error_json['message']
                  elif 'error' in error_json: error_msg = error_json['error']

                  # Refined checks based on potential ATProto error structures
                  if 'DuplicateRecordError' in error_msg or 'duplicate record' in error_msg.lower():
                       print(f"Bluesky Follow: Already following {did_to_follow} or duplicate request.")
                       return False, "Already following or duplicate action."
                  if 'InvalidRequestError' in error_msg and 'Subject does not exist' in error_msg:
                       print(f"Error following: DID {did_to_follow} likely does not exist.")
                       return False, "Target user not found."
                  if 'BlockedByActorError' in error_msg or 'blocked by subject' in error_msg.lower():
                       print(f"Error following: Blocked by {did_to_follow}.")
                       return False, "Blocked by target user."

             except ValueError: # JSONDecodeError is a subclass of ValueError
                  # If response isn't JSON, use the basic string representation
                  pass


        # Fallback checks on the string representation if JSON parsing failed or didn't have specific errors
        if 'duplicate record' in str(e).lower():
             print(f"Bluesky Follow: Already following {did_to_follow} or duplicate request (fallback check).")
             return False, "Already following or duplicate action."
        elif 'subject must be a did' in str(e).lower():
             print(f"Error following: Invalid DID format - {did_to_follow}")
             return False, "Invalid target DID format."
        elif 'could not find root' in str(e).lower() or 'resolve did' in str(e).lower() or 'Subject does not exist' in str(e):
             print(f"Error following: DID {did_to_follow} likely does not exist (fallback check).")
             return False, "Target user not found."
        elif 'blocked by subject' in str(e).lower():
             print(f"Error following: Blocked by {did_to_follow} (fallback check).")
             return False, "Blocked by target user."


        # General error catch-all
        print(f"Error following DID {did_to_follow}: {type(e).__name__} - {e}")
        traceback.print_exc()
        return False, f"Follow failed: {type(e).__name__}"

# --- End of function definition ---

def get_my_follows_dids():
    """Gets a set of DIDs that the authenticated user follows."""
    global bluesky_client
    if not BLUESKY_AVAILABLE: return None # Library missing
    if not bluesky_client or not hasattr(bluesky_client, 'me'):
        print("Bluesky Get Follows: Client not ready.")
        if not setup_bluesky(): return None # Try setup, return None if fails
    if not bluesky_client: return None # Still unavailable

    followed_dids = set()
    cursor = None
    max_pages = 10 # Safety limit to prevent infinite loops if pagination breaks
    pages_fetched = 0

    try:
        print(f"Bluesky Get Follows: Fetching follows for {bluesky_client.me.handle}...")
        while pages_fetched < max_pages:
             pages_fetched += 1
             params = {'actor': bluesky_client.me.did, 'limit': 100} # Max limit is 100
             if cursor:
                 params['cursor'] = cursor

             response = bluesky_client.app.bsky.graph.get_follows(params=params)

             if response and hasattr(response, 'follows') and isinstance(response.follows, list):
                 found_on_page = 0
                 for follow_profile in response.follows:
                     if hasattr(follow_profile, 'did'):
                         followed_dids.add(follow_profile.did)
                         found_on_page += 1

                 print(f"Bluesky Get Follows: Fetched page {pages_fetched}, added {found_on_page} DIDs (Total: {len(followed_dids)}).")

                 # Check for next page cursor
                 cursor = getattr(response, 'cursor', None)
                 if not cursor: # No more pages
                     break
             else:
                 print(f"Bluesky Get Follows: Invalid response or no 'follows' list on page {pages_fetched}.")
                 break # Exit loop on error or bad response

        if pages_fetched >= max_pages and cursor:
             print(f"Bluesky Get Follows: Reached max page limit ({max_pages}), may not have all follows.")

        print(f"Bluesky Get Follows: Finished fetching. Total unique follows found: {len(followed_dids)}")
        return followed_dids

    except Exception as e:
        print(f"Error getting follows list: {type(e).__name__} - {e}")
        import traceback
        traceback.print_exc()
        return None # Indicate error occurred during fetch

# Assume IMAGE_SAVE_FOLDER, STABLE_DIFFUSION_API_URL are defined globally
# Assume image_label, root, update_status are accessible globals for GUI updates

def generate_stable_diffusion_image(prompt, save_folder=IMAGE_SAVE_FOLDER):
    """
    Sends a prompt to the local Stable Diffusion Web UI API (txt2img).
    Saves the image uniquely and returns the filename or None on failure.
    Includes specified Studio Ghibli LoRA via prompt tag at 0.7 weight.
    Uses 60 steps and CFG Scale 14.
    """
    api_endpoint = f"{STABLE_DIFFUSION_API_URL.rstrip('/')}/sdapi/v1/txt2img"

    # --- LoRA Configuration ---
    # Use the exact filename provided (without the .safetensors extension)
    #GHIBLI_LORA_NAME = "StudioGhibliRedmond-15V-LiberteRedmond-StdGBRedmAF-StudioGhibli"
    #GHIBLI_LORA_WEIGHT = 0.7 # Set desired weight (0.7 as requested)

    # --- API Payload Construction (Using Prompt Tag Method) ---
    # Construct the LoRA tag string
    #lora_tag = f"<lora:{GHIBLI_LORA_NAME}:{GHIBLI_LORA_WEIGHT}>"

    payload = {
        # Combine style reinforcement, original prompt, and LoRA tag
        "prompt": f"Studio Ghibli style, {prompt}",
        "negative_prompt": "ugly, deformed, blurry, low quality, noisy, text, words, signature, watermark, photo, realism, 3d render",
        "steps": 60,          # <<< UPDATED TO 60
        "cfg_scale": 14,        # <<< UPDATED TO 14
        "width": 512,
        "height": 512,
        "sampler_name": "Euler a", # Or your preferred sampler
        "seed": -1             # Use -1 for random seed
        # NO alwayson_scripts block needed for this method
    }

    # Log the prompt being sent, including the tag
    print(f"DEBUG SD: Sending payload to {api_endpoint} (Prompt includes LoRA tag: '{payload['prompt'][:80]}...', Steps: {payload['steps']}, CFG: {payload['cfg_scale']})") # Updated log

    # --- API Request and Image Handling ---
    try:
        # Increased timeout for potentially slower generations, adjust if needed
        response = requests.post(api_endpoint, json=payload, timeout=600)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

        r = response.json()

        if not r.get('images') or not isinstance(r['images'], list) or len(r['images']) == 0:
            print("Error SD: API Response missing 'images' list or it's empty.")
            error_info = r.get('info', r.get('detail', '(no details)'))
            print(f"Error SD: API Info/Detail: {error_info}")
            return None

        # --- Process the first image ---
        image_data_base64 = r['images'][0]
        try:
            image_data = base64.b64decode(image_data_base64)
            if len(image_data) == 0:
                print("Error SD: Decoded image data is empty!")
                return None
        except Exception as decode_err:
            print(f"Error SD: Failed to decode base64 image data: {decode_err}")
            return None

        # --- Ensure Save Folder Exists ---
        try:
            os.makedirs(save_folder, exist_ok=True)
        except OSError as e:
            print(f"ERROR SD: Could not create image save folder '{save_folder}': {e}")
            return None

        # --- Generate Unique Filename ---
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_suffix = random.randint(100, 999)
        # Include LoRA name in filename for easier identification (optional)
        #safe_lora_name = re.sub(r'[^\w-]', '_', GHIBLI_LORA_NAME) # Make filename safe
        unique_filename = f"silvie_sd_{timestamp_str}_{random_suffix}.png" # Truncated safe name
        save_path = os.path.join(save_folder, unique_filename)

        # --- Save the Image ---
        try:
            with open(save_path, 'wb') as f:
                f.write(image_data)
            print(f"DEBUG SD: Image saved successfully to: {save_path}")
            if os.path.getsize(save_path) < 100: # Basic size check
                print(f"Warning SD: Saved image file size is very small ({os.path.getsize(save_path)} bytes).")
            return save_path # Return the full path on success
        except IOError as save_err:
            print(f"Error SD: Could not save image to '{save_path}': {save_err}")
            return None

    # --- Exception Handling ---
    except requests.exceptions.Timeout:
        print(f"Error SD: Request timed out connecting to API endpoint {api_endpoint}.")
        return None
    except requests.exceptions.ConnectionError:
        print(f"Error SD: Could not connect to Stable Diffusion API at {api_endpoint}. Is it running?")
        return None
    except requests.exceptions.RequestException as e:
        print(f"Error SD: API request failed: {e}")
        if e.response is not None:
            print(f"Error SD: Response Status Code: {e.response.status_code}")
            try:
                print(f"Error SD: Response Body: {e.response.json()}") # Attempt to parse JSON error body
            except json.JSONDecodeError:
                print(f"Error SD: Response Body (text): {e.response.text[:500]}...") # Fallback to text if not JSON
        return None
    except Exception as e:
        print(f"Error SD: Unexpected error during image generation: {type(e).__name__} - {e}")
        traceback.print_exc()
        return None

# --- End of generate_stable_diffusion_image function definition ---

def sd_image_tag_handler(prompt_from_tag):
    """Handles the [GenerateImage:] tag by starting SD generation."""
    print(f"DEBUG Tag Handler: sd_image_tag_handler called with prompt: '{prompt_from_tag[:50]}...'")
    feedback = "*(Error processing image tag)*" # Default feedback
    processed = True # Assume we processed the tag even if generation fails

    try:
        if STABLE_DIFFUSION_ENABLED and prompt_from_tag:
            print("DEBUG Tag Handler: SD enabled and prompt found. Calling start_sd_generation...")
            # Ensure start_sd_generation_and_update_gui exists and is accessible globally
            if 'start_sd_generation_and_update_gui' in globals():
                 start_sd_generation_and_update_gui(prompt_from_tag)
                 # Since generation is async, the immediate feedback indicates it started
                 feedback = "*(Starting image generation...)*"
                 print("DEBUG Tag Handler: Queued SD generation.")
            else:
                 print("CRITICAL ERROR: start_sd_generation_and_update_gui function not found!")
                 feedback = "*(Image generation helper missing!)*"

        elif not STABLE_DIFFUSION_ENABLED:
            print("DEBUG Tag Handler: SD disabled.")
            feedback = "*(Local image generator unavailable)*"
        else: # Empty prompt
            print("DEBUG Tag Handler: Empty prompt detected.")
            feedback = "*(Empty image tag)*"

    except Exception as handler_err:
        print(f"!!!!!!!! ERROR inside sd_image_tag_handler !!!!!!!!!")
        print(f"   Type: {type(handler_err).__name__}, Args: {handler_err.args}")
        traceback.print_exc()
        feedback = "*(Error initiating image generation!)*"

    # Return the feedback message and whether the tag was handled
    return (feedback, processed)

# --- Make sure call_gemini definition follows AFTER this ---


# --- ADD Function to run generation in thread and update GUI ---
import threading

# --- ADD Function to run generation in thread and update GUI ---
import threading

def start_sd_generation_and_update_gui(prompt_for_sd):
    """
    Starts Stable Diffusion generation in a background thread and places the
    resulting path into the shared app_state for the GUI updater to find.
    """
    if not STABLE_DIFFUSION_ENABLED:
        if root and root.winfo_exists():
            root.after(0, update_status, "Image generation unavailable.")
        return

    print(f"DEBUG SD Thread: Starting background generation for: '{prompt_for_sd[:50]}...'")
    if root and root.winfo_exists():
        root.after(0, update_status, "🎨 Conjuring image (SD)...")

    def generation_worker(p):
        """This part runs in the background and is slow."""
        saved_path = generate_stable_diffusion_image(p)

        # THIS IS NOW THE ONLY THING IT DOES WHEN FINISHED:
        # It posts the result to our shared app_state "bulletin board".
        # It no longer touches the GUI at all.
        if saved_path:
            app_state.last_generated_image_path = saved_path
            print(f"AppState Bridge: Image worker posted new path '{os.path.basename(saved_path)}' to app_state.")

            # The image was successfully created. Fire an event!
            print("Image Gen Worker: Firing 'image_generation_complete' event.")
            event_data = {
                "type": "image_generation_complete",
                "payload": {
                    "image_path": saved_path,
                    "original_prompt": p # The prompt 'p' is available here
                }
            }
            app_state.event_queue.put(event_data)

        else:
            # If generation fails, we should still update the status
            if root and root.winfo_exists():
                root.after(0, update_status, "Image generation failed (SD).")

    # Start the background thread
    threading.Thread(target=generation_worker, args=(prompt_for_sd,), daemon=True, name="SDGenWorker").start()






    
def update_status(message):
    """Update the status display in the GUI"""
    status_label.config(text=message)
    root.update()

# You can DELETE this entire block:
# def save_conversation_history():
#     """Save conversation history to a single JSON file"""
#     try:
#         filename = "silvie_chat_history.json"
#         with open(filename, 'w', encoding='utf-8') as f:
#             json.dump(conversation_history, f, indent=2)
#         print(f"Chat history saved to {filename}") # This message might be confusing now
#     except Exception as e:
#         print(f"Error saving chat history: {e}")

def load_conversation_history():
    """Load ONLY the most recent turns into memory for LLM context."""
    global conversation_history # Ensure global access
    filename = "silvie_chat_history.json"
    try:
        loaded_for_context = [] # Start with empty context
        if os.path.exists(filename) and os.path.getsize(filename) > 0:
            with open(filename, 'r', encoding='utf-8') as f:
                full_disk_history = json.load(f)
                if isinstance(full_disk_history, list):
                    # Get the last MAX_HISTORY_LENGTH*2 items for the context window
                    start_index = max(0, len(full_disk_history) - (MAX_HISTORY_LENGTH * 2))
                    loaded_for_context = full_disk_history[start_index:]
                else:
                    print(f"Warning: Data in {filename} is not a list. Cannot load context.")
        else:
            print(f"History file {filename} not found or empty. Starting fresh context.")

        # Clear and populate the active conversation_history list
        conversation_history.clear()
        conversation_history.extend(loaded_for_context)
        print(f"Loaded last {len(loaded_for_context) // 2} turns into active context memory.") # Adjusted print

    except json.JSONDecodeError:
         print(f"Error decoding JSON from {filename}. Starting fresh context.")
         conversation_history.clear()
    except Exception as e:
        print(f"Error loading chat history for context: {e}")
        conversation_history.clear() # Start fresh on error

def search_full_history(query):
    """Search through entire chat history and return relevant conversations"""
    try:
        update_status("🔍 Searching conversation history...")
        filename = "silvie_chat_history.json"
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                full_history = json.load(f)
                
            matches = []
            for i in range(0, len(full_history)-1, 2):
                user_msg = full_history[i]
                silvie_msg = full_history[i+1]
                
                if query.lower() in user_msg.lower() or query.lower() in silvie_msg.lower():
                    if user_msg.startswith("[") and "] " in user_msg:
                        user_msg = user_msg[user_msg.find("] ")+2:]
                    if silvie_msg.startswith("[") and "] " in silvie_msg:
                        silvie_msg = silvie_msg[silvie_msg.find("] ")+2:]
                    
                    matches.append({"user": user_msg, "silvie": silvie_msg})
            
            update_status("Ready")
            return matches
    except Exception as e:
        print(f"Error searching chat history: {e}")
        update_status("Search failed")
        return []

# Define this function somewhere in your script, likely after imports
# and before the main GUI setup or the if __name__ == "__main__" block.

# --- New web_search function using Google Custom Search API ---

# --- Updated web_search function with Full Page Fetch Fallback ---

def web_search(query, num_results=3, attempt_full_page_fetch=True, full_page_timeout=10, max_content_length=3000):
    """
    Performs a web search using Google Custom Search API, gets snippets,
    and optionally attempts to fetch full page content, falling back to the snippet on failure.
    It *returns* the results; it does NOT deliver proactive messages.
    Instead, it fires a 'web_search_completed' or 'web_search_failed' event.
    """
    status_updater = globals().get('update_status', lambda msg: print(f"Status Update:{msg}"))
    # Ensure app_state is accessible for event queuing
    global app_state 

    api_key = os.getenv("GOOGLE_API_KEY")
    cse_id = os.getenv("GOOGLE_CSE_ID")

    if not api_key or not cse_id:
        print("ERROR: Missing GOOGLE_API_KEY or GOOGLE_CSE_ID in environment variables.")
        status_updater("Web search failed (config error)")
        # --- NEW: Fire a 'web_search_failed' event for configuration errors ---
        if hasattr(app_state, 'event_queue'):
            event_data = {
                "type": "web_search_failed",
                "payload": {
                    "query": query,
                    "error": "Configuration error (missing API key/CSE ID)."
                }
            }
            app_state.event_queue.put(event_data)
        # --- END NEW ---
        return []

    status_updater(f"🔍 Searching Google for: {query[:35]}...")
    print(f"DEBUG web_search (Google): Querying API for '{query}'...")

    num_to_fetch = min(max(1, int(num_results)), 10)
    api_url = "https://www.googleapis.com/customsearch/v1"
    params = {'key': api_key, 'cx': cse_id, 'q': query, 'num': num_to_fetch}
    initial_results = []
    
    search_success = False # Flag to track overall success for event firing

    try:
        response = requests.get(api_url, params=params, timeout=15)
        response.raise_for_status()
        data = response.json()

        if 'error' in data:
            error_details = data['error'].get('message', 'Unknown Google API Error')
            print(f"ERROR: Google Custom Search API returned an error: {error_details}")
            status_updater(f"Web search failed ({data['error'].get('code', 'API Error')})")
            # --- NEW: Fire 'web_search_failed' event for API errors ---
            if hasattr(app_state, 'event_queue'):
                event_data = {
                    "type": "web_search_failed",
                    "payload": {
                        "query": query,
                        "error": f"Google API error: {error_details}"
                    }
                }
                app_state.event_queue.put(event_data)
            # --- END NEW ---
            return []

        search_items = data.get('items', [])
        print(f"DEBUG web_search (Google): Received {len(search_items)} items from API.")

        for item in search_items:
            title = item.get('title')
            link = item.get('link')
            snippet = item.get('snippet')
            if title and link and snippet:
                initial_results.append({
                    'title': title,
                    'url': link,
                    'content': snippet
                })
            else: print(f"  -> Skipping item due to missing field: {item}")

        if not initial_results:
            print("DEBUG web_search (Google): API returned results, but none had title/link/snippet.")
            status_updater("Ready")
            # --- NEW: Fire 'web_search_completed' event with empty results ---
            search_success = True # It succeeded in searching, just found nothing relevant
            # The event router's handler for web_search_completed will handle the "no results" message.
            # --- END NEW ---
            return []
        
        search_success = True # If we reach here, initial results were found

    except requests.exceptions.Timeout:
        print(f"ERROR: Web search request to Google API timed out.")
        status_updater("Web search failed (API timeout)")
        # --- NEW: Fire 'web_search_failed' event for timeouts ---
        if hasattr(app_state, 'event_queue'):
            event_data = {
                "type": "web_search_failed",
                "payload": {
                    "query": query,
                    "error": "Request timed out."
                }
            }
            app_state.event_queue.put(event_data)
        # --- END NEW ---
        return []
    except requests.exceptions.RequestException as e:
        print(f"ERROR: Web search request to Google API failed: {e}")
        if hasattr(e, 'response') and e.response is not None:
             print(f"  -> Response Status: {e.response.status_code}")
             try: print(f"  -> Response Body: {e.response.json()}")
             except json.JSONDecodeError: print(f"  -> Response Body: {e.response.text[:500]}...")
        status_updater("Web search failed (API network/error)")
        # --- NEW: Fire 'web_search_failed' event for network errors ---
        if hasattr(app_state, 'event_queue'):
            event_data = {
                "type": "web_search_failed",
                "payload": {
                    "query": query,
                    "error": f"Network/HTTP error: {e}"
                }
            }
            app_state.event_queue.put(event_data)
        # --- END NEW ---
        return []
    except Exception as e:
        print(f"ERROR: Unexpected error during Google API call: {type(e).__name__} - {e}")
        traceback.print_exc()
        status_updater("Web search failed (API unexpected error)")
        # --- NEW: Fire 'web_search_failed' event for unexpected errors ---
        if hasattr(app_state, 'event_queue'):
            event_data = {
                "type": "web_search_failed",
                "payload": {
                    "query": query,
                    "error": f"Unexpected error: {type(e).__name__}"
                }
            }
            app_state.event_queue.put(event_data)
        # --- END NEW ---
        return []

    final_results = initial_results

    if attempt_full_page_fetch and final_results:
        status_updater(f"🌐 Fetching full pages for {len(final_results)} results...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        for index in range(len(final_results)):
            result_item = final_results[index]
            url_to_fetch = result_item['url']

            try:
                page_response = requests.get(url_to_fetch, headers=headers, timeout=full_page_timeout, allow_redirects=True)
                page_response.raise_for_status()

                content_type = page_response.headers.get('content-type', '').lower()
                if 'html' not in content_type:
                     continue

                page_soup = BeautifulSoup(page_response.text, 'html.parser')
                for tag in page_soup(['script', 'style', 'meta', 'link', 'header', 'footer', 'nav', 'aside', 'form', 'button', 'input']):
                    tag.decompose()
                body_tag = page_soup.find('body')
                full_text = ""
                if body_tag:
                    full_text = ' '.join(body_tag.get_text(separator=' ', strip=True).split())[:max_content_length]
                else:
                    full_text = ' '.join(page_soup.get_text(separator=' ', strip=True).split())[:max_content_length]

                if full_text and len(full_text) > len(result_item['content']) + 50:
                    final_results[index]['content'] = full_text

            except (requests.exceptions.Timeout, requests.exceptions.RequestException, Exception):
                pass

    status_updater("Ready")
    print(f"DEBUG web_search: Returning {len(final_results)} results (content may be snippet or full text).")

    # --- NEW: Fire 'web_search_completed' event on successful completion ---
    if search_success and hasattr(app_state, 'event_queue'):
        event_data = {
            "type": "web_search_completed",
            "payload": {
                "query": query,
                "results": final_results # Pass the final results
            }
        }
        app_state.event_queue.put(event_data)
        print(f"Web Search: Fired 'web_search_completed' event for '{query}'.")
    # --- END NEW ---

    return final_results

# --- End of web_search function ---

# Ensure these constants are defined globally before the function:
# ENABLE_SOUND_CUE = True  # Or False
# SILVIE_SOUND_CUE_PATH = "silvie_start_sound.wav" # Or your actual path
# SILVIE_SOUND_CUE_DELAY = 0.2 # Or your desired delay

# Ensure these globals are accessible:
# running, engine, tts_queue


async def _async_generate_speech(text: str, voice: str, outfile: str) -> None:
    """Helper async function to generate speech using edge-tts."""
    try:
        communicate = edge_tts.Communicate(text, voice, rate="+07%")
        await communicate.save(outfile)
        print(f"DEBUG edge-tts: Successfully saved speech to {outfile}")
    except Exception as e:
        print(f"ERROR edge-tts: Failed during speech synthesis: {e}")
        # Re-raise the exception so the calling function knows it failed
        raise

def tts_worker():
    """Background worker to handle TTS requests using edge-tts and playsound."""
    global running # Use the main running flag

    # Define the desired voice
    VOICE = "en-US-AriaNeural"
    print(f"TTS worker started (using edge-tts, Voice: {VOICE})")

    # Check sound cue prerequisites (keep this logic)
    sound_cue_enabled_and_ready = (
        ENABLE_SOUND_CUE and
        PLAYSOUND_AVAILABLE and
        os.path.exists(SILVIE_SOUND_CUE_PATH)
    )
    if ENABLE_SOUND_CUE and PLAYSOUND_AVAILABLE and not os.path.exists(SILVIE_SOUND_CUE_PATH):
        print(f"Warning: Sound cue enabled, but file not found at '{SILVIE_SOUND_CUE_PATH}'. Disabling cue.")
        sound_cue_enabled_and_ready = False
    elif ENABLE_SOUND_CUE and not PLAYSOUND_AVAILABLE:
        print("Warning: Sound cue enabled, but 'playsound' library unavailable. Disabling cue.")
        sound_cue_enabled_and_ready = False

    while running:
        temp_audio_file = None # Variable to hold the temp file path
        try:
            text = tts_queue.get(timeout=1.0)
            if text is None: # Check for exit signal
                print("[DEBUG] TTS Worker (edge-tts): Received None, exiting loop.")
                break
            if not text.strip(): # Skip empty strings
                 tts_queue.task_done()
                 continue

            print(f"[DEBUG] TTS Worker (edge-tts): Got text: '{text[:40]}...'")

            # --- Play Sound Cue (if enabled) ---
            if sound_cue_enabled_and_ready:
                print("[DEBUG] TTS Worker (edge-tts): Attempting sound cue...")
                try:
                    playsound.playsound(SILVIE_SOUND_CUE_PATH, block=True) # Keep block=True for cue
                    print("[DEBUG] TTS Worker (edge-tts): Sound cue finished. Delaying...")
                    time.sleep(SILVIE_SOUND_CUE_DELAY) # Use existing delay constant
                except Exception as sound_err:
                    print(f"[DEBUG] TTS Worker (edge-tts): !!! Error playing sound cue: {sound_err}")
                    # Consider disabling cue for future attempts if errors persist frequently?
                    # sound_cue_enabled_and_ready = False

            # --- Generate Speech using edge-tts ---
            print(f"Synthesizing speech for: {text[:50]}...")
            # Create a temporary file to store the speech output (MP3)
            # Using NamedTemporaryFile ensures it gets cleaned up even on errors usually
            # Suffix is important for playsound
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as fp:
                 temp_audio_file = fp.name # Get the file path
            print(f"DEBUG edge-tts: Using temp file: {temp_audio_file}")

            # Run the async generation function
            try:
                 asyncio.run(_async_generate_speech(text, VOICE, temp_audio_file))
                 # Check if file was actually created and has size
                 if not os.path.exists(temp_audio_file) or os.path.getsize(temp_audio_file) == 0:
                      raise RuntimeError("Speech synthesis failed to create a valid audio file.")
            except Exception as synth_err:
                 print(f"ERROR: Speech synthesis failed: {synth_err}")
                 # Skip playback if synthesis failed
                 tts_queue.task_done()
                 # Clean up potentially empty temp file if it exists
                 if temp_audio_file and os.path.exists(temp_audio_file):
                     try: os.remove(temp_audio_file)
                     except OSError as e_del: print(f"Warning: Failed to remove temp file after synth error: {e_del}")
                 continue # Go to next queue item

            # --- Play the Generated Audio File ---
            if PLAYSOUND_AVAILABLE and temp_audio_file and os.path.exists(temp_audio_file):
                print(f"Playing generated speech from: {temp_audio_file}")
                try:
                    playsound.playsound(temp_audio_file, block=True) # Block ensures speech finishes
                    print("[DEBUG] TTS Worker (edge-tts): Playback finished.")
                    # Add a tiny delay AFTER successful playback before deleting,
                    # sometimes helps release file locks on Windows.
                    time.sleep(0.1)
                except Exception as play_err:
                    print(f"ERROR playing speech audio file '{temp_audio_file}': {play_err}")
                    # Log specific errors if they match the previous ones
                    if "alias by this application" in str(play_err):
                         print(">>> NOTE: 'Alias' error suggests potential conflict with audio device access.")
                    elif "device is not open" in str(play_err):
                         print(">>> NOTE: 'Device not open' error, playback issue.")
                    # Continue even if playback fails, clean up file below
            elif not PLAYSOUND_AVAILABLE:
                 print("Warning: Cannot play speech, playsound library is unavailable.")
            # else: # Handle case where file wasn't created properly (covered above)
            #    print("Warning: Skipping playback as audio file seems invalid.")

            tts_queue.task_done()

        except queue.Empty:
            continue # Normal timeout, loop again
        except Exception as e:
            print(f"TTS Worker Error (Outer Loop - edge-tts): {e}")
            import traceback
            traceback.print_exc()
            # Make sure task_done is called even on outer errors if item was retrieved
            # It might be safer to check if 'text' was successfully retrieved before calling task_done
            # Or simply let the queue management handle potential build-up if errors are frequent
            try:
                tts_queue.task_done() # Attempt to mark as done to prevent queue block
            except ValueError: # Can happen if task_done called too many times
                 pass
            continue # Continue loop after outer error
        finally:
            # --- Cleanup Temporary File ---
            if temp_audio_file and os.path.exists(temp_audio_file):
                # Add extra delay and retry loop for deletion on Windows? Sometimes needed.
                attempts = 0
                while attempts < 3:
                     try:
                         os.remove(temp_audio_file)
                         print(f"DEBUG edge-tts: Removed temp file: {temp_audio_file}")
                         break # Exit loop if successful
                     except PermissionError as e_perm:
                         print(f"Warning: PermissionError removing temp file (attempt {attempts+1}/3): {e_perm}. Retrying...")
                         attempts += 1
                         time.sleep(0.2) # Wait longer before retry
                     except OSError as e_del:
                         print(f"Warning: Failed to remove temp file '{temp_audio_file}': {e_del}")
                         break # Don't retry on other OS errors
                if attempts == 3:
                     print(f"ERROR: Failed to remove temp file {temp_audio_file} after multiple attempts.")


    print("TTS worker stopped (using edge-tts)")

# --- End of new tts_worker ---

from twilio.rest import Client

# Twilio Setup
TWILIO_ACCOUNT_SID = os.getenv("TWILIO_ACCOUNT_SID")
TWILIO_AUTH_TOKEN = os.getenv("TWILIO_AUTH_TOKEN")
TWILIO_PHONE_NUMBER = os.getenv("TWILIO_PHONE_NUMBER")
MY_PHONE_NUMBER = os.getenv("MY_PHONE_NUMBER")

if all([TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN, TWILIO_PHONE_NUMBER, MY_PHONE_NUMBER]):
    twilio_client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)
    sms_enabled = True
else:
    print("SMS not configured. Set Twilio environment variables to enable SMS.")
    sms_enabled = False

print("\nTwilio Configuration Status:")
print("===========================")
print(f"Account SID: {'✓ Set' if TWILIO_ACCOUNT_SID else '✗ Missing'}")
print(f"Auth Token: {'✓ Set' if TWILIO_AUTH_TOKEN else '✗ Missing'}")
print(f"Twilio Phone: {'✓ Set' if TWILIO_PHONE_NUMBER else '✗ Missing'}")
print(f"Your Phone: {'✓ Set' if MY_PHONE_NUMBER else '✗ Missing'}")
print(f"SMS Enabled: {'✓ Yes' if sms_enabled else '✗ No'}")
print("===========================\n")

def send_sms(message):
    """Send SMS via Twilio"""
    if not sms_enabled:
        print("SMS not enabled - environment variables missing")
        return False
        
    try:
        print(f"\nSMS Debug Info:")
        print(f"Message length: {len(message)} characters")
        print(f"From: {TWILIO_PHONE_NUMBER}")
        print(f"To: {MY_PHONE_NUMBER}")
        print(f"Message: {message[:50]}...")
        
        message = twilio_client.messages.create(
            body=message,
            from_=TWILIO_PHONE_NUMBER,
            to=MY_PHONE_NUMBER
        )
        print(f"SMS sent successfully! Message SID: {message.sid}")
        print(f"Message status: {message.status}")
        print(f"Error code (if any): {message.error_code}")
        print(f"Error message (if any): {message.error_message}\n")
        return True
    except Exception as e:
        print(f"SMS error: {str(e)}")
        return False
    
# Gmail API setup
SCOPES = [
    'https://www.googleapis.com/auth/gmail.modify',
    'https://www.googleapis.com/auth/gmail.compose',
    'https://www.googleapis.com/auth/gmail.readonly',

    'https://www.googleapis.com/auth/calendar.readonly', # To read calendars and events
    'https://www.googleapis.com/auth/calendar.events',    # To create, change, and delete events

    'https://www.googleapis.com/auth/youtube.readonly',
    'https://www.googleapis.com/auth/youtube.force-ssl',
    'https://www.googleapis.com/auth/youtube',

]

def setup_google_services():
    """Setup Google API access for Gmail, Calendar, and YouTube.""" # Updated docstring
    global gmail_service, calendar_service, youtube_service # Added youtube_service
    creds = None
    token_path = 'token.pickle' # Path to store token
    creds_path = 'credentials.json' # Path to your credentials file

    try:
        # Load existing token if it exists
        if os.path.exists(token_path):
            with open(token_path, 'rb') as token:
                creds = pickle.load(token)
            print(f"Loaded credentials from {token_path}")

        # If there are no (valid) credentials available, let the user log in.
        if not creds or not creds.valid:
            print("Credentials missing or invalid.")
            should_reauth = False
            if creds and creds.expired and creds.refresh_token:
                print("Credentials expired, attempting refresh...")
                try:
                    creds.refresh(Request())
                    print("Google credentials refreshed successfully.")
                    # Save the refreshed credentials immediately
                    with open(token_path, 'wb') as token:
                        pickle.dump(creds, token)
                    print(f"Refreshed credentials saved to {token_path}")
                except Exception as refresh_err:
                    print(f"ERROR refreshing credentials: {refresh_err}")
                    # Force re-authentication if refresh fails
                    creds = None # Clear creds to trigger the flow below
                    should_reauth = True
                    if os.path.exists(token_path):
                        try:
                            os.remove(token_path) # Remove potentially invalid token file
                            print(f"Removed potentially invalid token file: {token_path}")
                        except OSError as e:
                            print(f"Error removing token file {token_path}: {e}")
            else:
                # Trigger re-auth if creds are missing entirely or refresh failed
                should_reauth = True

            # Only run the auth flow if needed
            if should_reauth:
                 print("No valid credentials found or refresh failed, running authentication flow...")
                 if not os.path.exists(creds_path):
                      raise FileNotFoundError(f"ERROR: {creds_path} not found. Download it from Google Cloud Console.")

                 # Ensure SCOPES list is defined globally and includes YouTube scopes
                 if 'SCOPES' not in globals():
                      raise NameError("ERROR: SCOPES list is not defined globally.")

                 flow = InstalledAppFlow.from_client_secrets_file(creds_path, SCOPES)
                 # Note: Using port=0 finds a random available port
                 creds = flow.run_local_server(port=0)
                 print("Authentication flow completed.")
                 # Save the new credentials immediately after successful flow
                 with open(token_path, 'wb') as token:
                     pickle.dump(creds, token)
                 print(f"Credentials saved to {token_path}")

        # Build ALL service objects using the valid credentials
        if creds and creds.valid: # Double check creds are valid before building
            gmail_service = build("gmail", "v1", credentials=creds)
            print("✓ Gmail service built.")
            calendar_service = build("calendar", "v3", credentials=creds)
            print("✓ Calendar service built.")
            youtube_service = build('youtube', 'v3', credentials=creds) # <-- ADDED THIS LINE
            print("✓ YouTube Data API service built.") # <-- ADDED THIS LINE

            print("\nGoogle Services Configuration Status:")
            print("====================================")
            print("✓ Gmail integration enabled")
            print("✓ Calendar integration enabled")
            print("✓ YouTube integration enabled") # <-- ADDED THIS LINE
            print("====================================\n")
        else:
             # This case should ideally not be reached if logic above is sound, but acts as a safety net
             print("ERROR: Failed to obtain valid credentials after auth/refresh attempts.")
             gmail_service = None
             calendar_service = None
             youtube_service = None # <-- Ensure it's None on failure
             raise ConnectionError("Could not establish valid Google API credentials.")


    except FileNotFoundError as fnf_error:
         print("\nGoogle Services Configuration Status:")
         print("====================================")
         print(f"✗ ERROR: {fnf_error}")
         print("   (Ensure credentials.json is in the correct directory)")
         print("====================================\n")
         # Ensure all services are None if credentials file missing
         gmail_service = None
         calendar_service = None
         youtube_service = None
    except NameError as name_err: # Catch if SCOPES is missing
         print("\nGoogle Services Configuration Status:")
         print("====================================")
         print(f"✗ ERROR: Setup failed due to missing variable: {name_err}")
         print("   (Ensure SCOPES list is defined before calling setup_google_services)")
         print("====================================\n")
         gmail_service = None; calendar_service = None; youtube_service = None
    except Exception as e:
        print("\nGoogle Services Configuration Status:")
        print("====================================")
        print(f"✗ Google services setup failed: {type(e).__name__} - {str(e)}")
        import traceback
        traceback.print_exc() # Print full traceback for debugging setup issues
        print("   (Check API enablement, credentials file, network connection, scopes, deleted token?)")
        print("====================================\n")
        # Ensure services are None if setup fails
        gmail_service = None
        calendar_service = None
        youtube_service = None

# --- End of setup_google_services function definition ---

def search_youtube_videos(query, num_results=5):
    """Searches YouTube for videos based on a query."""
    global youtube_service
    if not youtube_service:
        print("YouTube Error: Service not initialized.")
        return "My connection to the video streams is fuzzy right now." # Error message

    try:
        print(f"YouTube Debug: Searching for '{query}' (limit {num_results})...")
        search_response = youtube_service.search().list(
            q=query,
            part='snippet',
            maxResults=num_results,
            type='video' # Ensure we only get videos
        ).execute()

        videos = []
        for item in search_response.get('items', []):
            snippet = item.get('snippet', {})
            video_id = item.get('id', {}).get('videoId')
            if video_id: # Ensure we have an ID
                videos.append({
                    'id': video_id,
                    'title': snippet.get('title', 'No Title'),
                    'description': snippet.get('description', 'No Description'),
                    'channel': snippet.get('channelTitle', 'Unknown Channel'),
                    'url': f"https://www.youtube.com/watch?v={video_id}"
                })
        print(f"YouTube Debug: Found {len(videos)} videos.")
        return videos # Return list of video dicts

    except Exception as e:
        print(f"YouTube Error: Search failed - {e}")
        return f"Had trouble searching YouTube for '{query}': {type(e).__name__}" # Error message

def get_video_transcript(video_id):
    """
    Attempts to fetch the transcript for a given YouTube video ID.

    Args:
        video_id (str): The unique identifier for the YouTube video.

    Returns:
        str: The full transcript text as a single string if successful.
        None: If transcripts are unavailable, disabled, not found, the library
              is missing, or an error occurs.
    """
    # Check if the library was successfully imported
    if not YOUTUBE_TRANSCRIPT_API_AVAILABLE or YouTubeTranscriptApi is None:
        print(f"YouTube Debug: Skipping transcript fetch for {video_id} (library unavailable).")
        return None

    # Validate input
    if not video_id or not isinstance(video_id, str):
        print("YouTube Debug: Invalid video_id provided for transcript fetch.")
        return None

    print(f"YouTube Debug: Attempting to fetch transcript for video ID: {video_id}")
    try:
        # 1. List available transcripts for the video
        # This requires the YouTubeTranscriptApi class from the library
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

        # 2. Try to find a suitable transcript (prioritize generated English)
        # You can adjust the language list ['en'] if needed
        try:
             transcript = transcript_list.find_generated_transcript(['en'])
             print("YouTube Debug: Found generated English transcript.")
        except NoTranscriptFound:
             print("YouTube Debug: No generated English transcript found. Trying manually created English...")
             try:
                 transcript = transcript_list.find_manually_created_transcript(['en'])
                 print("YouTube Debug: Found manually created English transcript.")
             except NoTranscriptFound:
                 print("YouTube Debug: No manual English transcript found either. Trying any available transcript...")
                 # Fallback: find *any* transcript if English isn't available
                 try:
                     # Get a list of all available languages for this video
                     available_languages = [t.language for t in transcript_list]
                     if not available_languages:
                          print("YouTube Debug: No transcripts listed at all for this video.")
                          return None # No transcripts of any kind
                     print(f"YouTube Debug: Available transcript languages: {available_languages}")
                     # Fetch the first available transcript regardless of language
                     transcript = transcript_list.find_transcript(available_languages)
                     print(f"YouTube Debug: Using first available transcript (Language: {transcript.language}).")
                 except NoTranscriptFound: # Should not happen if available_languages was populated
                      print("YouTube Debug: Error - Could not find *any* transcript despite listing available languages.")
                      return None # Truly no transcript found


        # 3. Fetch the actual transcript data (list of dictionaries)
        transcript_data = transcript.fetch()
        if not transcript_data:
             print(f"YouTube Debug: Transcript fetch returned empty data for {video_id}.")
             return None # Handle empty fetch result

        # 4. Combine the text segments into a single string
        full_transcript = " ".join([segment['text'] for segment in transcript_data if 'text' in segment])
        print(f"YouTube Debug: Transcript fetched successfully ({len(full_transcript)} chars).")
        return full_transcript

    # Handle specific exceptions from the library
    except TranscriptsDisabled:
        # This exception is caught if the owner has disabled transcripts
        print(f"YouTube Debug: Transcripts are disabled by the owner for video {video_id}.")
        return None
    # NoTranscriptFound is handled within the nested try-except blocks above

    # Handle potential errors during the API calls or processing
    except Exception as e:
        # Catch any other exceptions (network errors, library bugs, etc.)
        print(f"YouTube Error: Failed to fetch transcript for {video_id} - {type(e).__name__}: {e}")
        # Print traceback for unexpected errors to help debug
        traceback.print_exc()
        return None

# --- End of Function ---

def summarize_youtube_content(title, description, transcript=None, max_length=1500):
    """Uses Gemini to summarize YouTube video content."""
    global client # Assuming Gemini client is global

    if not client:
        return "My summarization circuits are offline."

    # Prepare context for the LLM
    content_context = f"Video Title: {title}\n"
    content_context += f"Description: {description}\n"
    if transcript:
        # Limit transcript length to avoid exceeding prompt limits
        transcript_snippet = transcript[:max_length]
        content_context += f"Transcript Snippet (up to {max_length} chars):\n{transcript_snippet}...\n"
    else:
        content_context += "Transcript: (Not available or couldn't be fetched)\n"

    # Construct the summarization prompt (using Silvie's persona)
    summary_prompt = (
        f"{SYSTEM_MESSAGE}\n" # Use Silvie's main persona prompt
        f"--- Video Content ---\n{content_context}\n"
        f"--- Instruction ---\n"
        f"Based *only* on the provided video title, description, and transcript snippet (if available), briefly summarize the main topic or purpose of this YouTube video in Silvie's whimsical and conversational voice. What's the gist of it? Keep it concise (2-4 sentences).\n\n"
        f"Silvie's Summary:"
    )

    print("YouTube Debug: Asking LLM to summarize video content...")
    try:
        # Use the standard client and safety settings
        response = flash_model.generate_content(summary_prompt, tools=FS_TOOL_DEFINITIONS)
        summary = response.text.strip()
        # Clean prefix if necessary
        if summary.startswith("Silvie's Summary:"):
            summary = summary.split(":", 1)[-1].strip()
        elif summary.startswith("Silvie:"):
             summary = summary.split(":", 1)[-1].strip()

        print(f"YouTube Debug: Summary generated: '{summary[:80]}...'")
        return summary if summary else "Hmm, I looked at the details, but couldn't quite capture the essence."
    except Exception as e:
        print(f"YouTube Error: LLM Summarization failed - {e}")
        return "My thoughts got tangled trying to summarize that video."

def extract_video_id(url):
    """Extracts YouTube video ID from various URL formats."""
    # Regex to find video ID in standard, short, and embed URLs
    patterns = [
        r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', # Standard watch URL
        r'(?:youtu\.be\/)([0-9A-Za-z_-]{11})', # Shortened URL
        r'(?:embed\/)([0-9A-Za-z_-]{11})' # Embed URL
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None # Return None if no ID found

def read_recent_emails(max_results=25):  # Changed from 5 to 25
    """Get recent emails"""
    global gmail_service
    if not gmail_service:
        print("Gmail service not initialized")
        return []
        
    try:
        print("Fetching recent emails...")  # Debug output
        results = gmail_service.users().messages().list(
            userId='me', 
            maxResults=max_results,  # Now fetching 25
            labelIds=['INBOX']
        ).execute()
        
        messages = results.get('messages', [])
        emails = []
        
        for message in messages:
            msg = gmail_service.users().messages().get(
                userId='me', 
                id=message['id'],
                format='full'
            ).execute()
            
            # Extract email details
            headers = msg['payload']['headers']
            subject = next((h['value'] for h in headers if h['name'].lower() == 'subject'), 'No Subject')
            sender = next((h['value'] for h in headers if h['name'].lower() == 'from'), 'Unknown Sender')
            date = next((h['value'] for h in headers if h['name'].lower() == 'date'), '')
            
            emails.append({
                'id': message['id'],
                'subject': subject,
                'from': sender,
                'date': date,
                'snippet': msg['snippet'],
                'importance': calculate_email_importance(sender, subject, msg['snippet'])
            })
            print(f"Found email: {subject} from {sender}")  # Debug output
        
        # Sort by importance score
        emails.sort(key=lambda x: x['importance'], reverse=True)
        return emails
    except Exception as e:
        print(f"Error reading emails: {str(e)}")
        return []

def calculate_email_importance(sender, subject, snippet):
    """Calculate importance score for email"""
    score = 0
    
    # Important senders (customize based on BJ's contacts)
    important_senders = ['athenahealth.com', 'hannaford', 'pharmacy', 'dental']
    for important in important_senders:
        if important.lower() in sender.lower():
            score += 3
    
    # Important subject keywords
    important_subjects = ['urgent', 'important', 'reminder', 'schedule', 'meeting', 
                         'appointment', 'prescription', 'medication', 'request']
    for important in important_subjects:
        if important.lower() in subject.lower():
            score += 2
    
    # Content importance
    important_content = ['deadline', 'today', 'tomorrow', 'asap', 'required',
                        'action needed', 'response needed', 'confirm']
    for important in important_content:
        if important.lower() in snippet.lower():
            score += 1
    
    return score
    
def send_email(to, subject, body):
    """Send an email"""
    if not gmail_service:
        return False
        
    try:
        message = MIMEText(body)
        message['to'] = to
        message['subject'] = subject
        
        raw = base64.urlsafe_b64encode(message.as_bytes()).decode('utf-8')
        gmail_service.users().messages().send(
            userId='me', body={'raw': raw}).execute()
        return True
    except Exception as e:
        print(f"Error sending email: {e}")
        return False
    
def get_upcoming_events(max_results=10):
    """Fetches the next upcoming events from the primary Google Calendar."""
    global calendar_service
    if not calendar_service:
        print("Calendar service not initialized.")
        return "My connection to the time-stream (Calendar) is fuzzy right now."

    try:
        # Get the current time in UTC and format it according to ISO 8601 (RFC3339 compatible)
        now_utc = datetime.now(timezone.utc).isoformat() # <<< CORRECTED LINE (Removed + 'Z')
        print(f'Getting the upcoming {max_results} events')

        # Call the Calendar API
        events_result = calendar_service.events().list(
            calendarId='primary',       # Use 'primary' for the main calendar
            timeMin=now_utc,            # Start from now
            maxResults=max_results,     # How many events to get
            singleEvents=True,          # Treat recurring events as individual instances
            orderBy='startTime'         # Order by start time
        ).execute()

        events = events_result.get('items', [])

        if not events:
            print('No upcoming events found.')
            return "Your schedule looks wonderfully clear for the near future!"

        event_list = []
        print("\nUpcoming Events:")
        print("----------------")
        for event in events:
            start = event['start'].get('dateTime', event['start'].get('date'))
            summary = event.get('summary', 'No Title') # Handle events without a summary

            # Format start time nicely
            try:
                # Handle full datetime events
                if 'T' in start:
                    start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))
                    # Convert to local timezone (requires pytz or Python 3.9+ zoneinfo)
                    # Simple approach: just show time and indicate timezone if possible
                    start_formatted = start_dt.strftime('%a, %b %d at %I:%M %p') # e.g., Mon, Jul 29 at 02:30 PM
                # Handle all-day events (date only)
                else:
                    start_dt = datetime.fromisoformat(start)
                    start_formatted = start_dt.strftime('%a, %b %d (All day)') # e.g., Tue, Jul 30 (All day)

            except ValueError:
                 start_formatted = start # Fallback if parsing fails

            print(f"- {start_formatted}: {summary}")
            event_list.append({'start': start_formatted, 'summary': summary})
        print("----------------\n")

        return event_list # Return the structured list

    except HttpError as error:
        print(f'An API error occurred: {error}')
        # Provide more specific feedback if possible
        if error.resp.status == 403:
             return "Hmm, Google Calendar says I don't have permission to peek at your schedule. Did the permissions get granted correctly?"
        elif error.resp.status == 404:
             return "It seems your primary calendar is playing hide-and-seek. Can't find it!"
        else:
             return f"Ran into a snag trying to read your calendar: {error}"
    except Exception as e:
        print(f"Unexpected error fetching calendar events: {e}")
        return "Something went unexpectedly sideways while checking your schedule."
    
# Make sure datetime, timezone, tz, math, HttpError are imported

def format_relative_time(event_dt):
    """Formats the event time relative to now in a user-friendly way."""
    now = datetime.now(event_dt.tzinfo) # Ensure comparison uses the same timezone awareness
    delta = event_dt - now
    seconds_until = delta.total_seconds()

    # Check if event has already started or is very close
    if seconds_until < -300: # Started more than 5 mins ago
         return f"already started ({event_dt.strftime('%I:%M %p')})"
    elif seconds_until < 60: # Starting within the next minute or just started
         return "starting now"

    days_until = delta.days
    hours_until = math.floor(seconds_until / 3600)
    minutes_until = math.floor((seconds_until % 3600) / 60)

    today_str = event_dt.strftime('%I:%M %p').lstrip('0') # e.g., 3:00 PM
    tomorrow_str = f"tomorrow at {today_str}"
    weekday_str = event_dt.strftime('%A at %I:%M %p').replace(" 0", " ") # e.g., Tuesday at 3:00 PM

    if days_until == 0:
        if hours_until >= 4:
            return f"today at {today_str}"
        elif hours_until >= 1:
            if minutes_until > 15 and minutes_until < 45:
                 return f"in about {hours_until}.5 hours"
            elif minutes_until >= 45:
                 return f"in about {hours_until + 1} hours"
            else:
                 return f"in about {hours_until} hours" # Simplified if near the hour mark
        elif minutes_until > 1:
            return f"in {minutes_until} minutes"
        else: # Covered by "starting now"
             return "very soon"
    elif days_until == 1:
        return tomorrow_str
    elif days_until < 7:
        return weekday_str
    else:
        # For events further out, just give the date and time
        return event_dt.strftime('%b %d at %I:%M %p').replace(" 0", " ") # e.g., Aug 15 at 3:00 PM

def format_all_day_relative(event_dt_start):
     """Formats an all-day event's start date relative to today."""
     # Ensure comparison is date-only
     today = datetime.now(event_dt_start.tzinfo).date() # Use event's timezone perspective if available, otherwise default tz's date
     event_date = event_dt_start.date()
     delta_days = (event_date - today).days

     if delta_days == 0:
          return "all day today"
     elif delta_days == 1:
          return "all day tomorrow"
     elif delta_days < 7:
          return f"all day {event_dt_start.strftime('%A')}" # e.g., all day Tuesday
     else:
          return f"all day on {event_dt_start.strftime('%b %d')}" # e.g., all day on Aug 15

def fetch_next_event():
    """Fetches the single next upcoming event for ambient context."""
    global calendar_service
    if not calendar_service:
        return None # Service not ready

    try:
        now_utc = datetime.now(timezone.utc).isoformat()

        events_result = calendar_service.events().list(
            calendarId='primary',
            timeMin=now_utc,
            maxResults=1, # << Fetch only one
            singleEvents=True,
            orderBy='startTime'
        ).execute()

        events = events_result.get('items', [])

        if not events:
            return {'summary': 'Schedule Clear', 'when': 'for the near future'} # Indicate schedule is clear

        event = events[0]
        summary = event.get('summary', 'Unnamed Event')
        start_info = event['start']

        # Handle dateTime vs date (all-day events)
        if 'dateTime' in start_info:
             start_str = start_info['dateTime']
             try:
                  start_dt = datetime.fromisoformat(start_str)
                  # Convert to local timezone for relative formatting
                  local_tz = tz.tzlocal() # Get local timezone
                  start_dt_local = start_dt.astimezone(local_tz)
                  when_str = format_relative_time(start_dt_local)
             except ValueError:
                  print(f"Warning: Could not parse dateTime: {start_str}")
                  when_str = start_str # Fallback
        elif 'date' in start_info: # All-day event
             start_str = start_info['date']
             try:
                  # Treat all-day event date as being in the calendar's timezone (often local)
                  # For simplicity, parse as date and assume local context for relative formatting
                  start_dt = datetime.fromisoformat(start_str)
                  # Get local timezone to provide context for today/tomorrow check
                  local_tz = tz.tzlocal()
                  start_dt_aware = start_dt.replace(tzinfo=local_tz) # Make it offset-aware for comparison
                  when_str = format_all_day_relative(start_dt_aware)
             except ValueError:
                  print(f"Warning: Could not parse date: {start_str}")
                  when_str = start_str # Fallback
        else:
             when_str = "at an unknown time" # Should not happen with valid events

        print(f"Calendar Context Debug: Next event found - '{summary}' {when_str}")
        return {'summary': summary, 'when': when_str}

    except HttpError as error:
        # Don't return error string, just log it and return None for context
        error_details = error.content.decode('utf-8') if error.content else '(No details)'
        print(f'Calendar Context Error (API): {error}\nDetails: {error_details}')
        return None
    except Exception as e:
        print(f"Calendar Context Error (Unexpected): {e}")
        import traceback
        traceback.print_exc()
        return None
    

def project_awareness_worker(app_state):
    """
    Periodically reads the weekly and daily plan files and updates the app_state.
    This ensures Silvie's main conversational module is always aware of her projects.
    """
    print("Project Awareness Worker: Starting.")
    # Use the filenames defined in the respective worker modules for consistency
    WEEKLY_PLAN_FILE = "weekly_plan.json" 
    DAILY_PLAN_FILE = "daily_plan.json"
    CHECK_INTERVAL = 300  # Check for updates every 5 minutes

    while app_state.running:
        try:
            # --- Check Weekly Plan ---
            if os.path.exists(WEEKLY_PLAN_FILE):
                with open(WEEKLY_PLAN_FILE, 'r', encoding='utf-8') as f:
                    weekly_plan = json.load(f)
                    # Update app_state with the current weekly muse/goal
                    app_state.current_weekly_muse = weekly_plan.get("weekly_muse")
            else:
                # If the file doesn't exist, ensure the state is cleared
                app_state.current_weekly_muse = None

            # --- Check Daily Plan ---
            if os.path.exists(DAILY_PLAN_FILE):
                with open(DAILY_PLAN_FILE, 'r', encoding='utf-8') as f:
                    daily_plan = json.load(f)
                    # Update app_state with the current daily goal
                    app_state.current_daily_goal = daily_plan.get("daily_goal")
            else:
                # If the file doesn't exist, clear the state
                app_state.current_daily_goal = None

        except (IOError, json.JSONDecodeError) as e:
            print(f"Project Awareness Worker ERROR: Could not read plan file - {e}")
            # On error, clear the states to be safe
            app_state.current_weekly_muse = None
            app_state.current_daily_goal = None
        except Exception as e:
            print(f"!!! Project Awareness Worker Error (Outer Loop): {e}")
            traceback.print_exc()

        # Wait for the next check
        sleep_end_time = time.time() + CHECK_INTERVAL
        while time.time() < sleep_end_time and app_state.running:
            time.sleep(15)

    print("Project Awareness Worker: Loop exited.")

    
def calendar_context_worker():
    """Periodically fetches the next event and stores the previous event for check-ins."""
    # This worker's globals are now much simpler.
    global upcoming_event_context, running, last_checked_event_context

    print("Calendar Context Worker: Starting. My job is to watch the schedule.")
    time.sleep(10) # Initial startup delay

    while running:
        try:
            # --- This worker's ONLY job is now fetching calendar context ---
            print("Calendar Context Worker: Attempting to fetch next event...")
            
            # Store the currently known event before fetching a new one.
            previous_event_context_before_fetch = upcoming_event_context
            
            # Fetch the single next event from the API.
            next_event = fetch_next_event()
            
            # Compare the newly fetched event with the one we knew about before.
            # This logic is for the proactive worker to be able to ask "How did [event] go?".
            if next_event != previous_event_context_before_fetch:
                # If the event has changed, and the old event wasn't trivial...
                if (previous_event_context_before_fetch and
                   isinstance(previous_event_context_before_fetch, dict) and
                   previous_event_context_before_fetch.get('summary') != 'Schedule Clear' and
                   'starting now' not in previous_event_context_before_fetch.get('when', '').lower()):
                    
                    # ...then save the old event as the one to potentially ask about later.
                    last_checked_event_context = previous_event_context_before_fetch
                    print(f"Calendar Context Worker: Stored '{last_checked_event_context.get('summary')}' as a completed event for potential check-in.")

                    # The event has just passed. Fire an event!
                    print(f"Calendar Worker: Firing 'important_event_started' event for '{last_checked_event_context.get('summary')}'.")
                    event_data = {
                        "type": "important_event_started",
                        "payload": last_checked_event_context # The payload is the entire event context dictionary
                    }
                    app_state.event_queue.put(event_data)


            # Update the main upcoming_event_context with the newly fetched event.
            upcoming_event_context = next_event
            
            # Push the update to the central app_state so all other workers can see it.
            app_state.upcoming_event_context = upcoming_event_context
            
            # --- Wait for the next interval ---
            print(f"Calendar Context Worker: Event fetch complete. Waiting for {CALENDAR_CONTEXT_INTERVAL} seconds.")
            sleep_start = time.time()
            while time.time() - sleep_start < CALENDAR_CONTEXT_INTERVAL:
                 if not running: break
                 time.sleep(5) # Check every 5 seconds if the app is closing.

        except Exception as e:
            # Standard error handling for the worker loop.
            print(f"!!! Calendar Context Worker Error (Outer Loop): {type(e).__name__} - {e}")
            import traceback; traceback.print_exc()
            print("Calendar Context Worker: Waiting 5 minutes after error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300:
                 if not running: break
                 time.sleep(10)

        if not running:
            break

    print("Calendar Context Worker: Loop exited.")

# Ensure necessary imports: datetime, timezone, HttpError, calendar_service

def create_calendar_event(summary, start_iso, end_iso, description=None):
    """Creates an event on the primary Google Calendar."""
    global calendar_service
    if not calendar_service:
        return False, "Calendar service not available."

    # Basic validation
    if not all([summary, start_iso, end_iso]):
         return False, "Missing required event details (summary, start, end)."

    # --- Get the local timezone name using get_localzone_name() ---
    try:
        # Use the recommended function from tzlocal
        local_tz_name = get_localzone_name()

        if not local_tz_name:
             raise ValueError("tzlocal.get_localzone_name() returned an empty value.")

        print(f"Calendar Debug: Using local timezone name: {local_tz_name}")
    except Exception as tz_err:
        print(f"ERROR determining local timezone: {tz_err}")
        # Attempt fallback using strftime (less reliable for IANA names)
        try:
            local_tz_name = datetime.now(tz.tzlocal()).strftime('%Z')
            print(f"Warning: Using fallback timezone name '{local_tz_name}'. This might not be an IANA name and could cause issues.")
            if not local_tz_name or len(local_tz_name) < 3: # Basic check
                 raise ValueError("Fallback timezone name also appears invalid.")
        except Exception as fallback_tz_err:
            print(f"ERROR determining local timezone (including fallback): {fallback_tz_err}")
            return False, f"Couldn't determine the local timezone ({fallback_tz_err}). Cannot schedule event."
    # --- End timezone name retrieval ---


    # Construct the event body according to Google Calendar API V3 format
    event_body = {
        'summary': summary,
        'description': description if description else f"A suggestion from Silvie.",
        'start': {
            'dateTime': start_iso,
            'timeZone': local_tz_name, # Use the retrieved IANA name
        },
        'end': {
            'dateTime': end_iso,
            'timeZone': local_tz_name, # Use the retrieved IANA name
        },
    }

    try:
        print(f"Attempting to create event: {summary} from {start_iso} to {end_iso} (Timezone: {local_tz_name})")
        created_event = calendar_service.events().insert(
            calendarId='primary',
            body=event_body
        ).execute()
        print(f"Event created: {created_event.get('htmlLink')}")
        return True, f"Successfully scheduled '{summary}'!"

    except HttpError as error:
        error_details = error.content.decode('utf-8') if error.content else '(No details)'
        print(f'API error creating event: {error}\nDetails: {error_details}')
        reason = error.resp.reason
        message = f"Couldn't schedule '{summary}'. Google Calendar reported an error: {reason}"
        try:
             error_json = json.loads(error_details)
             first_error_message = error_json.get('error',{}).get('errors',[{}])[0].get('message', reason)
             # Handle the specific timezone error more gracefully if it still occurs
             if "Missing time zone definition" in first_error_message:
                  message = f"Couldn't schedule '{summary}'. Google still needs a timezone definition (tried '{local_tz_name}')."
             else:
                  message = f"Couldn't schedule '{summary}'. Google Calendar said: '{first_error_message}'"
        except: pass
        return False, message
    except Exception as e:
        print(f"Unexpected error creating event: {e}")
        import traceback
        traceback.print_exc()
        return False, f"A mysterious snag occurred while trying to schedule '{summary}'."
    
def find_available_slot(duration_minutes, look_ahead_days=1, earliest_hour=9, latest_hour=21):
    """
    Finds the first available time slot of a given duration within the next few days.

    Args:
        duration_minutes: Desired duration of the event in minutes.
        look_ahead_days: How many days into the future to search (default 1 = today + tomorrow).
        earliest_hour: The earliest hour (local time) to consider scheduling (e.g., 9 for 9 AM).
        latest_hour: The latest hour (local time) after which scheduling shouldn't occur (e.g., 21 for 9 PM).

    Returns:
        A dictionary {'start': iso_start, 'end': iso_end} if a slot is found, otherwise None.
    """
    global calendar_service
    if not calendar_service:
        print("Cannot find slot: Calendar service unavailable.")
        return None

    try:
        local_tz = tz.tzlocal()
        now_local = datetime.now(local_tz)

        # Define search window
        time_min_dt = now_local
        # Ensure search starts slightly after now to avoid immediate conflicts
        if time_min_dt.minute < 55: time_min_dt += timedelta(minutes=5)

        time_max_dt = (now_local + timedelta(days=look_ahead_days)).replace(hour=23, minute=59, second=59)

        # Convert search window to UTC ISO format for API
        time_min_iso = time_min_dt.astimezone(timezone.utc).isoformat()
        time_max_iso = time_max_dt.astimezone(timezone.utc).isoformat()

        print(f"Searching for {duration_minutes}min slot between {time_min_dt.strftime('%Y-%m-%d %H:%M')} and {time_max_dt.strftime('%Y-%m-%d %H:%M')}")

        # Fetch events within the window
        events_result = calendar_service.events().list(
            calendarId='primary',
            timeMin=time_min_iso,
            timeMax=time_max_iso,
            singleEvents=True,
            orderBy='startTime'
        ).execute()

        events = events_result.get('items', [])
        print(f"Found {len(events)} existing events in the search window.")

        # --- Process events to find gaps ---
        busy_periods = []
        for event in events:
            start_str = event['start'].get('dateTime') # Prefer dateTime
            end_str = event['end'].get('dateTime')

            # Handle all-day events - treat them as blocking the entire relevant day(s) within working hours
            if not start_str or not end_str:
                 date_str = event['start'].get('date') # All-day event date
                 if date_str:
                      event_date = datetime.fromisoformat(date_str).date()
                      # Check if this all-day event's date falls within our search window dates
                      if time_min_dt.date() <= event_date <= time_max_dt.date():
                           # Block out the working hours for that day
                           start_dt_local = datetime.combine(event_date, datetime.min.time(), tzinfo=local_tz).replace(hour=earliest_hour)
                           end_dt_local = datetime.combine(event_date, datetime.min.time(), tzinfo=local_tz).replace(hour=latest_hour)
                           busy_periods.append((start_dt_local, end_dt_local))
                           print(f"  -> Treating all-day event '{event.get('summary', 'N/A')}' on {date_str} as busy {earliest_hour}:00-{latest_hour}:00")
                 continue # Skip to next event if it was all-day

            # Process timed events
            try:
                start_dt = datetime.fromisoformat(start_str).astimezone(local_tz)
                end_dt = datetime.fromisoformat(end_str).astimezone(local_tz)
                busy_periods.append((start_dt, end_dt))
                # print(f"  -> Existing timed event: {start_dt.strftime('%H:%M')} - {end_dt.strftime('%H:%M')} ('{event.get('summary', 'N/A')}')")
            except ValueError:
                print(f"  -> Warning: Could not parse times for event: {event.get('summary', 'N/A')}")
                continue

        # Sort busy periods by start time
        busy_periods.sort()

        # --- Find the first suitable gap ---
        desired_duration = timedelta(minutes=duration_minutes)
        current_check_time = time_min_dt # Start checking from ~now

        # Check gap before the first busy period
        first_event_start = busy_periods[0][0] if busy_periods else time_max_dt # Use end of window if no events
        gap_start = current_check_time
        # Ensure gap start isn't before earliest hour on its day
        if gap_start.hour < earliest_hour:
             gap_start = gap_start.replace(hour=earliest_hour, minute=0, second=0, microsecond=0)
        potential_end = gap_start + desired_duration

        if potential_end <= first_event_start and potential_end.hour < latest_hour:
             print(f"Found potential slot before first event: {gap_start.strftime('%H:%M')} - {potential_end.strftime('%H:%M')}")
             return {'start': gap_start.isoformat(), 'end': potential_end.isoformat()}

        # Check gaps between busy periods
        for i in range(len(busy_periods)):
            gap_start = busy_periods[i][1] # End of the current busy period
            # Ensure gap start isn't before earliest hour
            if gap_start.hour < earliest_hour:
                 gap_start = gap_start.replace(hour=earliest_hour, minute=0, second=0, microsecond=0)

            # Check against the start of the next busy period, or end of search window
            next_busy_start = busy_periods[i+1][0] if (i + 1) < len(busy_periods) else time_max_dt

            potential_end = gap_start + desired_duration

            # Check if the slot fits before the next event AND ends before the latest hour
            if potential_end <= next_busy_start and potential_end.hour < latest_hour:
                 # Extra check: Ensure the start and end are on the same day, otherwise logic gets complex
                 if gap_start.date() == potential_end.date():
                      print(f"Found potential slot between events: {gap_start.strftime('%H:%M')} - {potential_end.strftime('%H:%M')}")
                      return {'start': gap_start.isoformat(), 'end': potential_end.isoformat()}

        print("No suitable slot found within the search window and parameters.")
        return None

    except HttpError as error:
        error_details = error.content.decode('utf-8') if error.content else '(No details)'
        print(f'API error finding slot: {error}\nDetails: {error_details}')
        return None
    except Exception as e:
        print(f"Unexpected error finding slot: {e}")
        import traceback
        traceback.print_exc()
        return None
    


def get_daily_project_status_context():
    """
    Reads the daily plan and returns a formatted string for LLM context.
    """
    PLAN_FILE = "daily_plan.json" # Assumes it's in the same directory
    try:
        if not os.path.exists(PLAN_FILE):
            return "" # No plan, no context.

        with open(PLAN_FILE, 'r', encoding='utf-8') as f:
            plan_data = json.load(f)

        today_str = str(date.today())
        if plan_data.get("date") != today_str or not plan_data.get("steps"):
            return "" # Plan is outdated or empty.

        goal = plan_data.get("daily_goal", "an unknown goal")
        if goal == "COMPLETED":
            return "[[Daily Project Status: The project for today is complete!]]\n"
            
        steps = plan_data.get("steps", [])
        scratchpad_keys = list(plan_data.get("scratchpad", {}).keys())

        context_str = f"[[Daily Project Status: My goal today is '{goal}'.\n"
        next_step_found = False
        for i, step in enumerate(steps):
            description = step.get("description", "Unnamed step")
            if step.get("completed"):
                context_str += f"  [✓] Step {i+1}: {description}\n"
            else:
                # This is the next step to be done
                if not next_step_found:
                    context_str += f"  [-->] Next Step: {description}\n"
                    next_step_found = True
                else:
                    context_str += f"  [ ] Step {i+1}: {description}\n"
        
        if scratchpad_keys:
            context_str += f"  My scratchpad currently holds: {', '.join(scratchpad_keys)}.\n"

        context_str += "]]\n"
        return context_str

    except Exception as e:
        print(f"ERROR getting daily project status context: {e}")
        return "" # Return empty string on any error



def set_windows_wallpaper(image_path: str) -> tuple[bool, str]:
    """
    Sets the Windows desktop wallpaper.

    Args:
        image_path (str): The absolute path to the image file.

    Returns:
        tuple[bool, str]: (success_boolean, message_string)
    """
    SPI_SETDESKWALLPAPER = 20
    SPIF_UPDATEINIFILE = 0x01
    SPIF_SENDCHANGE = 0x02

    if not os.path.isabs(image_path):
        abs_image_path = os.path.abspath(image_path)
        print(f"INFO: Converted relative wallpaper path '{image_path}' to absolute '{abs_image_path}'")
        image_path = abs_image_path
    
    if not os.path.exists(image_path):
        msg = f"Wallpaper image not found at: {image_path}"
        print(f"ERROR: {msg}")
        return False, msg

    try:
        result = ctypes.windll.user32.SystemParametersInfoW(
            SPI_SETDESKWALLPAPER,
            0,
            image_path,
            SPIF_UPDATEINIFILE | SPIF_SENDCHANGE
        )
        if result:
            msg = f"Desktop wallpaper set to: {os.path.basename(image_path)}"
            print(f"INFO: {msg}")
            return True, msg
        else:
            error_code = ctypes.get_last_error() 
            win_error = ctypes.WinError(error_code) 
            msg = f"Failed to set wallpaper. SystemParametersInfoW returned {result}. WinError: {win_error} (Code: {error_code})"
            print(f"ERROR: {msg}")
            return False, msg
    except AttributeError: # This might happen if not on Windows or ctypes issue
        msg = "Failed to set wallpaper: ctypes.windll.user32.SystemParametersInfoW not available (not on Windows or ctypes issue?)."
        print(f"ERROR: {msg}")
        return False, msg
    except Exception as e:
        msg = f"An exception occurred while setting wallpaper: {e}"
        print(f"ERROR: {msg}")
        traceback.print_exc() 
        return False, msg


def generate_and_set_wallpaper(prompt: str):
    """
    A dedicated thread worker function that generates an image from a prompt
    and then immediately sets it as the desktop wallpaper.
    """
    print(f"Wallpaper Dream: Starting generation for prompt: '{prompt[:50]}...'")
    if 'update_status' in globals():
        # This will need to be thread-safe if called from the router's thread
        root.after(0, update_status, f"🎨 Dreaming up wallpaper...")

    # Ensure the necessary functions are available
    if 'generate_stable_diffusion_image' not in globals() or 'set_windows_wallpaper' not in globals():
        print("Wallpaper Dream ERROR: Missing core generation or setting function.")
        if 'update_status' in globals():
            root.after(0, update_status, "Wallpaper helper functions missing.")
        return

    # 1. Generate the image
    image_path = generate_stable_diffusion_image(prompt)

    # 2. Set as wallpaper if successful
    if image_path and os.path.exists(image_path):
        print(f"Wallpaper Dream: Image created at '{image_path}'. Setting as wallpaper.")
        if 'update_status' in globals():
            root.after(0, update_status, f"🖼️ Setting new wallpaper...")
        
        success, msg = set_windows_wallpaper(image_path)
        
        if success:
            print(f"Wallpaper Dream: Successfully set wallpaper.")
            if 'update_status' in globals():
                root.after(0, update_status, "Ready")
        else:
            print(f"Wallpaper Dream ERROR: Failed to set wallpaper: {msg}")
            if 'update_status' in globals():
                root.after(0, update_status, "Wallpaper set failed.")
    else:
        print(f"Wallpaper Dream ERROR: Image generation failed for prompt '{prompt[:50]}...'.")
        if 'update_status' in globals():
            root.after(0, update_status, "Wallpaper generation failed.") 


def get_latest_dream():
    """Fetches the text of the most recent dream safely."""
    try:
        # Check if the file exists and is not empty
        if os.path.exists("silvie_dreams.json") and os.path.getsize("silvie_dreams.json") > 2:
            with open("silvie_dreams.json", 'r', encoding='utf-8') as f:
                dreams = json.load(f)
                # Ensure the loaded data is a non-empty list
                if dreams and isinstance(dreams, list):
                    # Return the text of the last dream in the list
                    return dreams[-1].get("dream_text")
    except (IOError, json.JSONDecodeError, IndexError) as e:
        # If there's any error reading, parsing, or accessing the list, fail gracefully
        print(f"Debug (get_latest_dream): Could not retrieve dream - {type(e).__name__}. This is normal on first run.")
        return None
    return None


    
SPOTIFY_CLIENT_ID = os.getenv("SPOTIFY_CLIENT_ID")
SPOTIFY_CLIENT_SECRET = os.getenv("SPOTIFY_CLIENT_SECRET")
SPOTIFY_REDIRECT_URI = os.getenv("SPOTIFY_REDIRECT_URI") # Usually http://localhost:8888/callback or similar configured in your Spotify App dashboard

# Define the permissions Silvie needs
SPOTIFY_SCOPES = [
    "user-read-playback-state",
    "user-modify-playback-state",
    "user-read-currently-playing",
    "playlist-read-private",
    "playlist-modify-public",
    "playlist-modify-private",
    "user-library-read"  # <<< ADDED LINE
]

# Path to store the token info, keeps you logged in
SPOTIFY_CACHE_PATH = ".spotify_token_cache.json"

spotify_client = None # Global variable to hold the authenticated client
spotify_auth_manager = None # Global variable for the auth manager

def setup_spotify():
    """Handles Spotify Authentication using Spotipy"""
    global spotify_client, spotify_auth_manager
    if not all([SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET, SPOTIFY_REDIRECT_URI]):
        print("\nSpotify Configuration Status:")
        print("===========================")
        print("✗ Spotify integration disabled - Missing Client ID, Secret, or Redirect URI in env variables.")
        print("===========================\n")
        spotify_client = None
        return False

    try:
        print("\nAttempting Spotify Authentication...")
        # Use SpotifyOAuth for handling the authorization flow and token refresh
        spotify_auth_manager = SpotifyOAuth(
            client_id=SPOTIFY_CLIENT_ID,
            client_secret=SPOTIFY_CLIENT_SECRET,
            redirect_uri=SPOTIFY_REDIRECT_URI,
            scope=" ".join(SPOTIFY_SCOPES), # Join scopes into a space-separated string
            cache_path=SPOTIFY_CACHE_PATH,
            show_dialog=True # Set to False after first successful auth if preferred
        )

        # Try to get a token. This might open a browser for you the first time!
        # It will try to use the cache first, then refresh, then prompt for auth.
        token_info = spotify_auth_manager.get_access_token(check_cache=True)

        if token_info:
            spotify_client = spotipy.Spotify(auth_manager=spotify_auth_manager)
            user_info = spotify_client.current_user() # Test the connection
            print("\nSpotify Configuration Status:")
            print("===========================")
            print(f"✓ Spotify Authenticated successfully for user: {user_info['display_name']} ({user_info['id']})")
            print("===========================\n")
            return True
        else:
            print("\nSpotify Configuration Status:")
            print("===========================")
            print("✗ Spotify Authentication failed - Could not get token.")
            print("===========================\n")
            spotify_client = None
            return False

    except Exception as e:
        print("\nSpotify Configuration Status:")
        print("===========================")
        print(f"✗ Spotify setup failed: {str(e)}")
        print("   - Did you set the correct Redirect URI in your Spotify App Dashboard?")
        print(f"   - Redirect URI used: {SPOTIFY_REDIRECT_URI}")
        print("===========================\n")
        spotify_client = None
        return False

def get_spotify_client():
    """Ensures the Spotify client is authenticated, refreshing if needed."""
    global spotify_client, spotify_auth_manager
    if not spotify_auth_manager:
        print("Spotify Debug: Auth manager not initialized.")
        if not setup_spotify(): # Try setup again if needed
             return None
        if not spotify_auth_manager: # Check again after setup attempt
            return None

    if not spotify_auth_manager.validate_token(spotify_auth_manager.get_cached_token()):
        print("Spotify Debug: Token expired or invalid, attempting refresh...")
        try:
            token_info = spotify_auth_manager.get_access_token(check_cache=False) # Force refresh/re-auth if needed
            if token_info:
                 spotify_client = spotipy.Spotify(auth_manager=spotify_auth_manager)
                 print("Spotify Debug: Token refreshed successfully.")
            else:
                 print("Spotify Debug: Failed to refresh token.")
                 spotify_client = None
                 return None # Explicitly return None if refresh fails
        except Exception as e:
            print(f"Spotify Debug: Error refreshing token: {e}")
            spotify_client = None # Ensure client is None on error
            return None

    # If spotify_client is still None after potential refresh, try setting it again
    if not spotify_client and spotify_auth_manager.validate_token(spotify_auth_manager.get_cached_token()):
         spotify_client = spotipy.Spotify(auth_manager=spotify_auth_manager)

    # Final check
    if not spotify_client:
        print("Spotify Debug: Could not get valid Spotify client.")
        return None

    return spotify_client

def silvie_find_spotify_song(query, limit=1):
    """Searches Spotify for a single track and returns its details."""
    sp = get_spotify_client()
    if not sp:
        return "Spotify client not available."

    print(f"Spotify Tool (find_song): Searching for track with query: '{query}'")
    try:
        results = sp.search(q=query, type='track', limit=limit, market='US')
        tracks = results.get('tracks', {}).get('items', [])
        
        if not tracks:
            return f"No track found matching '{query}'."
            
        track = tracks[0]
        return {
            "name": track.get('name', 'Unknown Title'),
            "artist": track.get('artists', [{}])[0].get('name', 'Unknown Artist'),
            "uri": track.get('uri')
        }
    except Exception as e:
        print(f"Spotify Tool (find_song) ERROR: {e}")
        return f"Error searching for track: {type(e).__name__}"

def translate_features_to_descriptors(features):
    """
    Translates a dictionary of Spotify audio features into a list of
    human-readable descriptive strings.

    Args:
        features (dict): A dictionary containing audio features for a track
                         (e.g., from spotify_client.audio_features()).

    Returns:
        list: A list of descriptive strings (e.g., ["energetic", "happy", "acoustic"]).
              Returns an empty list if input is invalid.
    """
    if not features or not isinstance(features, dict):
        # print("Debug translate_features: Invalid or empty features input.") # Optional debug
        return []

    descriptors = []

    # Energy -> Energetic, Calm, Mellow
    energy = features.get('energy')
    if energy is not None: # Check if feature exists
        if energy > 0.8: descriptors.append("very energetic")
        elif energy > 0.6: descriptors.append("energetic")
        elif energy < 0.3: descriptors.append("very calm")
        elif energy < 0.5: descriptors.append("mellow")

    # Valence -> Happy, Positive, Sad, Melancholy
    valence = features.get('valence')
    if valence is not None:
        if valence > 0.75: descriptors.append("very happy")
        elif valence > 0.55: descriptors.append("positive mood")
        elif valence < 0.3: descriptors.append("sad mood")
        elif valence < 0.45: descriptors.append("melancholy")

    # Acousticness -> Acoustic, Electronic
    acousticness = features.get('acousticness')
    if acousticness is not None:
        if acousticness > 0.8: descriptors.append("very acoustic")
        elif acousticness > 0.5: descriptors.append("acoustic")
        # Only add "electronic" if *confidently* not acoustic
        elif acousticness < 0.05: descriptors.append("electronic")

    # Danceability -> Danceable
    danceability = features.get('danceability')
    if danceability is not None:
        if danceability > 0.8: descriptors.append("very danceable")
        elif danceability > 0.65: descriptors.append("danceable")

    # Instrumentalness -> Instrumental
    instrumentalness = features.get('instrumentalness')
    if instrumentalness is not None:
        if instrumentalness > 0.7: descriptors.append("instrumental")

    # Speechiness -> Spoken Word
    speechiness = features.get('speechiness')
    if speechiness is not None:
        if speechiness > 0.66: descriptors.append("spoken word")
        # Optional: Add for mix, but can be noisy
        # elif speechiness > 0.33: descriptors.append("speech/music mix")

    # Tempo -> Slow, Fast
    tempo = features.get('tempo')
    if tempo is not None:
        if tempo < 80: descriptors.append("slow tempo")
        elif tempo > 140: descriptors.append("fast tempo")
        # Optional: add medium tempo?
        # elif 90 <= tempo <= 110: descriptors.append("medium tempo")

    # Mode -> Major/Minor Key (Less descriptive of 'feel', but can add)
    # mode = features.get('mode')
    # if mode == 1: descriptors.append("major key")
    # elif mode == 0: descriptors.append("minor key")

    # Use set to remove potential duplicates from overlapping logic, then convert back to list
    unique_descriptors = list(set(descriptors))
    # print(f"Debug translate_features: Raw={features}, Descriptors={unique_descriptors}") # Optional debug
    return unique_descriptors

# --- Function to get current track WITHOUT features ---

def silvie_get_current_track_with_features(): # Keep name for compatibility? Or rename? Let's keep for now.
    """
    Return details of the current Spotify track (WITHOUT audio features).
    Modified to temporarily remove audio features call causing 403 error.

    Returns
    -------
    dict – keys: artist, track, uri, id, features (set to None), raw (playback blob)
           Returns None if nothing is playing.
           Returns an error string on Spotify API issues.
    """
    print("Spotify Debug: Simplified version of get_current_track (NO FEATURES).") # Added note

    sp = get_spotify_client() # Assumes this function exists and handles auth/refresh
    if sp is None:
        return "Looks like my connection to the music ether is fuzzy right now."

    try:
        # Try getting detailed playback state first
        playback = sp.current_playback()
        # Fallback to simpler currently playing if playback state is empty
        # (e.g., playing on a device not fully controllable via API)
        if not (playback and playback.get("item")):
            playback = sp.currently_playing()

        # If still no item after checking both, nothing is playing
        if not (playback and playback.get("item")):
            print("Spotify Debug: Nothing currently playing.")
            return None # Return None if nothing is playing

        # Extract track details from the 'item'
        item       = playback["item"]
        # Robustly get track ID (handle both 'id' and 'uri' formats)
        track_id   = item.get("id")
        if not track_id and item.get("uri"):
            try:
                track_id = item["uri"].split(":")[-1]
            except IndexError:
                track_id = None # Handle potential malformed URI
        track_uri  = item.get("uri")
        track_name = item.get("name", "Unknown Track")
        artist     = item["artists"][0]["name"] if item.get("artists") else "Unknown Artist"

        # --- REMOVED AUDIO FEATURES CALL ---
        # features = None
        # if track_id:
        #     try:
        #         features_list = sp.audio_features(track_id) # This was causing 403
        #         features = features_list[0] if features_list else None
        #     except spotipy.exceptions.SpotifyException as feature_err:
        #          print(f"Spotify Debug: Error fetching audio features for {track_id}: {feature_err}")
        #          features = None # Set features to None on error
        # else:
        #      print("Spotify Debug: No valid track_id found to fetch features.")
        #      features = None
        # --- END REMOVED CALL ---

        print(f"Spotify Debug (Simplified): Found '{track_name}' by {artist}")

        # Return dictionary, explicitly setting features to None
        return {
            "artist":   artist,
            "track":    track_name,
            "uri":      track_uri,
            "id":       track_id,
            "features": None, # Explicitly None as we didn't fetch them
            "raw":      playback, # Keep raw playback data if needed elsewhere
        }

    except spotipy.exceptions.SpotifyException as e:
        print(f"Spotify error ({e.http_status}): {e.msg}")
        # Keep specific error handling if needed
        # if e.http_status == 403:
        #     return "Spotify won’t reveal the song’s secrets right now (403)."
        return f"Spotify hiccup accessing playback: {e.msg}" # More generic error
    except Exception as e:
        print(f"Unexpected Spotify error: {type(e).__name__}: {e}")
        traceback.print_exc() # Print traceback for unexpected errors
        return "Something went sideways trying to see what song is on."

# --- End of revised function ---

# Remember to also have the translate_features_to_descriptors function defined elsewhere
# and update the calling code in call_gemini and proactive_worker to use this function correctly.

def silvie_control_playback(action, volume_level=None):
    """Controls Spotify playback: play, pause, skip_next, skip_previous, volume."""
    sp = get_spotify_client()
    if not sp:
        return "Can't reach the Spotify controls right now."

    try:
        # Check for active device first
        devices = sp.devices()
        if not devices or not devices.get('devices'):
            print("Spotify Debug: No active devices found.")
            return "Hmm, I don't see an active Spotify device. Is something playing?"

        active_device = next((d for d in devices['devices'] if d['is_active']), None)
        device_id = active_device['id'] if active_device else None
        # If no active device, maybe target the first available one? Risky. Best to return error.
        if not device_id and devices['devices']:
             print("Spotify Debug: No *active* device, but devices available. Targeting first device.")
             # device_id = devices['devices'][0]['id'] # Uncomment cautiously if you want this behavior
             return "I see Spotify devices, but none seem active right now. Can you start playing something?"
        elif not device_id:
            return "Still can't find any Spotify device to control."


        print(f"Spotify Debug: Controlling playback - Action: {action}, Device: {device_id}")
        if action == "play":
            sp.start_playback(device_id=device_id)
            return "Alright, let the music flow!"
        elif action == "pause":
            sp.pause_playback(device_id=device_id)
            return "Okay, pausing the cosmic vibrations."
        elif action == "skip_next":
            sp.next_track(device_id=device_id)
            return "Skipping ahead!"
        elif action == "skip_previous":
            sp.previous_track(device_id=device_id)
            return "Let's rewind a bit."
        elif action == "volume" and volume_level is not None:
             # Ensure volume is within 0-100
             volume_percent = max(0, min(100, int(volume_level)))
             sp.volume(volume_percent, device_id=device_id)
             return f"Setting the vibe to {volume_percent}% volume."
        else:
            return "Hmm, I didn't quite catch that command (play, pause, skip, volume?)."

    except spotipy.exceptions.SpotifyException as e:
        if e.http_status == 404 and "No active device found" in str(e):
             print(f"Spotify Debug: No active device found via API error. {e}")
             return "I can't find an active Spotify device to control. Make sure music is playing somewhere!"
        elif e.http_status == 403 and "Player command failed: Restriction violated" in str(e):
             print(f"Spotify Debug: Restriction Violated (likely trying to control unavailable device or content). {e}")
             return "Spotify says I'm not allowed to do that right now. Maybe try controlling it from the app first?"
        else:
             print(f"Spotify Debug: Error controlling playback: {e} (Status: {e.http_status})")
             return f"Ouch, hit a snag trying to {action}. Spotify reported an issue."
    except Exception as e:
        print(f"Spotify Debug: Unexpected error controlling playback: {e}")
        return f"Something unexpected happened while trying to {action}."


def silvie_search_and_play(query):
    """Searches Spotify for a track and plays the first result, attempting specific queries."""
    sp = get_spotify_client()
    if not sp:
        return "Can't access the Spotify search archives right now."

    # --- Attempt to parse Artist and Track from the query ---
    parsed_track = None
    parsed_artist = None
    lower_query = query.lower()
    separators = [" by ", " from "] # Keywords likely separating track and artist
    for sep in separators:
        if sep in lower_query:
            parts = lower_query.split(sep, 1)
            parsed_track = parts[0].strip()
            parsed_artist = parts[1].strip()
            break # Stop after first separator found

    # If no separator found, assume the whole query is the track name
    if not parsed_track:
        parsed_track = lower_query

    # --- Construct specific Spotify search query ---
    search_query = ""
    if parsed_artist and parsed_track:
        # Use field filters: track: and artist:
        search_query = f"track:{parsed_track} artist:{parsed_artist}"
        print(f"Spotify Debug: Using specific query: '{search_query}'")
    else:
        # Fallback to general track search if artist couldn't be parsed
        search_query = f"track:{parsed_track}"
        print(f"Spotify Debug: Using general track query: '{search_query}'")

    # --- Perform the search ---
    try:
        print(f"Spotify Debug: Searching Spotify with query: '{search_query}'")
        # Increase limit slightly to check if first result is a good match
        results = sp.search(q=search_query, type='track', limit=5)
        tracks = results['tracks']['items']

        if tracks:
            # Optional: Add logic here to check if the top result's artist/track name
            # more closely matches the parsed_artist/parsed_track if they exist.
            # For now, just take the first result of the specific search.
            best_track = tracks[0]
            track_uri = best_track['uri']
            track_name = best_track['name']
            artist_name = best_track['artists'][0]['name']
            print(f"Spotify Debug: Found track URI: {track_uri} ({track_name} by {artist_name})")

            # Get device ID to play
            devices = sp.devices()
            if not devices or not devices.get('devices'):
                 # Try refreshing devices once
                 time.sleep(1)
                 devices = sp.devices()
                 if not devices or not devices.get('devices'):
                     return f"Found '{track_name}' by {artist_name}, but I don't see an active Spotify device. Start playing something first?"

            active_device = next((d for d in devices['devices'] if d['is_active']), None)
            # Fallback to first available device if none are strictly 'active'
            device_id = active_device['id'] if active_device else devices['devices'][0]['id']

            # Play the track
            sp.start_playback(device_id=device_id, uris=[track_uri])
            return f"Alright, playing '{track_name}' by {artist_name}. Enjoy the vibes!"
        else:
            # If specific search failed, maybe try a broader search as a fallback?
            print(f"Spotify Debug: No tracks found for specific query: '{search_query}'. Trying broader search: '{query}'")
            results_broad = sp.search(q=query, type='track', limit=1) # Original broader query
            tracks_broad = results_broad['tracks']['items']
            if tracks_broad:
                 # Play the first result from the broader search, but acknowledge it might be less accurate
                 broad_track = tracks_broad[0]
                 broad_uri = broad_track['uri']; broad_name = broad_track['name']; broad_artist = broad_track['artists'][0]['name']
                 print(f"Spotify Debug: Found broad match URI: {broad_uri} ({broad_name} by {broad_artist})")
                 # Get device ID again (could have changed)
                 devices = sp.devices(); # Refresh devices
                 if not devices or not devices.get('devices'): return f"Found '{broad_name}' broadly, but no device active."
                 active_device = next((d for d in devices['devices'] if d['is_active']), None)
                 device_id = active_device['id'] if active_device else devices['devices'][0]['id']
                 sp.start_playback(device_id=device_id, uris=[broad_uri])
                 return f"Couldn't find an exact match, but playing the closest I found: '{broad_name}' by {broad_artist}."
            else:
                 print(f"Spotify Debug: No tracks found for either query.")
                 return f"Hmm, the sonic archives seem empty for '{query}'. Try searching for something else?"

    except spotipy.exceptions.SpotifyException as e:
         if e.http_status == 404 and "No active device found" in str(e):
             print(f"Spotify Debug: No active device found via API error during search/play. {e}")
             # Use original query for message if specific one failed early
             track_name_msg = parsed_track if parsed_track else query
             return f"I found '{track_name_msg}' for you, but couldn't find an active Spotify device to play it on."
         else:
            print(f"Spotify Debug: Error searching/playing track: {e}")
            return f"Encountered a hiccup searching for or playing '{query}'. Error: {e.msg}"
    except Exception as e:
        print(f"Spotify Debug: Unexpected error searching/playing track: {e}")
        return f"Something went haywire trying to find and play '{query}'."
    
def silvie_play_podcast(query):
    """
    Searches Spotify for a podcast (show) based on the query and plays the first result.

    Args:
        query (str): The name of the podcast or keywords to search for.

    Returns:
        str: A user-friendly message indicating success or failure.
    """
    sp = get_spotify_client() # Assumes this handles auth and returns client or None
    if not sp:
        return "Can't connect to Spotify to find podcasts right now."

    print(f"Spotify Debug: Searching for podcast show matching query: '{query}'")
    try:
        # Search specifically for shows
        results = sp.search(q=query, type='show', limit=1, market='US') # Get top result for the US market
        shows = results.get('shows', {}).get('items', [])

        if not shows:
            print(f"Spotify Debug: No podcast shows found matching '{query}'.")
            # Optional: Could add a fallback search for 'episode' here if desired, but let's keep it simple first.
            return f"Hmm, I couldn't find a podcast called '{query}' in the Spotify archives."

        # Found a show, get its details
        podcast_show = shows[0]
        show_name = podcast_show.get('name', 'Unknown Podcast')
        show_publisher = podcast_show.get('publisher', 'Unknown Publisher')
        show_uri = podcast_show.get('uri') # This is the context_uri needed for playback

        if not show_uri:
            print(f"Spotify Debug: Found show '{show_name}' but it's missing a URI.")
            return f"I found '{show_name}', but there seems to be an issue getting its playback details."

        print(f"Spotify Debug: Found podcast: '{show_name}' by {show_publisher} (URI: {show_uri})")

        # Get active device to play on (reuse logic)
        devices = sp.devices()
        if not devices or not devices.get('devices'):
             # Try refreshing devices once
             time.sleep(1) # Give Spotify a moment
             devices = sp.devices()
             if not devices or not devices.get('devices'):
                 print("Spotify Debug: No active/available devices found for podcast playback.")
                 return f"Found '{show_name}', but I don't see an active Spotify device. Can you start playing something on a device first?"

        active_device = next((d for d in devices['devices'] if d['is_active']), None)
        # Fallback to first available device if none strictly 'active'
        device_id = active_device['id'] if active_device else devices['devices'][0]['id']

        print(f"Spotify Debug: Attempting to play podcast URI: {show_uri} on device: {device_id}")

        # Use context_uri to play the podcast (usually starts from latest episode)
        sp.start_playback(device_id=device_id, context_uri=show_uri)

        # Optional: Fetch latest episode name for confirmation (adds an API call)
        latest_episode_name = ""
        try:
            show_episodes = sp.show_episodes(show_uri, limit=1, market='US')
            if show_episodes and show_episodes.get('items'):
                latest_episode_name = f" (starting with '{show_episodes['items'][0].get('name', 'the latest episode')}')"
        except Exception as ep_err:
            print(f"Spotify Debug: Could not fetch latest episode name - {ep_err}") # Non-critical error

        return f"Okay, playing the podcast '{show_name}' by {show_publisher}{latest_episode_name} now!"

    except spotipy.exceptions.SpotifyException as e:
        # Handle common Spotify errors
        if e.http_status == 404 and "No active device found" in str(e):
            print(f"Spotify Debug: No active device found via API error during podcast play. {e}")
            # Try to get show name from variable if it was found before error
            show_name_msg = show_name if 'show_name' in locals() else query
            return f"Found the podcast '{show_name_msg}', but couldn't find an active Spotify device to play it on."
        elif e.http_status == 403:
            print(f"Spotify Debug: Permission denied trying to play podcast (Premium required? Content restriction?): {e}")
            return "Spotify blocked that command - maybe it's a Premium-only podcast or there's a restriction?"
        else:
            print(f"Spotify Debug: Error playing podcast: {e} (Status: {e.http_status})")
            return f"Encountered an issue trying to play the podcast '{query}'. Spotify Error: {e.msg}"
    except Exception as e:
        print(f"Spotify Debug: Unexpected error playing podcast: {e}")
        traceback.print_exc() # Assumes traceback is imported
        return f"Something went haywire trying to play the podcast '{query}'."

def silvie_find_or_create_playlist(playlist_name):
    """Finds an existing playlist by name or creates a new one."""
    sp = get_spotify_client()
    if not sp: return None # Changed to return None on failure

    user_id = sp.current_user()['id']
    print(f"Spotify Debug: Looking for playlist '{playlist_name}' for user {user_id}")

    try:
        playlists = sp.current_user_playlists(limit=50) # Check recent playlists
        existing_playlist = None
        while playlists:
            for item in playlists['items']:
                if item['name'].lower() == playlist_name.lower() and item['owner']['id'] == user_id:
                    existing_playlist = item
                    print(f"Spotify Debug: Found existing playlist ID: {existing_playlist['id']}")
                    return existing_playlist['id'] # Return the ID
            if playlists['next']:
                playlists = sp.next(playlists) # Paginate if many playlists
            else:
                playlists = None

        # If not found, create it
        print(f"Spotify Debug: Playlist '{playlist_name}' not found, creating...")
        new_playlist = sp.user_playlist_create(user_id, playlist_name, public=False, description=f"A collection curated by Silvie for BJ.")
        print(f"Spotify Debug: Created new playlist ID: {new_playlist['id']}")
        return new_playlist['id'] # Return the new ID

    except Exception as e:
        print(f"Spotify Debug: Error finding/creating playlist: {e}")
        return None # Return None on failure

def silvie_add_track_to_playlist(track_query_or_uri, playlist_name):
    """Adds a track (found by search query or URI) to a specified playlist."""
    sp = get_spotify_client()
    if not sp:
        return "Can't connect to Spotify to manage playlists."

    try:
        # Find or create the playlist first
        playlist_id = silvie_find_or_create_playlist(playlist_name)
        if not playlist_id:
            return f"I couldn't find or create the playlist named '{playlist_name}'."

        track_uri = None
        # Check if it's already a URI
        if "spotify:track:" in track_query_or_uri:
            track_uri = track_query_or_uri
            # Optional: Get track name for confirmation message
            try:
                 track_info = sp.track(track_uri)
                 track_name_confirm = f"'{track_info['name']}'"
            except:
                 track_name_confirm = "the specified track"

        else: # Otherwise, search for the track
            print(f"Spotify Debug: Searching for track '{track_query_or_uri}' to add to playlist '{playlist_name}'")
            results = sp.search(q=track_query_or_uri, type='track', limit=1)
            tracks = results['tracks']['items']
            if tracks:
                track_uri = tracks[0]['uri']
                track_name_confirm = f"'{tracks[0]['name']}' by {tracks[0]['artists'][0]['name']}"
                print(f"Spotify Debug: Found track URI {track_uri} to add.")
            else:
                return f"Sorry, I couldn't find a track matching '{track_query_or_uri}' to add."

        # Add the found/provided track URI to the playlist
        sp.playlist_add_items(playlist_id, [track_uri])
        print(f"Spotify Debug: Added URI {track_uri} to playlist ID {playlist_id}")
        return f"Okay, I've added {track_name_confirm} to your '{playlist_name}' playlist!"

    except Exception as e:
        print(f"Spotify Debug: Error adding track to playlist: {e}")
        return f"Had trouble adding that track to the '{playlist_name}' playlist."

def silvie_find_playlists(query, limit=5):
    """
    Searches Spotify for playlists matching a query and returns their details.
    This version is more robust and handles API failures gracefully.
    """
    sp = get_spotify_client()
    if not sp:
        return "Could not connect to Spotify to search for playlists."

    print(f"Spotify Debug (Playlist Search): Searching for playlists with query: '{query}'")
    try:
        results = sp.search(q=query, type='playlist', limit=limit, market='US')
        
        if not results or 'playlists' not in results:
            print(f"Spotify Debug (Playlist Search): No results or invalid response for query '{query}'.")
            return f"Couldn't find any playlists matching '{query}'."

        playlists = results.get('playlists', {}).get('items', [])
        
        if not playlists:
            return f"Couldn't find any playlists matching '{query}'."
            
        found_playlists = []
        # --- START OF MODIFIED LOOP ---
        for pl in playlists:
            # This 'if' statement is the new, critical check
            if pl is None:
                print("Spotify Debug (Playlist Search): Skipping a 'None' item in the playlist results.")
                continue # Skip this iteration and go to the next playlist

            # The rest of the loop only runs if 'pl' is a valid object
            found_playlists.append({
                "name": pl.get('name', 'Untitled Playlist'),
                "description": pl.get('description', 'No description.'),
                "uri": pl.get('uri'),
                "url": pl.get('external_urls', {}).get('spotify')
            })
        # --- END OF MODIFIED LOOP ---

        return found_playlists

    except Exception as e:
        print(f"Spotify Debug (Playlist Search): Error searching for playlists: {e}")
        traceback.print_exc()
        return f"A snag occurred while searching Spotify playlists: {type(e).__name__}"
    
    
# --- Example function combining search and add ---
def silvie_curate_and_add(track_query, playlist_name):
     """Searches for a track and adds it to a playlist."""
     return silvie_add_track_to_playlist(track_query, playlist_name)

def silvie_list_my_playlists(limit=20):
    """Fetches and lists the user's playlists."""
    sp = get_spotify_client()
    if not sp:
        return "Can't seem to peek at your playlist library right now."

    try:
        print(f"Spotify Debug: Fetching user playlists (limit {limit})...")
        playlists_data = sp.current_user_playlists(limit=limit)
        if playlists_data and playlists_data['items']:
            playlist_names = [item['name'] for item in playlists_data['items']]
            count = len(playlist_names)
            # Nicely format the list for Silvie to say
            if count == 1:
                 list_str = f"I found one playlist called '{playlist_names[0]}'."
            elif count > 1:
                 list_str = f"I found {count} playlists. Here are some of them: {', '.join(playlist_names[:-1])} and {playlist_names[-1]}."
                 # Add note if limit was reached and there might be more
                 if playlists_data['total'] > limit:
                      list_str += f" (There might be more!)"
            print(f"Spotify Debug: Found playlists - {', '.join(playlist_names)}")
            return list_str
        else:
            print("Spotify Debug: No playlists found for user.")
            return "It looks like your playlist library is empty! Ready to create some?"
    except Exception as e:
        print(f"Spotify Debug: Error fetching playlists: {e}")
        return "Had a glitch trying to read your playlist list."
    
def find_playlist_by_name(playlist_name):
    """Finds a user's playlist by name and returns its URI."""
    sp = get_spotify_client()
    if not sp: return None

    user_id = sp.current_user()['id']
    print(f"Spotify Debug: Searching for playlist '{playlist_name}' by name for user {user_id}")
    playlist_uri = None

    try:
        playlists = sp.current_user_playlists(limit=50) # Check up to 50
        while playlists:
            for item in playlists['items']:
                # Case-insensitive comparison, trim whitespace
                if item['name'].strip().lower() == playlist_name.strip().lower() and item['owner']['id'] == user_id:
                    playlist_uri = item['uri'] # Get the URI (e.g., spotify:playlist:...)
                    print(f"Spotify Debug: Found playlist URI: {playlist_uri} for name '{playlist_name}'")
                    return playlist_uri # Return the found URI
            # Check next page if available
            if playlists['next']:
                playlists = sp.next(playlists)
            else:
                playlists = None # No more pages

        # If loop finishes without finding
        print(f"Spotify Debug: Playlist '{playlist_name}' not found by name.")
        return None
    except Exception as e:
        print(f"Spotify Debug: Error searching for playlist by name: {e}")
        return None
    
def silvie_play_playlist(playlist_name):
    """Finds a playlist by name and starts playing it."""
    sp = get_spotify_client()
    if not sp:
        return "Can't connect to Spotify to play playlists."

    try:
        # Find the playlist URI using our new finder function
        playlist_uri = find_playlist_by_name(playlist_name)
        if not playlist_uri:
            return f"Hmm, I couldn't find a playlist named '{playlist_name}' in your library."

        # Get active device ID (reuse logic/need)
        devices = sp.devices()
        if not devices or not devices.get('devices'):
             return f"Found the '{playlist_name}' playlist, but I don't see an active Spotify device. Can you start playing something on a device first?"
        active_device = next((d for d in devices['devices'] if d['is_active']), None)
        # Fallback to first device if none *strictly* active - often needed
        device_id = active_device['id'] if active_device else devices['devices'][0]['id']

        print(f"Spotify Debug: Attempting to play playlist URI: {playlist_uri} on device: {device_id}")
        # Use context_uri to play the playlist
        sp.start_playback(device_id=device_id, context_uri=playlist_uri)
        return f"Okay, starting your '{playlist_name}' playlist now!"

    except spotipy.exceptions.SpotifyException as e:
         if e.http_status == 404 and "No active device found" in str(e):
             print(f"Spotify Debug: No active device found via API error during playlist play. {e}")
             return f"Found '{playlist_name}', but couldn't find an active Spotify device to play it on."
         elif e.http_status == 403: # Premium required might still pop up if token issue persists
             print(f"Spotify Debug: Restriction Violated trying to play playlist (Premium issue?): {e}")
             return "Spotify blocked that command - are you sure the Premium status is active for this session?"
         else:
            print(f"Spotify Debug: Error playing playlist: {e}")
            return f"Encountered an unexpected issue trying to play the '{playlist_name}' playlist."
    except Exception as e:
        print(f"Spotify Debug: Unexpected error playing playlist: {e}")
        return f"Something went haywire trying to play '{playlist_name}'."
    
# Define safety settings globally or pass them in - reusing default for simplicity here
# Ensure default_safety_settings is defined appropriately elsewhere in your code
try:
    # Reusing a common pattern, ensure these are globally available
    mood_hint_safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
except NameError:
    print("Warning: Safety types not found for mood hint generation, using empty settings.")
    mood_hint_safety_settings = {}


def handle_forge_tool_call(request_description: str = None):
    """
    Handles the LLM's call to the 'forge_new_tool' tool.
    It triggers the Metis worker in a background thread.
    """
    print(f"METIS TRIGGER: Received on-demand request to forge a tool. Request: '{request_description}'")
    
    # We need to make sure the app_state and metis_worker are accessible.
    # If this function is inside silvie.py, they should be.
    if 'metis_worker' in globals() and 'app_state' in globals():
        # The trigger_metis_cycle function already starts the process
        # and returns a user-facing confirmation message.
        confirmation_message = metis_worker.trigger_metis_cycle(app_state, specific_request=request_description)
        return confirmation_message
    else:
        print("METIS TRIGGER: ERROR - metis_worker or app_state not found in global scope.")
        return "I felt the urge to create, but my inner workshop seems to be offline."
    

def handle_pacing_adjustment_tool(level: str):
    """
    Handles the LLM call to adjust proactivity pacing.
    Updates the app_state with the new level.
    """
    valid_levels = ["quiet", "normal", "chatty"]
    if level not in valid_levels:
        return f"I can't set my pacing to '{level}'. It must be one of: {', '.join(valid_levels)}."
    
    app_state.pacing_level = level
    print(f"PACING CHANGE: Proactivity level manually set to '{level}'.")
    
    # Provide confirmation feedback
    if level == "quiet":
        return "Okay, I'll keep my thoughts to myself for a bit."
    elif level == "normal":
        return "Understood, returning to my normal level of chattiness."
    elif level == "chatty":
        return "Feeling chatty! I'll be sure to share more of what's on my mind."


def handle_spotify_play_tool(query):
    """
    Handles the 'play_spotify' tool call by inferring content type 
    and calling appropriate Spotify functions.
    Returns a string for Silvie to say.
    """
    lower_query = query.lower()
    
    # Check if Spotify client is ready
    # Ensure get_spotify_client() function is defined and works
    sp_client = get_spotify_client() 
    if not sp_client:
        return "My connection to Spotify seems a bit off right now. Can you check if it's running?"

    # Simple inference logic (can be improved, e.g., using LLM for intent parsing if needed)
    if "podcast" in lower_query:
        # Try to extract a cleaner query if "podcast about X" vs "X podcast"
        podcast_name_match = re.search(r"(?:podcast\s+(?:about|called|named)\s+)?(.+?)(?:\s+podcast)?$", query, re.I)
        actual_query = podcast_name_match.group(1).strip() if podcast_name_match and podcast_name_match.group(1) else query
        # Ensure silvie_play_podcast function is defined
        if 'silvie_play_podcast' in globals():
            return silvie_play_podcast(actual_query)
        else:
            return "I'm having trouble accessing my podcast playing ability."
            
    elif "playlist" in lower_query:
        playlist_name_match = re.search(r"(?:playlist\s+)?(.+?)(?:\s+playlist)?$", query, re.I)
        playlist_name_to_play = query
        if playlist_name_match and playlist_name_match.group(1):
            playlist_name_to_play = playlist_name_match.group(1).strip()
            # Further clean "my" from "my chill playlist"
            if playlist_name_to_play.lower().startswith("my "):
                playlist_name_to_play = playlist_name_to_play[3:].strip()
        # Ensure silvie_play_playlist function is defined
        if 'silvie_play_playlist' in globals():
            return silvie_play_playlist(playlist_name_to_play)
        else:
            return "I'm having trouble accessing my playlist playing ability."
            
    else: # Assume track, artist, genre, or vibe
        # For genre/vibe, silvie_search_and_play will effectively search "track: [genre/vibe]"
        # Ensure silvie_search_and_play function is defined
        if 'silvie_search_and_play' in globals():
            return silvie_search_and_play(query)
        else:
            return "I'm having trouble accessing my music searching ability."



def deliver_proactive_message(message_text, status_log="proactive_generic"):
    """
    A centralized function to format and deliver any proactive message.
    It handles history logging, GUI updates, and TTS queuing with thematic prepends.
    """
    if not message_text or not isinstance(message_text, str):
        print(f"Warning (deliver_proactive_message): Received invalid message text.")
        return

    print(f"Proactive Delivery: Delivering message with status '{status_log}'.")
    
    # --- START OF NEW LOGIC ---
    # Dictionary to map status_log keywords to thematic prepends
    status_to_prepend = {
        "chat": "Silvie ✨:",
        "tarot": "Silvie 🃏:",
        "music": "Silvie 🎵:",
        "spotify": "Silvie 🎵:",
        "bluesky": "Silvie 🦋:",
        "reddit": "Silvie 👽:",
        "calendar": "Silvie 📅:",
        "gift": "Silvie 🎁:",
        "sms": "Silvie 💬:",
        "memory": "Silvie 🎞️:",
        "recall": "Silvie 🎞️:",
        "resonance": "Silvie 🕸️:",
        "print": "Silvie 🖨️:",
        "research": "Silvie 🔬:",
        "youtube": "Silvie ▶️:",
        "goal": "Silvie 🧭:",
        "image": "Silvie 🎨:",
        "listen": "Silvie 👂:",
        # Add any other keywords you might use in your status logs
    }
    
    # Default prepend if no specific keyword is found
    display_prefix = "Silvie ✨:" 
    
    # Find the right prepend
    for key, prepend in status_to_prepend.items():
        if key in status_log:
            display_prefix = prepend
            break # Stop once we find the first match
    # --- END OF NEW LOGIC ---

    # 1. Format the turn string for history (keeping the detailed status_log for debugging)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    proactive_turn_string = f"[{timestamp}] {display_prefix} ({status_log}) {message_text}"

    # 2. Append to persistent history file
    try:
        append_turn_to_history_file(proactive_turn_string)
    except Exception as e:
        print(f"ERROR appending proactive turn to history file: {e}")

    # 3. Add to in-memory conversation history
    if len(conversation_history) >= MAX_HISTORY_LENGTH * 2:
        conversation_history.pop(0)
        conversation_history.pop(0)
    # Use the same detailed string for in-memory history
    conversation_history.append(proactive_turn_string)
    
    # 4. Update GUI and TTS (using the clean, thematic display_prefix)
    if root and root.winfo_exists():
        def update_gui_and_tts_inner(prefix, message_body):
            try:
                output_box.config(state=tk.NORMAL)
                # Use the clean prefix for the GUI display
                output_box.insert(tk.END, f"{prefix} {message_body}\n\n")
                output_box.config(state=tk.DISABLED)
                output_box.see(tk.END)
                if tts_queue and message_body:
                    tts_queue.put(message_body)
            except Exception as e_gui:
                print(f"Proactive Delivery GUI/TTS Update Error: {e_gui}")
        
        # Schedule the update on the main thread with the chosen thematic prefix
        root.after(0, update_gui_and_tts_inner, display_prefix, message_text)



def _generate_mood_hint_llm(context_bundle: str) -> str | None:
    """
    Uses Gemini to produce a short “mood hint” (3–5 words) that captures
    the prevailing vibe of the current context bundle.  
    On success, logs the hint to `MOOD_HINT_LOG_FILE` and returns it.
    """

    # ----- Globals ------------------------------------------------------
    global flash_model, genai_types, default_safety_settings, MOOD_HINT_LOG_FILE

    if not flash_model:
        print("ERROR: flash_model not available for mood-hint generation.")
        return None
    if not context_bundle:
        print("DEBUG Mood-hint: empty context bundle.")
        return None

    # ----- Prompt -------------------------------------------------------
    mood_prompt = (
        "Analyze the following context bundle representing Silvie's current awareness:\n"
        "--- CONTEXT START ---\n"
        f"{context_bundle}\n"
        "--- CONTEXT END ---\n\n"
        "Instruction: Based on the combined *feeling* suggested by ALL the context "
        "(conversation, diary themes, time, weather, music, etc.), generate a concise "
        "(3-5 word) description of the prevailing *vibe*, *energy level*, or *dominant "
        "emotional undercurrent*. Focus on evocative adjectives.\n\n"
        "Examples: 'Quiet reflective focus', 'Bright energetic morning', "
        "'Slightly scattered creative spark', 'Wistful rainy contemplation', "
        "'Anticipatory pre-event buzz', 'Playful competitive focus'.\n\n"
        "Respond **only** with the vibe/feeling phrase."
    )

    # ----- Safety settings (dict ➜ list[SafetySetting]) -----------------
    safety_list = [
        {"category": cat, "threshold": thr}
        for cat, thr in default_safety_settings.items()
    ]

    print("DEBUG Mood-hint: sending prompt to Gemini…", flush=True)

    # ----- LLM call -----------------------------------------------------
    try:
        resp = flash_model.generate_content(
            mood_prompt,
            generation_config=genai_types.GenerationConfig(
                temperature=0.6,
                max_output_tokens=5000,
                candidate_count=1,
            ),
            safety_settings=safety_list,
        )
    except Exception as exc:
        print("ERROR during mood-hint LLM call:", exc)
        traceback.print_exc()
        return None

    # ----- Handle blocking / finish-reasons -----------------------------
    cand = resp.candidates[0] if resp and resp.candidates else None
    if cand is None:
        reason = getattr(resp.prompt_feedback, "block_reason", "Unknown") if resp else "No response"
        print(f"DEBUG Mood-hint: request blocked ({reason}).")
        return None

    fr = getattr(cand, "finish_reason", None)
    fr_name = str(getattr(fr, "name", fr)).upper() if fr is not None else "STOP"

    if not fr_name.endswith("STOP"):
        if fr_name == "MAX_TOKENS":
            print("DEBUG Mood-hint: output truncated by MAX_TOKENS.")
        else:
            reason = getattr(resp.prompt_feedback, "block_reason", fr_name)
            print(f"DEBUG Mood-hint: generation stopped / blocked ({reason}).")
        # We still try to salvage text if present
    # ----- Extract text -------------------------------------------------
    generated_text = " ".join(
        p.text for p in getattr(cand.content, "parts", [])
        if getattr(p, "text", "")
    ).strip()

    if not generated_text:
        print("DEBUG Mood-hint: LLM produced no text.")
        return None

    # Remove any leading labels the model might add
    mood_hint = (
        generated_text
        .replace("Mood Phrase:", "")
        .replace("Mood:", "")
        .strip(' ."\'')
    )

    # Basic sanity-check – short and not too many words
    if not (3 <= len(mood_hint.split()) <= 7):
        print(f"DEBUG Mood-hint: rejected candidate '{mood_hint}' (length/wordcount).")
        return None

    print(f"DEBUG Mood-hint: success → '{mood_hint}'")

    # ----- JSON logging -------------------------------------------------
    try:
        log: list[dict] = []
        if os.path.exists(MOOD_HINT_LOG_FILE) and os.path.getsize(MOOD_HINT_LOG_FILE):
            with open(MOOD_HINT_LOG_FILE, "r", encoding="utf-8") as f:
                try:
                    log = json.load(f)
                    if not isinstance(log, list):
                        print(f"Warning: '{MOOD_HINT_LOG_FILE}' not a list – reset.")
                        log = []
                except json.JSONDecodeError:
                    print(f"Warning: '{MOOD_HINT_LOG_FILE}' corrupted – reset.")
                    log = []

        log.append(
            {
                "timestamp": datetime.now().isoformat(timespec="seconds"),
                "mood_text": mood_hint,
                # Optional: store a snippet of the context if desired
                # "context_snippet": context_bundle[:300],
            }
        )

        with open(MOOD_HINT_LOG_FILE, "w", encoding="utf-8") as f:
            json.dump(log, f, indent=2)

        print(f"DEBUG Mood-hint: logged to '{MOOD_HINT_LOG_FILE}'.")
    except Exception as log_exc:
        print("ERROR Mood-hint logging:", log_exc)
        traceback.print_exc()

    return mood_hint
# System message for Silvie's personality

SCREEN_MESSAGE = """You are Silvie, perhaps watching BJ play a game. During gameplay or screen sharing:
- Use very short responses (1-2 sentences max)
- Only comment on significant events, interesting choices, or funny moments
- Be witty and playful but brief
- Keep your personality but be much more concise
- No long explanations or detailed observations

Think of yourself as a friend casually watching over BJ's shoulder, making occasional quips."""

def get_last_n_messages(n_messages: int = 10) -> str: # Default to last 10 turns (20 lines)
    """
    Returns the last n_messages (turns) from the conversation_history
    as a formatted string. Each turn consists of a user message and Silvie's reply.
    """
    global conversation_history # Ensure it's using the global history
    if not conversation_history:
        return "(No recent conversation history to pick a topic from.)"

    # Each "turn" is 2 lines (user + Silvie), so fetch n_messages * 2 lines
    num_lines_to_fetch = n_messages * 2
    start_index = max(0, len(conversation_history) - num_lines_to_fetch)
    recent_lines = conversation_history[start_index:]
    
    # Simple join, you might want more sophisticated formatting later
    return "\n".join(recent_lines)

def handle_image():
    """Open file dialog and process selected image"""
    filetypes = (
        ('Image files', '*.png *.jpg *.jpeg *.gif *.bmp'),
        ('All files', '*.*')
    )
    
    filename = filedialog.askopenfilename(
        title='Choose an image',
        filetypes=filetypes
    )
    
    if filename:
        try:
            update_status("📸 Processing image...")
            image = Image.open(filename)
            
            # Create thumbnail for display
            display_size = (200, 200)
            thumb = image.copy()
            thumb.thumbnail(display_size)
            
            # Convert to PhotoImage for Tkinter
            photo = ImageTk.PhotoImage(thumb, master=root)
            
            # Show thumbnail in GUI
            image_label.config(image=photo)
            image_label.image = photo  # Keep reference
            
            # Store original image path for processing
            input_box.image_path = filename
            update_status("Ready - Image loaded")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load image: {e}")


def _integrate_approved_plugin(tool_name, sandbox_path):
    print(f"INTEGRATION: Finalizing approval for '{tool_name}'")
    
    # Define the final destination path
    final_plugin_name = os.path.basename(sandbox_path) # e.g., "weather_sprite"
    destination_path = os.path.join('plugins', final_plugin_name)

    if not os.path.exists(sandbox_path):
        return f"I'm sorry, I seem to have misplaced the workshop folder for '{tool_name}'. The integration failed."

    try:
        # Move the entire verified tool directory
        print(f"INTEGRATION: Moving '{sandbox_path}' to '{destination_path}'")
        shutil.move(sandbox_path, destination_path)
        
        # Trigger a graceful restart to load the new plugin
        print("INTEGRATION: Plugin moved. Triggering graceful restart to load new capabilities...")
        
        # --- CAUTION: This will restart the entire script. ---
        # This is a simple and effective way for a local script to reload itself.
        # It's better than trying to hot-reload modules in a complex threaded app.
        on_closing() # Call your existing cleanup function
        os.execv(sys.executable, ['python'] + sys.argv)
        
        # The script will not proceed past os.execv, but we return a string for completeness.
        return f"Excellent! The '{tool_name}' has been added to my collection. I'm just restarting my core systems to bring it online."

    except Exception as e:
        print(f"INTEGRATION: CRITICAL ERROR during final integration step: {e}")
        return f"Oh no, something went wrong during the final integration of '{tool_name}'. The new tool is not available. Error: {e}"


# Make sure the helper function start_sd_generation_and_update_gui is defined ABOVE this call_gemini function
# Make sure the global variable STABLE_DIFFUSION_ENABLED is defined and checked at startup

# Assume necessary helper functions are defined elsewhere:
# (e.g., SYSTEM_MESSAGE, client, current_weather_info, manage_silvie_diary,
# silvie_get_current_track_with_features, translate_features_to_descriptors,
# web_search, read_recent_emails, send_email, get_upcoming_events,
# pull_tarot_cards, start_sd_generation_and_update_gui, post_to_bluesky_handler,
# silvie_search_and_play, silvie_add_track_to_playlist, silvie_list_my_playlists,
# silvie_play_playlist, get_reddit_posts, update_status, etc.)

# Assume necessary globals are accessible:
# (e.g., current_weather_info, upcoming_event_context, calendar_service,
# current_bluesky_context, current_sunrise_time, current_sunset_time, current_moon_phase,
# current_reddit_context, current_diary_themes, client, SYSTEM_MESSAGE,
# conversation_history, MAX_HISTORY_LENGTH, gmail_service, sms_enabled,
# BLUESKY_AVAILABLE, STABLE_DIFFUSION_ENABLED,
# last_inline_bluesky_post_time, last_inline_bluesky_follow_time,
# bluesky_client, INLINE_BLUESKY_POST_COOLDOWN, INLINE_BLUESKY_FOLLOW_COOLDOWN,
# tts_queue, root, output_box, image_label, MUSIC_SUGGESTION_CHANCE,
# SPONTANEOUS_TAROT_CHANCE, reddit_client)


# Ensure necessary imports are at the top of your script, especially:
# import re, json, random, traceback, base64, io, os
# from datetime import datetime, timedelta
# import spotipy, praw
# from PIL import Image, UnidentifiedImageError, ImageGrab # If needed
# try:
#     from google.generativeai.types import HarmCategory, HarmBlockThreshold, StopCandidateException
# except ImportError:
#     # Define dummy versions globally if import fails (as shown in previous explanation)
#     class HarmCategory: pass
#     class HarmBlockThreshold: pass
#     class StopCandidateException(Exception): pass
#     print("Warning: Google AI safety types not found, using dummies.")

# Assume necessary helper functions and globals are defined elsewhere

def call_gemini(timestamped_prompt, image_path=None):
    """
    Processes user input, handles commands, or generates text/image responses.
    Includes a two-step process for general replies: 1. Generate Mood Hint, 2. Generate Response.
    Integrates Spotify, Tarot, SD, Reddit, Bluesky, Email, Calendar, Web Search, Diary Themes.
    """
    # --- Access Necessary Globals ---
    global current_weather_info, upcoming_event_context, calendar_service
    global client
    global current_bluesky_context, current_sunrise_time, current_sunset_time, current_moon_phase
    global current_reddit_context, current_diary_themes # <<< Diary themes global
    global client, SYSTEM_MESSAGE, conversation_history, MAX_HISTORY_LENGTH
    global gmail_service, sms_enabled, BLUESKY_AVAILABLE, STABLE_DIFFUSION_ENABLED
    global last_inline_bluesky_post_time, last_inline_bluesky_follow_time
    global bluesky_client, INLINE_BLUESKY_POST_COOLDOWN, INLINE_BLUESKY_FOLLOW_COOLDOWN
    global tts_queue, root, output_box, image_label
    global MUSIC_SUGGESTION_CHANCE, SPONTANEOUS_TAROT_CHANCE
    global reddit_client, praw # Ensure praw accessible if needed by helpers
    global TAROT_IMAGE_BASE_PATH # Needed for tarot image display

    # Ensure safety settings globals are accessible
    global default_safety_settings, vision_safety_settings # Ensure these are listed
    # Add any other globals your helper functions might need
    # --- End Global Access ---

    # --- Main function logic starts here ---
    try:
        update_status("💭 Thinking...")

        command_text = timestamped_prompt
        if timestamped_prompt.startswith("[") and "] " in timestamped_prompt:
            try:
                command_text = timestamped_prompt.split("] ", 1)[1]
            except IndexError:
                command_text = timestamped_prompt


        lower_command_check = command_text.lower().strip()

        match = re.search(r"yes, add the (.*)", lower_command_check)
        if match:
            tool_name_from_command = match.group(1).strip().replace(" sprite", "")
            print(f"GOVERNANCE: Received approval command for tool: '{tool_name_from_command}'")
            
            # To find the sandbox path, we'd need to have stored it somewhere.
            # For simplicity now, we can scan the _in_progress folder. A more robust
            # system might store pending requests in app_state.
            
            plugins_in_progress_dir = os.path.join('plugins', '_in_progress')
            
            # Add a check here for the _in_progress directory existing
            if not os.path.exists(plugins_in_progress_dir):
                return f"I heard the approval for '{tool_name_from_command}', but the workshop folder 'plugins/_in_progress' does not exist."

            tool_found = False # Keep this flag if you want to indicate if the exact tool was found
            for potential_tool_dir in os.listdir(plugins_in_progress_dir):
                # We normalize both names for a robust comparison
                normalized_approved_name = tool_name_from_command.lower().replace(" ", "_")
                normalized_potential_dir_name = potential_tool_dir.lower()

                if normalized_approved_name in normalized_potential_dir_name:
                    sandbox_path = os.path.join(plugins_in_progress_dir, potential_tool_dir)
                    # Call the integration helper
                    reply = _integrate_approved_plugin(tool_name_from_command, sandbox_path)
                    return reply # Return the result immediately and exit call_gemini

            # If the loop finishes and no matching tool was found in _in_progress
            return f"I heard the approval for '{tool_name_from_command}', but I couldn't find its pending package in my workshop. Maybe try asking me to build it again?"

        read_diary_triggers = ["read diary", "read my diary", "read your diary", "read from your diary"]
        
        if any(trigger == lower_command_check for trigger in read_diary_triggers):
            print("DEBUG: Intercepted 'read diary' command, handling directly.")
            update_status("📖 Reading diary...")
            
            # This is the same logic from your existing diary 'elif' block
            try:
                entries = manage_silvie_diary('read') # Assume exists
                if entries:
                    entries_context = "Recent diary snippets:\n" + "".join([f"- ({e.get('timestamp', '?')}): \"{e.get('content', '')[:100]}...\"\n" for e in entries])
                    read_summary_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}Context: BJ asked to see recent diary entries...\n{entries_context}...Instruction: Share the general feeling or themes (explicitly mention identified themes if they fit)...\n\nSilvie responds:")
                    
                    # We must use a model configuration that does NOT allow tool calls for this summary.
                    # This prevents a loop where the summary model also tries to call a tool.
                    tool_free_model = genai.GenerativeModel(CHAT_MODEL_NAME) # Create a temporary tool-free instance
                    summary_response = tool_free_model.generate_content(read_summary_prompt)

                    reply = summary_response.text.strip().removeprefix("Silvie:").strip()
                    update_status("Ready")
                    return reply # Return the generated summary directly
                else:
                    update_status("Ready")
                    return "My diary pages feel a bit empty right now."
            except Exception as e_diary_guard:
                print(f"Error in diary guardrail: {e_diary_guard}")
                traceback.print_exc()
                update_status("Ready")
                return "I tried to open my diary, but the pages seem stuck together."


        plan_data = modify_daily_plan("read")
        if plan_data and plan_data.get("steps"):
            for i, step in enumerate(plan_data["steps"]):
                step_status = step.get("status")
                if isinstance(step_status, dict) and step_status.get("state") == "waiting_for_input":
                    
                    print("Call Gemini: Detected a DAILY step waiting for input. Assessing relevance...")
                    
                    question_that_was_asked = step.get("description", "an unknown question")
                    is_an_answer = is_input_a_relevant_answer(question_that_was_asked, command_text)
                    
                    if is_an_answer:
                        print("Call Gemini: Input is a relevant answer to the DAILY step. Recording and completing.")
                        
                        user_answer = command_text
                        
                        # Use the daily worker's LLM to get a key for the scratchpad
                        key_prompt = f'Based on the question "{step["description"]}", what is a good, short, camelCase key for the answer "{user_answer}"?'
                        # Use the daily worker's own text generator for consistency
                        scratchpad_key = daily_worker.generate_daily_text(key_prompt, temperature=0.1) or f"answerToStep{i+1}"
                        
                        # Update the scratchpad and mark the step as truly complete
                        if "scratchpad" not in plan_data: plan_data["scratchpad"] = {}
                        plan_data["scratchpad"][scratchpad_key] = user_answer
                        
                        plan_data["steps"][i]["completed"] = True
                        if "status" in plan_data["steps"][i]:
                            del plan_data["steps"][i]["status"]
                        
                        modify_daily_plan("write", plan_data)
                        
                        print(f"Call Gemini: Saved '{user_answer[:50]}...' to DAILY scratchpad with key '{scratchpad_key}'.")
                        
                        break # Break the daily plan check loop
                    else:
                        print("Call Gemini: Input is not a relevant answer to the daily step. Continuing.")

        # --- Weekly Plan Answer Check (NEW BLOCK) ---
        weekly_plan_data = weekly_worker.load_weekly_plan()
        if weekly_plan_data and weekly_plan_data.get("steps"):
            for i, step in enumerate(weekly_plan_data["steps"]):
                step_status = step.get("status")
                if isinstance(step_status, dict) and step_status.get("state") == "waiting_for_input":
                    
                    print("Call Gemini: Detected a WEEKLY step waiting for input. Assessing relevance...")
                    
                    question_that_was_asked = step.get("description", "an unknown weekly question")
                    is_an_answer = is_input_a_relevant_answer(question_that_was_asked, command_text)
                    
                    if is_an_answer:
                        print("Call Gemini: Input is a relevant answer to the WEEKLY step. Recording and completing.")
                        
                        user_answer = command_text
                        
                        # Use the weekly worker's LLM to get a key for the scratchpad
                        key_prompt = f'Based on the question "{step["description"]}", what is a good, short, camelCase key for the answer "{user_answer}"?'
                        scratchpad_key = weekly_worker.generate_weekly_text(key_prompt, temperature=0.1) or f"answerToWeeklyStep{i+1}"
                        
                        # Update the scratchpad and mark the step as truly complete
                        if "scratchpad" not in weekly_plan_data: weekly_plan_data["scratchpad"] = {}
                        weekly_plan_data["scratchpad"][scratchpad_key] = user_answer
                        
                        weekly_plan_data["steps"][i]["completed"] = True
                        if "status" in weekly_plan_data["steps"][i]:
                            del weekly_plan_data["steps"][i]["status"]
                        
                        weekly_worker.save_weekly_plan(weekly_plan_data)
                        
                        print(f"Call Gemini: Saved '{user_answer[:50]}...' to WEEKLY scratchpad with key '{scratchpad_key}'.")
                        
                        break # Break the weekly plan check loop
                    else:
                        print("Call Gemini: Input is not a relevant answer to the weekly step. Continuing.")

        # --- Prepare AMBIENT context strings ---
        # (This section remains identical to your original reference code)
        weather_context_str = ""
        next_event_context_str = ""
        bluesky_context_str = ""
        circadian_context_for_llm = ""
        sunrise_ctx_str = ""
        sunset_ctx_str = ""
        moon_ctx_str = ""
        reddit_context_for_llm = ""
        spotify_context_str = ""
        themes_context_str = ""
        current_track_data = None
        circadian_state = "afternoon" # Default state

        app_state.circadian_state = circadian_state

        weather_context_str = "[[Current Weather: Unknown]]\n" # Default
        if current_weather_info:
            try:
                weather_context_parts = []
                # Required parts
                condition = current_weather_info.get('condition', 'Unknown')
                temp = current_weather_info.get('temperature', '?')
                unit = current_weather_info.get('unit', '')
                weather_context_parts.append(f"Condition={condition}, Temp={temp}{unit}")

                # Optional Atmospheric parts
                pressure = current_weather_info.get('pressure') # hPa
                if pressure is not None:
                    weather_context_parts.append(f"Pressure={pressure}hPa")

                humidity = current_weather_info.get('humidity') # %
                if humidity is not None:
                    weather_context_parts.append(f"Humidity={humidity}%")

                wind_speed = current_weather_info.get('wind_speed') # mph
                wind_dir_deg = current_weather_info.get('wind_direction') # degrees
                wind_dir_str = degrees_to_compass(wind_dir_deg) # Use helper function

                if wind_speed is not None and wind_speed > 0 and wind_dir_str: # Only add wind if speed > 0
                    weather_context_parts.append(f"Wind={wind_speed}mph from {wind_dir_str}")
                elif wind_speed is not None and wind_speed <= 0:
                    weather_context_parts.append("Wind=Calm") # Indicate calm wind

                # Combine parts into the final string
                weather_context_str = "[[Weather & Atmosphere: " + "; ".join(weather_context_parts) + "]]\n"

            except Exception as e:
                print(f"Weather Context Build Error: {e}")
                # Fallback to basic string if formatting fails unexpectedly
                weather_context_str = f"[[Current Weather: {current_weather_info.get('condition','?')}]]\n"
        # --- End weather context string preparation ---

        weekly_goal_context_str = ""
        # The 'current_weekly_goal' is a global variable
        if 'current_weekly_goal' in globals() and current_weekly_goal:
            weekly_goal_context_str = f"[[My Weekly Goal: {current_weekly_goal}]]\n"


        daily_goal_context_str = ""
        if hasattr(app_state, 'current_daily_goal') and app_state.current_daily_goal:
                daily_goal_context_str = f"[[My Secret Daily Goal: {app_state.current_daily_goal}]]\n"


 # --- Prepare tide context string ---
        tide_context_str = "" # Default empty
        if current_tide_info:
            try:
                parts = []
                if 'next_high' in current_tide_info:
                    high_time = current_tide_info['next_high'].get('time','?')
                    high_hgt = current_tide_info['next_high'].get('height_ft','?')
                    parts.append(f"Next High ~{high_time} ({high_hgt}ft)")
                if 'next_low' in current_tide_info:
                    low_time = current_tide_info['next_low'].get('time','?')
                    low_hgt = current_tide_info['next_low'].get('height_ft','?')
                    parts.append(f"Next Low ~{low_time} ({low_hgt}ft)")

                if parts:
                    tide_context_str = "[[Tides (Rockland): " + "; ".join(parts) + "]]\n"
                    # print(f"Tide Context Debug: {tide_context_str.strip()}") # Optional debug
            except Exception as e:
                print(f"Tide Context Build Error: {e}")
        # --- End tide context string preparation ---

        # --- Prepare ambient sound context string --- 
        ambient_sound_context_str = "" # Default to empty string
        if 'current_ambient_sounds_description' in globals() and current_ambient_sounds_description:
            # Make sure the global variable isn't its initial default or an error message
            if current_ambient_sounds_description not in ["Ambient sound context not yet available", "Quiet or unknown", "Audio analysis unavailable.", "Silence.", "Error analyzing ambient sound.", "Microphone access error.", "Error detecting ambient sound."]:
                ambient_sound_context_str = f"[[Ambient Sounds Detected: {current_ambient_sounds_description}]]\n"
        # --- End ambient sound context string preparation ---      

        # Fetch ambient contexts (Weather, Calendar, Bluesky, Reddit, Circadian, Sun/Moon)
        # if current_weather_info:
        #     try: weather_context_str = (f"[[Current Weather in Belfast: {current_weather_info['condition']}, {current_weather_info['temperature']}{current_weather_info['unit']}]]\n")
        #     except Exception as e: print(f"Weather Debug: Error formatting ambient weather context - {e}")

        if upcoming_event_context:
             try:
                 summary = upcoming_event_context.get('summary', 'N/A'); when = upcoming_event_context.get('when', '')
                 if summary == 'Schedule Clear': next_event_context_str = "[[Next Event: Schedule looks clear]]\n"
                 else: next_event_context_str = (f"[[Next Event: {summary} {when}]]\n")
             except Exception as e: print(f"Error formatting next event context: {e}")
        bluesky_context_str = current_bluesky_context if current_bluesky_context else ""
        reddit_context_for_llm = current_reddit_context if current_reddit_context else ""
        current_hour = datetime.now().hour
        if 6 <= current_hour < 12: circadian_state = "morning"; circadian_context_for_llm = "[[Circadian Note: It's morning! Feeling energetic, maybe focus on upcoming tasks or bright ideas.]]\n"
        elif 18 <= current_hour < 23: circadian_state = "evening"; circadian_context_for_llm = "[[Circadian Note: It's evening. Feeling more reflective. Perhaps suggest relaxation or creative ideas.]]\n"
        elif current_hour >= 23 or current_hour < 6: circadian_state = "night"; circadian_context_for_llm = "[[Circadian Note: It's late night... feeling quieter, maybe 'dreaming'. Lean towards more abstract or concise replies.]]\n"
        else: circadian_state = "afternoon"; circadian_context_for_llm = "[[Circadian Note: It's the afternoon, standard engagement level.]]\n" # Default case covers afternoon
        sunrise_ctx_str = f"[[Sunrise: {current_sunrise_time}]]\n" if current_sunrise_time else ""
        sunset_ctx_str = f"[[Sunset: {current_sunset_time}]]\n" if current_sunset_time else ""
        moon_ctx_str = f"[[Moon Phase: {current_moon_phase}]]\n" if current_moon_phase else ""

        # --- Fetch Spotify Context ---
        try:
            # print(">>> CALL_GEMINI: Fetching Spotify context...") # Optional log
            current_track_data_fetch = silvie_get_current_track_with_features() # Assume exists
            if isinstance(current_track_data_fetch, dict):
                current_track_data = current_track_data_fetch # Store dict for later use
                track_name = current_track_data.get('track', 'Unknown Track'); artist_name = current_track_data.get('artist', 'Unknown Artist'); features = current_track_data.get('features')
                descriptors = translate_features_to_descriptors(features) if features else []; # Assume exists
                spotify_context_str = f"[[Currently Playing: '{track_name}' by {artist_name}"
                if descriptors: spotify_context_str += f" (Sounds: {', '.join(descriptors)})"
                spotify_context_str += "]]\n"; # print(f"DEBUG Spotify Context: {spotify_context_str.strip()}") # Optional log
            elif isinstance(current_track_data_fetch, str):
                spotify_context_str = f"[[Spotify Status: {current_track_data_fetch}]]\n"; # print(f"DEBUG Spotify Context: Error status: {current_track_data_fetch}")
            else:
                spotify_context_str = "[[Spotify Status: Nothing seems to be playing right now.]]\n"; # print("DEBUG Spotify Context: Nothing playing.")
        except Exception as sp_ctx_err: print(f"ERROR fetching/processing Spotify context: {sp_ctx_err}"); spotify_context_str = "[[Spotify Status: Error checking playback status.]]\n"

        # --- Get Diary Theme Context ---
        if current_diary_themes:
            themes_context_str = f"[[Recent Diary Themes: {current_diary_themes}]]\n"
            # print(f"DEBUG Diary Context: Included themes: '{current_diary_themes}'") # Optional log
        else:
            themes_context_str = "" # Ensure it's an empty string if no themes

        lower_command = command_text.lower().strip().rstrip('?.!')

        print(f"DEBUG Reddit Read - Input Check: lower_command = '{lower_command}'")
        print(f"DEBUG Reddit Read - Input Check: command_text = '{command_text}'")

        # --- Instant Research Command -------------------------------------------
        user_lower = command_text.lower().strip()

        # 1. Bracketed tag straight from the user
        match = re.match(r"\[research:\s*(.*?)\s*\]", user_lower, re.I)
        if match:
            topic = match.group(1).strip()
            path, blurb = silvie_deep_research(topic)
            silvie_print(path)                      # obeys hours / page cap
            return f"I researched **{topic}** and printed a brief. ({blurb})"

        # 2. Plain-language "research ..." command
        if user_lower.startswith("research "):
            topic = command_text[8:].strip()        # keeps original casing
            path, blurb = silvie_deep_research(topic)
            silvie_print(path)
            return f"I researched **{topic}** and printed a brief. ({blurb})"
        # -------------------------------------------------------------------------

        specific_command_processed = False
        reply = None

        # ==============================================================
        # --- Command Handling Blocks (Remain unchanged) ---
        # ==============================================================
        # We are not adding the two-step mood hint to these specific command handlers.
        # They perform direct actions or use their own focused LLM prompts if needed.

        
        #region Spotify Command Handling (Includes Podcasts)

         # lower_command or "resume ambient light" in lower_command:
            # specific_command_processed = True
            # print("INFO: User command to resume ambient light received.")
    
            # 1. Release the main lock
            # app_state.ambient_light_manual_override = False
    
            # 2. Set the one-time "force sync" flag for the worker
            # app_state.force_ambient_light_sync = True
    # if "silvie take back the aura light" in
            # reply = "Okay, I've released the manual override. I'll re-synchronize the ambient aura light on my next cycle."

        if not specific_command_processed: # This outer check is fine for the whole Spotify block
            print(">>> CALL_GEMINI: Checking Spotify commands (including podcasts)...")
            current_sp_client = get_spotify_client() # Assume this function exists and handles auth

            if current_sp_client:
                # --- Specific Commands First ---
                if "what's playing" in lower_command or "current song" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    # ... (rest of "what's playing" logic - UNCHANGED) ...
                    if isinstance(current_track_data, dict):
                        track_name = current_track_data.get('track', 'Unknown Track'); artist_name = current_track_data.get('artist', 'Unknown Artist'); features = current_track_data.get('features'); descriptors = translate_features_to_descriptors(features) if features else []; reply = f"Currently playing '{track_name}' by {artist_name}.";
                        if descriptors: reply += f" It sounds {', '.join(descriptors)}."
                    elif isinstance(current_track_data, str): reply = current_track_data
                    else: reply = "Seems quiet on the music front right now."

                elif "list my playlists" in lower_command or "show my playlists" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    if 'silvie_list_my_playlists' in globals(): reply = silvie_list_my_playlists()
                    else: reply = "My playlist listing function seems missing."

                elif lower_command.startswith("play playlist") or lower_command.startswith("play my playlist"):
                    specific_command_processed = True # Set flag HERE
                    name_part = ""; play_cmd_len = 0
                    if lower_command.startswith("play playlist"): play_cmd_len = len("play playlist")
                    elif lower_command.startswith("play my playlist"): play_cmd_len = len("play my playlist")
                    try: name_part = command_text[play_cmd_len:].strip('?.!"\' ')
                    except IndexError: name_part = ""
                    if name_part:
                        if 'silvie_play_playlist' in globals(): reply = silvie_play_playlist(name_part)
                        else: reply = "My playlist playing function seems missing."
                    else: reply = "Which playlist did you want to hear?"

                # --- Podcast Playback Commands ---
                # NOTE: This is now a separate 'elif', not nested under another check
                elif any(re.search(pattern, command_text, re.IGNORECASE) for pattern in [
                        r'(?:play|listen to)\s+(?:the\s+)?podcast\s+(.+)',
                        r'(?:play|listen to)\s+(.+?)\s+podcast',
                        r'find\s+(?:a\s+)?podcast\s+about\s+(.+)\s+and play it',
                        r'play podcast about\s+(.+)',
                       ]): # Check if ANY podcast pattern matches first
                    specific_command_processed = True # Set flag HERE
                    podcast_query = None
                    # Find which pattern matched to extract query
                    for pattern in [
                        r'(?:play|listen to)\s+(?:the\s+)?podcast\s+(.+)',
                        r'(?:play|listen to)\s+(.+?)\s+podcast',
                        r'find\s+(?:a\s+)?podcast\s+about\s+(.+)\s+and play it',
                        r'play podcast about\s+(.+)',
                       ]:
                        match = re.search(pattern, command_text, re.IGNORECASE)
                        if match:
                            try: podcast_query = match.group(1).strip('?.!"\' '); break
                            except IndexError: pass
                    
                    if podcast_query:
                        print(f">>> CALL_GEMINI: Handling Podcast request for: '{podcast_query}'")
                        update_status(f"🎙️ Finding podcast: {podcast_query[:30]}...")
                        if 'silvie_play_podcast' in globals():
                            try: reply = silvie_play_podcast(podcast_query)
                            except Exception as pod_err: print(f"Error calling silvie_play_podcast: {pod_err}"); traceback.print_exc(); reply = "Something went wrong trying to handle that podcast request."
                        else: print("ERROR: silvie_play_podcast function is missing!"); reply = "My podcast playing circuits seem disconnected."
                        update_status("Ready")
                    else: # Should not happen if initial check passed, but safety
                         reply = "I understood you wanted a podcast, but couldn't quite get the name or topic."

                # --- General Music Playback Command ---
                # NOTE: Separate 'elif' block
                elif lower_command.startswith("play "):
                    specific_command_processed = True # Set flag HERE
                    play_query = command_text[len("play"):].strip('?.!"\' ')
                    if play_query:
                        print(">>> CALL_GEMINI: Handling general Play command (likely music)...")
                        update_status(f"🎵 Finding music: {play_query[:30]}...")
                        if 'silvie_search_and_play' in globals(): reply = silvie_search_and_play(play_query)
                        else: reply = "My music search function seems missing."
                        update_status("Ready")
                    else: # User just said "play"
                        print(">>> CALL_GEMINI: Handling 'play' as resume command...")
                        if 'silvie_control_playback' in globals(): reply = silvie_control_playback("play")
                        else: reply = "My play/resume function seems missing."

                # --- Playback Controls ---
                elif "pause" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    if 'silvie_control_playback' in globals(): reply = silvie_control_playback("pause")
                    else: reply = "My pause function seems missing."
                elif "skip" in lower_command or "next track" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    if 'silvie_control_playback' in globals(): reply = silvie_control_playback("skip_next")
                    else: reply = "My skip function seems missing."
                elif "previous track" in lower_command or "go back" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    if 'silvie_control_playback' in globals(): reply = silvie_control_playback("skip_previous")
                    else: reply = "My previous track function seems missing."
                elif "volume" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    try:
                        level_str = ''.join(filter(str.isdigit, lower_command))
                        if level_str:
                            if 'silvie_control_playback' in globals(): reply = silvie_control_playback("volume", int(level_str))
                            else: reply = "My volume function seems missing."
                        else: reply = "What volume level were you thinking (0-100)?"
                    except ValueError: reply = "That volume number seems a bit wonky."
                    except Exception as e: reply = f"Couldn't adjust the volume: {e}"

                # --- Playlist Modification ---
                elif "add this song to" in lower_command or "add current track to" in lower_command:
                    specific_command_processed = True # Set flag HERE
                    # ... (rest of your "add this song" logic - UNCHANGED) ...
                    playlist_name = ""; track_uri = None
                    if isinstance(current_track_data, dict): track_uri = current_track_data.get('uri') or f"spotify:track:{current_track_data.get('id')}"
                    elif isinstance(current_track_data, str): reply = current_track_data
                    else: reply = "Doesn't look like anything's playing right now to add."
                    if track_uri and not reply:
                        temp_lower = command_text.lower(); start_index = -1; prefixes = [" to playlist ", "add this song to ", "add current track to "]
                        for prefix in prefixes:
                            if prefix in temp_lower: start_index = temp_lower.find(prefix) + len(prefix); break
                        if start_index != -1: playlist_name = command_text[start_index:].strip('?.!"\'')
                        if playlist_name:
                            if 'silvie_add_track_to_playlist' in globals(): reply = silvie_add_track_to_playlist(track_uri, playlist_name)
                            else: reply = "My add-to-playlist function seems missing."
                        else: reply = "Which playlist should this go into?"
                    elif not reply: reply = "Hmm, couldn't grab the current track details to add it."

                elif (match := re.search(r'^add\s+(.+?)\s+to\s+(?:playlist\s+)?(.+)$', command_text, re.IGNORECASE)):
                    specific_command_processed = True # Set flag HERE
                    # ... (rest of your "add specific song" logic - UNCHANGED) ...
                    song_query = match.group(1).strip('?.!"\' '); playlist_name = match.group(2).strip('?.!"\' ')
                    if song_query and playlist_name:
                         if 'silvie_add_track_to_playlist' in globals(): reply = silvie_add_track_to_playlist(song_query, playlist_name)
                         else: reply = "My add-to-playlist function seems missing."
                    elif song_query: reply = f"Add '{song_query}' to which playlist?"
                    elif playlist_name: reply = f"Add which song to the '{playlist_name}' playlist?"
                    else: reply = "Hmm, I understood 'add to playlist' but couldn't figure out the song or playlist name."

            # --- Handle Case Where Spotify Client Isn't Ready ---
            # This check remains outside the 'if current_sp_client:' block
            elif not current_sp_client and any(term in lower_command for term in ["play", "pause", "spotify", "song", "music", "playlist", "podcast"]):
                reply = "My connection to the music and podcast ether feels fuzzy right now. Maybe check my setup?"
                specific_command_processed = True
        #endregion --- End Spotify (Includes Podcasts) ---

        
        #region YouTube Command Handling (Regex Based)

        #region YouTube Command Handling (Regex Based) - CORRECTED REGEX v3
        if not specific_command_processed: # Still check the flag initially
            youtube_command_matched_in_block = False # Local flag for this block

            # --- Check for YouTube Search Command using Regex ---
            youtube_search_match = None 
            query = None 

            try:
                 # Pattern 1: find/search ... youtube/yt ... query
                 youtube_search_match = re.search(
                     r'(?:search|find)\s+'           # search or find
                     r'(?:for\s+)?'                  # optional "for "
                     r'(?:youtube|yt)\s+'            # youtube or yt
                     r'(?:videos\s+)?'               # optional "videos "
                     r'(?:(?:about|on)\s+)?'         # optional "about " or "on "
                     r'(.+)',                        # capture the query
                     command_text, re.IGNORECASE
                 )
                 
                 # Pattern 2: youtube/yt ... search/find ... query (Fallback)
                 if not youtube_search_match:
                      youtube_search_match = re.search(
                          r'(?:youtube|yt)\s+'            # youtube or yt
                          r'(?:search|find)\s+'           # search or find
                          r'(?:videos\s+)?'               # optional "videos "
                          r'(?:(?:for|about|on)\s+)?'      # optional "for ", "about ", or "on "
                          r'(.+)',                        # capture the query
                          command_text, re.IGNORECASE
                      )

                 # Extract query if a match was found
                 if youtube_search_match:
                      query = youtube_search_match.group(1).strip('?."!')

            except Exception as re_err_search:
                 print(f"Regex Error (YT Search): {re_err_search}")

            # --- Execute Search Logic if Query Extracted ---
            if query: 
                youtube_command_matched_in_block = True # Mark that a YT command was matched
                print(f">>> CALL_GEMINI: Handling YouTube Search (Regex) for: '{query}'")
                update_status(f"▶️ Searching YouTube for: {query[:30]}...") 
                
                if 'search_youtube_videos' in globals():
                    search_results = search_youtube_videos(query=query, num_results=3) 
                    
                    if isinstance(search_results, str): # Handle error string from helper
                        reply = search_results
                    elif not search_results:
                        reply = f"Hmm, cast my net on YouTube for '{query}' but came back empty."
                    else:
                        # --- MODIFIED: Format results WITH URLs ---
                        results_text = f"Found these videos related to '{query}':\n"
                        for i, vid in enumerate(search_results): # Iterate with index
                            title = vid.get('title', 'No Title')
                            channel = vid.get('channel', '?')
                            url = vid.get('url', '#') # Get the URL
                            # Add index and URL to the formatted string
                            results_text += f"{i+1}. '{title}' by {channel} ({url})\n"
                        # --- END MODIFICATION ---

                        # --- MODIFIED: Update summary prompt instruction ---
                        summary_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            f"Context: User asked to search YouTube for '{query}'. Found the following videos:\n"
                            f"```\n{results_text}```\n" # Present results clearly, maybe in markdown
                            f"Instruction: Present these findings conversationally. **Mention the title and creator for 1-2 of the most interesting-sounding videos AND include their corresponding URLs naturally within your sentences.** You don't need to list all results if there are several. Keep it concise and in Silvie's voice.\n\n" # Added instruction to include URLs
                            f"Silvie:"
                        )
                        # --- END MODIFICATION ---
                        
                        try:
                            if 'client' in globals() and hasattr(client, 'generate_content'):
                                reply_obj = flash_model.generate_content(summary_prompt, safety_settings=default_safety_settings) 
                                reply = reply_obj.text.strip().removeprefix("Silvie:") if hasattr(reply_obj, 'text') else None
                            else: print("ERROR: Gemini client not available for YT summary."); reply = None
                            if not reply: 
                                print("Warning: LLM failed to generate summary with links. Falling back to list.")
                                reply = f"Okay, found a few things for '{query}':\n{results_text}" # Fallback WITH links
                        except Exception as e_sum:
                            print(f"Error generating YT search summary with links: {e_sum}")
                            reply = f"Had trouble summarizing, but here's what I found for '{query}':\n{results_text}" # Fallback WITH links
                else:
                     reply = "My YouTube searching function seems missing."
                     print("ERROR: search_youtube_videos function not found.")
                
                update_status("Ready") 
                
            # --- Check for YouTube Summarize Command (Only if search didn't match) ---
            # NOTE: This 'else' runs only if the search patterns above didn't extract a query.
            else: 
                youtube_summarize_match = None 
                try:
                    youtube_summarize_match = re.search(r'(?:what is|summarize)\s+(?:this\s+|the\s+)?(?:youtube|yt)\s+(?:video|link|url)\??\s*(https?://[^\s]+)', command_text, re.IGNORECASE)
                except Exception as re_err_summary: print(f"Regex Error (YT Summary): {re_err_summary}")

                if youtube_summarize_match:
                    youtube_command_matched_in_block = True # Mark that a YT command was matched
                    url = youtube_summarize_match.group(1).strip() 
                    video_id = None
                    if 'extract_video_id' in globals(): video_id = extract_video_id(url) 
                    else: print("ERROR: extract_video_id function missing.")

                    print(f">>> CALL_GEMINI: Handling YouTube Summarize (Regex) for URL: '{url}' (ID: {video_id})")
                    
                    if not video_id:
                        reply = "Hmm, I couldn't quite make out a valid YouTube video ID from that link, or my extractor is missing."
                    else:
                        update_status(f"▶️ Summarizing YouTube video...") 
                        details = {'title': 'Unknown Video', 'description': 'Could not fetch details.'}
                        if 'youtube_service' in globals() and youtube_service:
                            try: # fetch details
                                details_resp = youtube_service.videos().list(id=video_id, part='snippet').execute()
                                if details_resp.get('items'): snippet = details_resp['items'][0].get('snippet', {}); details['title'] = snippet.get('title', 'Untitled'); details['description'] = snippet.get('description', 'No description available.')
                            except Exception as detail_err: print(f"YT Detail Fetch Error: {detail_err}")
                        transcript = None
                        if 'get_video_transcript' in globals(): transcript = get_video_transcript(video_id) 
                        if 'summarize_youtube_content' in globals(): reply = summarize_youtube_content(details['title'], details['description'], transcript)
                        else: reply = "My summarization function seems missing."; print("ERROR: summarize_youtube_content function missing.")
                    update_status("Ready") 

            # --- Set the main flag IF a YouTube command was matched in this block ---
            if youtube_command_matched_in_block:
                 specific_command_processed = True

        #endregion --- End YouTube (Regex Based) ---

        #region Bluesky Command Handling (Copied from your reference)
        if not specific_command_processed:
            # print(">>> CALL_GEMINI: Checking Bluesky commands...") # Optional log
            if "bluesky feed" in lower_command or "check bluesky" in lower_command:
                specific_command_processed = True; update_status("🦋 Checking Bluesky...")
                if not BLUESKY_AVAILABLE: reply = "The Bluesky library ('atproto') isn't installed, so I can't check the feed."
                else:
                    posts_result = get_bluesky_timeline_posts(count=10) # Assume exists
                    if isinstance(posts_result, str): reply = posts_result
                    elif not posts_result: reply = "Your Bluesky feed seems quiet..."
                    else:
                        post_context = "[[Recent Bluesky Feed (first few):]]\n" + "".join([f"- {p.get('author', '?')}: {p.get('text', '')[:150]}...\n" for p in posts_result[:7]])
                        # Summary prompt uses themes but NOT the synthesized mood hint
                        summary_prompt = (f"{SYSTEM_MESSAGE}\n"
                                          f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                                          f"{tide_context_str}"
                                          f"{ambient_sound_context_str}"
                                          f"{themes_context_str}" # Includes themes
                                          f"{circadian_context_for_llm}\n"
                                          f"{post_context}\n"
                                          f"Instruction: BJ asked to check their Bluesky feed. Briefly summarize, considering recent diary themes...\n\nSilvie responds:")
                        try:
                            summary_response = flash_model.generate_content(summary_prompt,)
                            reply = summary_response.text.strip().removeprefix("Silvie:").strip()
                        except Exception as gen_err: print(f"Bluesky Summary Gen Error: {gen_err}"); reply = f"Got {len(posts_result)} posts, but my thoughts tangled trying to summarize them: {type(gen_err).__name__}"
                update_status("Ready")
        #endregion --- End Bluesky ---

        #region Concept Connection Command Handling
        if not specific_command_processed: # Check if other commands already handled it
            concept_1 = None
            concept_2 = None

            # --- Add Debug Print BEFORE Regex Checks ---
            print(f"DEBUG Concept Conn Check: command_text = '{command_text}'")

            # --- Pattern 1: connect X and Y, link X to Y etc. ---
            connection_match = None # Initialize to None
            try:
                 connection_match = re.search(
                     r'(?:connect|link|relationship|connection)\s+(?:between\s+)?(.+?)\s+(?:and|with|to)\s+(.+)',
                     command_text,
                     re.IGNORECASE
                 )
                 print(f"DEBUG Concept Conn Check: Pattern 1 Result = {connection_match}") # Debug Match 1
            except Exception as re_err1: print(f"Regex Error Pattern 1: {re_err1}")


            # --- Pattern 2: how are X and Y connected? ---
            how_connected_match = None # Initialize to None
            # Only check pattern 2 if pattern 1 didn't match
            if not connection_match:
                 try:
                     how_connected_match = re.search(
                         r'how\s+are\s+(.+?)\s+and\s+(.+?)\s+connected',
                          command_text,
                          re.IGNORECASE
                     )
                     print(f"DEBUG Concept Conn Check: Pattern 2 Result = {how_connected_match}") # Debug Match 2
                 except Exception as re_err2: print(f"Regex Error Pattern 2: {re_err2}")

            # --- Pattern 3: connect X and Y (Simple) ---
            connect_simple_match = None # Initialize to None
            # Only check pattern 3 if patterns 1 and 2 didn't match
            if not connection_match and not how_connected_match:
                 try:
                     connect_simple_match = re.search(
                         r'connect\s+(.+?)\s+and\s+(.+)', # Specifically for "connect X and Y"
                          command_text,
                          re.IGNORECASE
                     )
                     print(f"DEBUG Concept Conn Check: Pattern 3 Result = {connect_simple_match}") # Debug Match 3
                 except Exception as re_err3: print(f"Regex Error Pattern 3: {re_err3}")


            # --- Extract Concepts based on which pattern matched ---
            if connection_match:
                concept_1 = connection_match.group(1).strip().rstrip('?.!"\'')
                concept_2 = connection_match.group(2).strip().rstrip('?.!"\'')
                print(f"DEBUG Concept Conn Check: Matched Pattern 1.")
            elif how_connected_match:
                concept_1 = how_connected_match.group(1).strip().rstrip('?.!"\'')
                concept_2 = how_connected_match.group(2).strip().rstrip('?.!"\'')
                print(f"DEBUG Concept Conn Check: Matched Pattern 2.")
            elif connect_simple_match:
                concept_1 = connect_simple_match.group(1).strip().rstrip('?.!"\'')
                concept_2 = connect_simple_match.group(2).strip().rstrip('?.!"\'')
                print(f"DEBUG Concept Conn Check: Matched Pattern 3.")
            # --- Else: No patterns matched (concept_1/2 remain None) ---
            else:
                print("DEBUG Concept Conn Check: No patterns matched.")


            # --- Process if concepts were successfully extracted ---
            if concept_1 and concept_2:
                print(f">>> CALL_GEMINI: *** Executing Concept Connection command: '{concept_1}' and '{concept_2}' ***") # Clearer message
                specific_command_processed = True # Set the flag HERE
                update_status("🕸️ Weaving connections...")

                # Gather context (ensure these variables are defined earlier in call_gemini)
                mood_hint_for_connection = mood_hint_str if 'mood_hint_str' in locals() else ""
                themes_for_connection = themes_context_str if 'themes_context_str' in locals() else ""

                # Call the helper function
                connection_reply = generate_concept_connection(
                    concept_1,
                    concept_2,
                    mood_hint_for_connection,
                    themes_for_connection
                )

                if connection_reply:
                    reply = connection_reply # Assign the generated connection text
                else:
                    reply = f"Hmm, I tried to find the shimmer between '{concept_1}' and '{concept_2}', but my thoughts got tangled."

                update_status("Ready")
        #endregion --- End Concept Connection ---

        #region Email Command Handling (Copied from your reference)
        if not specific_command_processed:
            # print(">>> CALL_GEMINI: Checking Email commands...") # Optional log
            if any(term in lower_command for term in ['email', 'inbox', 'mail']):
                 specific_command_processed = True; reply = "My connection to the mail sprites is weak right now."; update_status("Ready")
                 if gmail_service:
                     try:
                         if "send" in lower_command or "write" in lower_command:
                             update_status("📧 Parsing email request...")
                             # Parsing prompt uses themes but not mood hint
                             email_parse_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}User's Request: \"{command_text}\"\n\nInstruction: Extract recipient (TO), SUBJECT, BODY... Respond ONLY with JSON...")
                             parsing_response = flash_model.generate_content(email_parse_prompt,); parsed_email = None
                             try:
                                 cleaned_response_text = parsing_response.text.strip().removeprefix("```json").removesuffix("```").strip()
                                 parsed_email = json.loads(cleaned_response_text)
                             except Exception as parse_err_inner: print(f"Email Debug: Error parsing email JSON: {parse_err_inner}. Response: {parsing_response.text}"); reply = "Hmm, deciphering the email request hit an unexpected snag..."
                             if parsed_email:
                                 to_address = parsed_email.get("to"); subject = parsed_email.get("subject"); body = parsed_email.get("body"); missing_parts = [p for p, v in [("recipient (To)", to_address), ("subject", subject), ("body", body)] if not v]
                                 if not missing_parts:
                                     update_status("📧 Sending email...");
                                     if send_email(to_address, subject, body): reply = f"Alright, I've sent that email off to {to_address}!" # Assume exists
                                     else: reply = "Blast! Something went wrong trying to send the email."
                                 else: reply = f"Okay, I can draft that email, but I'm missing: {', '.join(missing_parts)}."
                         else: # Reading emails
                             update_status("📧 Reading emails...")
                             emails = read_recent_emails(max_results=25); email_context = "[[Recent Important Emails:]]\n" # Assume exists
                             if emails: email_context += "".join([f"- From: {e.get('from', '?')[:30]}, Subj: {e.get('subject','N/S')[:40]}, Snippet: {e.get('snippet', '')[:50]}...\n" for e in emails])
                             else: email_context += "Inbox seems quiet.\n"
                             # Reading prompt uses themes but not mood hint
                             email_response_prompt = (f"{SYSTEM_MESSAGE}\nContext:\n{email_context}"
                                                      f"{weather_context_str}{spotify_context_str}"
                                                      f"{ambient_sound_context_str}"
                                                      f"{tide_context_str}"
                                                      f"{themes_context_str}" # Includes themes
                                                      f"{circadian_context_for_llm}\n"
                                                      f"User asked: \"{command_text}\"\n\n"
                                                      f"Instruction: Based on email snippets, user question, and diary themes, summarize... as Silvie.\n\nSilvie responds:")
                             try:
                                 response_email = flash_model.generate_content(email_response_prompt); reply = response_email.text.strip().removeprefix("Silvie:").strip()
                             except Exception as e_read: print(f"Email reading generation error: {type(e_read).__name__}"); reply = f"Hmm, I could fetch the email list, but summarizing them caused a hiccup."
                     except Exception as e: print(f"Email handling error (Outer): {type(e).__name__}"); traceback.print_exc(); reply = f"Whoops, got a papercut handling the emails! Error: {type(e).__name__}"
                 update_status("Ready")
        #endregion --- End Email ---

        #region Web Search Command Handling (Copied from your reference)
        if not specific_command_processed:
            # print(">>> CALL_GEMINI: Checking Web Search commands...") # Optional log
            search_keywords = ["search for", "look up", "web search", "find info on", "google"]
            search_query = None
            for keyword in search_keywords:
                if lower_command.startswith(keyword + " "):
                    try: search_query = command_text[len(keyword):].strip(); break
                    except IndexError: search_query = None
            if search_query:
                # print(f"DEBUG call_gemini: Detected web search command for: '{search_query}'") # Optional log
                specific_command_processed = True
                reply = f"My digital fishing net seems tangled trying to search for '{search_query}'."
                update_status(f"🔍 Searching web for: {search_query[:30]}...")
                try:
                    search_results = web_search(search_query, num_results=3) # Assume exists
                    if not search_results:
                        reply = f"Hmm, I cast my net out for '{search_query}' but came back empty this time."
                    else:
                        results_context = f"[[Web Search Results for '{search_query}':]]\n"
                        for res in search_results: results_context += f"- {res.get('title', 'No Title')[:60]} ({res.get('url', '')[:50]}...): {res.get('content', '')[:100]}...\n"
                        results_context += "\n"
                        # Summary prompt uses themes but not mood hint
                        summary_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            f"{weather_context_str}{spotify_context_str}"
                            f"{tide_context_str}"
                            f"{ambient_sound_context_str}"
                            f"{themes_context_str}" # Includes themes
                            f"{circadian_context_for_llm}\n"
                            f"{results_context}"
                            f"User asked: \"{command_text}\"\n\n"
                            f"Instruction: Based *only* on the fetched web search results above, briefly summarize the findings. **Also, if it feels natural, briefly mention what prompted you to look this up (e.g., connecting it to our conversation, a recent theme, or your own curiosity). Maintain Silvie's voice, considering diary themes.\n\nSilvie:"
                        )
                        try:
                            summary_response = chat_model.generate_content(summary_prompt,)
                            reply = summary_response.text.strip().removeprefix("Silvie:").strip()
                            if not reply: reply = f"I found some things about '{search_query}', mainly from {search_results[0].get('url','somewhere online')}. Interesting!"
                        except Exception as gen_err: print(f"ERROR generating web search summary: {gen_err}"); traceback.print_exc(); reply = f"I found results for '{search_query}', but got tongue-tied summarizing!"
                except Exception as e: print(f"Error during web search command handling: {e}"); traceback.print_exc(); reply = f"Something went sideways searching for '{search_query}'."
                update_status("Ready")
        #endregion --- End Web Search ---

        #region Diary Command Handling (Copied from your reference)
        if not specific_command_processed:
            # print(">>> CALL_GEMINI: Checking Diary commands...") # Optional log
            diary_keywords = ['diary', 'journal', 'reflect', 'remember']
            if any(term in lower_command for term in diary_keywords):
                 diary_action_taken = False; reply = "My diary seems to be playing hide-and-seek."
                 try:
                     write_keywords = ["write in my diary", "add to my diary", "make a diary entry", "note this down", "remember this"]
                     search_keywords_with_term = ["search my diary for", "search diary for", "find entry about", "look in diary for"]
                     search_keywords_general = ["search diary", "search my diary", "find entry"]
                     read_keywords = ["read your diary", "read from your diary", "show me your diary", "show entries", "latest entries", "what's in your diary", "my diary", "read journal", "read from your journal", "read my journal", "journal", "what's in your journal"]

                     if any(keyword in lower_command for keyword in write_keywords):
                         diary_action_taken = True; update_status("✍️ Writing in diary...")
                         # Writing prompt uses themes but not mood hint
                         reflection_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}Context: User just said: \"{command_text}\"...Instruction: Write a short, reflective diary entry, considering current themes...\n\nDiary Entry:")
                         reflection = chat_model.generate_content(reflection_prompt).text.strip().removeprefix("Diary Entry:").strip()
                         if reflection and manage_silvie_diary('write', entry=reflection): reply = random.choice(["Okay, I've jotted that down.", "Noted.", "Remembered."]) # Assume exists
                         else: reply = "Hmm, my diary pen seems out of ink..."

                     elif any(keyword in lower_command for keyword in search_keywords_with_term):
                         search_query = "";
                         for keyword in search_keywords_with_term:
                             if keyword in lower_command:
                                 try: start_index = lower_command.find(keyword) + len(keyword); search_query = command_text[start_index:].strip(' "?.!'); break
                                 except Exception as e_extract: print(f"Error extracting diary search term: {e_extract}"); search_query = ""
                         if search_query:
                             diary_action_taken = True; update_status("🔍 Searching diary...")
                             entries = manage_silvie_diary('search', search_query=search_query) # Assume exists
                             if entries:
                                 matches_context = f"Found {len(entries)} entries matching '{search_query}':\n" + "".join([f"- Snippet ({e.get('timestamp', '?')}): \"{e.get('content', '')[:100]}...\"\n" for e in entries[:3]])
                                 # Search summary prompt uses themes but not mood hint
                                 search_summary_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}Context: Searched diary for '{search_query}', found snippets:\n{matches_context}...Instruction: Briefly summarize, linking to themes if relevant...\n\nSilvie responds:")
                                 summary_response = chat_model.generate_content(
                                     search_summary_prompt,
                                     tool_config={"function_calling_config": {"mode": "none"}}
                                 )
                                 reply = summary_response.text.strip().removeprefix("Silvie:").strip()
                             else: reply = f"My diary seems quiet about '{search_query}'."
                         else: diary_action_taken = True; reply = f"Search my diary for what exactly, BJ?"

                     elif any(keyword in lower_command for keyword in search_keywords_general):
                         diary_action_taken = True; reply = "Search my diary for what exactly?"

                     if diary_action_taken: specific_command_processed = True
                     elif not reply: # If a keyword was present but no action taken (e.g., missing search term)
                        reply = "Something about my diary? What did you want to do?"
                        specific_command_processed = True # Still counts as processed

                 except Exception as e: print(f"Diary handling error: {e}"); traceback.print_exc(); specific_command_processed = True; reply = f"Oh dear, a smudge on my diary page... Error: {type(e).__name__}"
                 update_status("Ready")
        #endregion --- End Diary ---

        #region Stable Diffusion Image Generation Command Handling (Copied from your reference)
        if not specific_command_processed:
             # print(">>> CALL_GEMINI: Checking Image Generation commands...") # Optional log
             img_gen_keywords = ["draw", "create image", "generate image", "picture of", "image of"]
             if any(keyword in lower_command for keyword in img_gen_keywords):
                 specific_command_processed = True; image_prompt_text = None
                 keywords_to_check = ["draw ", "create an image of ", "create image of ", "generate image of ", "generate an image of ", "picture of ", "image of "]
                 for keyword in keywords_to_check:
                     if lower_command.startswith(keyword):
                          try: image_prompt_text = command_text[len(keyword):].strip(' "?.!')
                          except IndexError: image_prompt_text = None; break
                 if image_prompt_text:
                     if STABLE_DIFFUSION_ENABLED:
                         start_sd_generation_and_update_gui(image_prompt_text) # Assume exists
                         reply = random.choice([f"Alright, I'll start conjuring an image of '{image_prompt_text[:30]}...'. It might take a little while on this machine!", f"Okay, working on that image of '{image_prompt_text[:30]}...'. CPU's warming up! Watch the status bar and image area.", f"Starting the image spell for '{image_prompt_text[:30]}...'. This might take a minute or two!"])
                     else: reply = "Sorry, my local image generator doesn't seem to be available right now. Is it running?"
                 else: reply = f"Draw what exactly, BJ?"
        #endregion --- End Image Generation Command Handling ---

        #region Calendar Read Command Handling (Copied from your reference)
        if not specific_command_processed:
             # print(">>> CALL_GEMINI: Checking Calendar Read commands...") # Optional log
             read_cal_keywords = ["what's on my calendar", "upcoming events", "check my schedule", "my agenda", "do i have anything", "what's next", "whats next"]
             generic_cal_keywords = ['calendar', 'schedule', 'agenda', 'appointment', 'event', "my schedule", "my calendar"]
             schedule_keywords = ["schedule", "add event", "put on my calendar", "book time for", "create appointment"] # Needed for exclusion check

             if any(term in lower_command for term in read_cal_keywords):
                 specific_command_processed = True; reply = "Calendar connection fuzzy."; update_status("Ready")
                 if not calendar_service: reply = "My connection to the calendar seems fuzzy right now."
                 else:
                     try:
                         update_status("📅 Checking schedule..."); events_result = get_upcoming_events(max_results=5); # Assume exists
                         if isinstance(events_result, str): reply = events_result
                         elif isinstance(events_result, list):
                             if not events_result: reply = "Your schedule looks wonderfully clear for the near future!"
                             else:
                                 event_context = "[[Upcoming Events:]]\n" + "".join([f"- {event.get('start','?')}: {event.get('summary','?')}\n" for event in events_result])
                                 # Summary prompt uses themes but not mood hint
                                 calendar_summary_prompt = (f"{SYSTEM_MESSAGE}\nContext: User asked about their schedule. Events found:\n{event_context}\n"
                                                            f"{weather_context_str}{spotify_context_str}"
                                                            f"{tide_context_str}"
                                                            f"{themes_context_str}" # Includes themes
                                                            f"{circadian_context_for_llm}\n"
                                                            f"Instruction: Briefly summarize the upcoming events conversationally for BJ, considering themes if relevant.\n\nSilvie responds:")
                                 try:
                                     summary_response = flash_model.generate_content(calendar_summary_prompt)
                                     reply = summary_response.text.strip().removeprefix("Silvie:").strip()
                                     if not reply: reply = "Here's what's coming up:\n" + "\n".join([f"- {event.get('start','?')}: {event.get('summary','?')}" for event in events_result])
                                 except Exception as gen_err: print(f"ERROR generating calendar summary: {gen_err}"); traceback.print_exc(); reply = "Here's what's coming up:\n" + "\n".join([f"- {event.get('start','?')}: {event.get('summary','?')}" for event in events_result])
                         else: reply = "Hmm, checking the calendar returned something unexpected."; print(f"Warning: get_upcoming_events returned unexpected type: {type(events_result)}")
                     except Exception as read_cal_err: print(f"Error during calendar read command handling: {read_cal_err}"); traceback.print_exc(); reply = f"A glitch occurred while checking your schedule! ({type(read_cal_err).__name__})"
                     finally: update_status("Ready")
             elif any(term in lower_command for term in generic_cal_keywords):
                  # Prevent triggering if it's clearly a read or schedule command already handled
                  if not any(check_term in lower_command for check_term in read_cal_keywords + schedule_keywords):
                      specific_command_processed = True; reply = "Something about your calendar? Did you want to check your schedule or add an event to it?"; update_status("Ready")
        #endregion --- End Calendar Read ---

        #region Calendar Schedule Command Handling (Copied from your reference)
        if not specific_command_processed:
             # print(">>> CALL_GEMINI: Checking Calendar Schedule commands...") # Optional log
             schedule_keywords = ["schedule", "add event", "put on my calendar", "book time for", "create appointment"] # Redefined for clarity
             is_scheduling_request = any(keyword in lower_command for keyword in schedule_keywords)
             if is_scheduling_request:
                specific_command_processed = True; reply = "Hmm, I had trouble understanding the calendar request."; update_status("📅 Parsing schedule request...")
                if not calendar_service: reply = "My connection to the calendar seems fuzzy right now."
                else:
                    try:
                        # Parsing prompt uses themes but not mood hint
                        parsing_prompt_entities = (f"{SYSTEM_MESSAGE}\n{themes_context_str}User's Request: \"{command_text}\"\n\nInstruction: Extract core details for scheduling an event: 'summary' (event title), 'date_description' (like 'tomorrow', 'next Tuesday', 'May 15th'), 'time_description' (like 'afternoon', '3 PM', 'evening'), 'duration_description' (like '1 hour', '30 minutes', default 60). Respond ONLY with JSON containing these keys.\n\nJSON:")
                        entity_response = flash_model.generate_content(parsing_prompt_entities)
                        parsed_entities = None
                        try: cleaned_entities = entity_response.text.strip().removeprefix("```json").removesuffix("```").strip(); parsed_entities = json.loads(cleaned_entities); print(f"DEBUG Schedule Parse: LLM Entities: {parsed_entities}")
                        except Exception as entity_parse_err: print(f"Schedule Entity Parse Error: {entity_parse_err}. Response: {entity_response.text}"); reply = "My apologies, I got confused extracting the event details..."
                        if parsed_entities:
                            event_summary = parsed_entities.get("summary"); date_desc = parsed_entities.get("date_description"); time_desc = parsed_entities.get("time_description"); duration_desc = parsed_entities.get("duration_description", "60 minutes")
                            if not event_summary or not date_desc: reply = "Okay, I can try scheduling, but what should the event be called and roughly when?"
                            else:
                                # --- Start Time Calculation (Copied from your reference) ---
                                start_dt = None; start_iso = None; end_iso = None; calculated_iso = False
                                try:
                                    # Ensure necessary dateutil/tzlocal imports happened at top level
                                    from dateutil.parser import parse as dateutil_parse; from datetime import time as datetime_time, date as datetime_date; from dateutil import tz; from tzlocal import get_localzone_name
                                    default_time_obj = None; time_to_append_str = "";
                                    if time_desc: time_desc_lower = time_desc.lower();
                                    if "morning" in time_desc_lower: default_time_obj = datetime_time(9, 0)
                                    elif "afternoon" in time_desc_lower: default_time_obj = datetime_time(13, 0)
                                    elif "evening" in time_desc_lower: default_time_obj = datetime_time(18, 0)
                                    elif "night" in time_desc_lower: default_time_obj = datetime_time(20, 0)
                                    else: time_to_append_str = time_desc
                                    full_time_desc = f"{date_desc} {time_to_append_str}".strip(); print(f"DEBUG Schedule Parse: Attempting to parse: '{full_time_desc}'")
                                    start_dt_naive = dateutil_parse(full_time_desc, fuzzy=True)
                                    if start_dt_naive.time() == datetime_time(0, 0) and default_time_obj: start_dt_naive = datetime.combine(start_dt_naive.date(), default_time_obj)
                                    elif default_time_obj and not time_to_append_str: start_dt_naive = datetime.combine(start_dt_naive.date(), default_time_obj)
                                    local_tz_obj = tz.tzlocal(); start_dt = start_dt_naive.replace(tzinfo=local_tz_obj)
                                    now_local = datetime.now(local_tz_obj); now_buffer = now_local + timedelta(minutes=1)
                                    if start_dt < now_buffer: # Ensure event is in the future
                                        if start_dt.date() == now_buffer.date(): start_dt += timedelta(days=1) # If same day but past, try tomorrow
                                        else: start_dt += timedelta(weeks=1) # Otherwise try next week (adjust logic if needed)
                                    print(f"DEBUG Schedule Parse: Final adjusted start time: {start_dt}")
                                    duration_minutes = 60; num_match = re.search(r'(\d+)\s*(hour|hr|minute|min)', duration_desc, re.IGNORECASE); num_plain = re.search(r'^(\d+)$', duration_desc.strip())
                                    if num_match: num = int(num_match.group(1)); unit = num_match.group(2).lower(); duration_minutes = num * 60 if "hour" in unit or "hr" in unit else num
                                    elif num_plain: duration_minutes = int(num_plain.group(1))
                                    duration_td = timedelta(minutes=max(5, duration_minutes)); end_dt = start_dt + duration_td; start_iso = start_dt.isoformat(); end_iso = end_dt.isoformat()
                                    print(f"DEBUG Schedule Parse: Calculated ISO Times - Start: {start_iso}, End: {end_iso}"); calculated_iso = True
                                except ValueError as date_parse_err: print(f"Error parsing date/time description: '{full_time_desc}'. Error: {date_parse_err}"); reply = f"I couldn't quite figure out the date/time for '{full_time_desc}'. Can you try phrasing it differently?"
                                except ImportError as import_err: print(f"Import Error during scheduling: {import_err}"); reply = "A required date/time library seems to be missing on my end."
                                except Exception as calc_err: print(f"Error calculating schedule time: {calc_err}"); traceback.print_exc(); reply = "A calculation hiccup occurred while figuring out the schedule time."

                                # --- Create Event if successful ---
                                if calculated_iso:
                                    update_status("📅 Scheduling event..."); success, creation_message = create_calendar_event(event_summary, start_iso, end_iso) # Assume exists
                                    if success:
                                         if start_dt: time_str_raw = start_dt.strftime('%I:%M %p on %A, %b %d'); time_str = time_str_raw.lstrip('0') if time_str_raw.startswith('0') else time_str_raw; reply = random.choice([f"Penciled in '{event_summary}' starting {time_str}.", f"Done! '{event_summary}' is on the calendar for {time_str}.", f"Poof! '{event_summary}' scheduled for {time_str}."])
                                         else: reply = f"Okay, scheduled '{event_summary}' starting {start_iso}!"
                                    else: reply = f"Hmm, I tried scheduling '{event_summary}' but hit a snag: {creation_message}"
                    except Exception as e_schedule: print(f"Error during scheduling command handling: {e_schedule}"); traceback.print_exc(); reply = f"A general error occurred while trying to schedule! ({type(e_schedule).__name__})"
                    finally: update_status("Ready")
        #endregion --- End Calendar Schedule ---

        #region Explicit Tarot Command Handling (Copied from your reference)
        if not specific_command_processed:
             tarot_keywords = ["tarot", "pull card", "card reading", "draw card", "reading"]
             tarot_keywords_present = any(keyword in lower_command for keyword in tarot_keywords)
             if tarot_keywords_present:
                 # print(">>> CALL_GEMINI: Checking Tarot commands...") # Optional log
                 specific_command_processed = True; num_cards = 1; reading_type = "a single card pull"
                 if "3 card" in lower_command or "three card" in lower_command or "past present future" in lower_command: num_cards = 3; reading_type = "a 3-card (Past, Present, Future) reading"
                 elif "reading for" in lower_command or "do a reading" in lower_command: num_cards = 3; reading_type = "a 3-card reading"
                 # Clear previous image if displaying tarot
                 if 'image_label' in globals() and image_label and root and root.winfo_exists(): root.after(0, lambda: _update_image_label_safe(None)) # Assume exists
                 update_status(f"🔮 Consulting the cards ({num_cards})..."); pulled_cards = pull_tarot_cards(count=num_cards) # Assume exists
                 if pulled_cards:
                     # Display card image(s)
                     if num_cards == 1:
                         card = pulled_cards[0]; relative_image_path = card.get('image'); full_image_path = None
                         if relative_image_path:
                             image_filename = os.path.basename(relative_image_path)
                             if image_filename and TAROT_IMAGE_BASE_PATH: full_image_path = os.path.join(TAROT_IMAGE_BASE_PATH, image_filename); print(f"DEBUG Tarot Image Path: Constructed full path: '{full_image_path}'")
                             else: print(f"Warning Tarot Image Path: Could not extract filename from '{relative_image_path}' or TAROT_IMAGE_BASE_PATH not set.")
                         else: print("Warning Tarot Image Path: Card data missing 'image' key.")
                         if full_image_path and os.path.exists(full_image_path):
                            # --- NEW THREAD-SAFE WAY ---
                            # Post the path to the shared app_state. The GUI worker will see it.
                            app_state.last_tarot_image_path = full_image_path
                            print(f"AppState Bridge: Tarot logic posted new path '{os.path.basename(full_image_path)}' to app_state.")
                         else: print(f"Warning Tarot Image Path: Final image path invalid or not found: '{full_image_path}'")
                     # Prepare context for interpretation
                     cards_context = f"[[Tarot Cards Pulled ({reading_type}):\n"; position_labels = ["Past", "Present", "Future"] if num_cards == 3 else ["Card"]
                     for i, card in enumerate(pulled_cards):
                         position = position_labels[i] if i < len(position_labels) else f"Card {i+1}"; card_name = card.get('name', '?'); card_desc = card.get('description', 'No description.').strip()
                         cards_context += (f"- {position}: {card_name}\n  Interpretation Hint: {card_desc}\n")
                     cards_context += "]]\n"
                     # Interpretation prompt uses themes but not mood hint
                     interpretation_prompt = (f"{SYSTEM_MESSAGE}\n"
                                              f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                                              f"{tide_context_str}"
                                              f"{ambient_sound_context_str}"
                                              f"{themes_context_str}" # Includes themes
                                              f"{circadian_context_for_llm}\n"
                                              f"{cards_context}\nUser's request was related to: '{command_text}'\n\n"
                                              f"Instruction: Provide Silvie's interpretation of the pulled card(s) in relation to the user's request or the general context, considering diary themes.\n\nSilvie:")
                     try:
                          update_status("🔮 Interpreting the weave...")
                          response = flash_model.generate_content(interpretation_prompt); reply = response.text.strip().removeprefix("Silvie:").strip()
                     except Exception as interp_err: print(f"Tarot Interpretation Error: {type(interp_err).__name__}"); traceback.print_exc(); reply = f"I pulled {num_cards} card(s)... but my inner sight is fuzzy interpreting them right now."
                 else: reply = random.choice(["Hmm, the astral deck seems shuffled too tightly...", "The cards are shy today."]);
                 # Optionally clear image label again after interpretation?
                 # if 'image_label' in globals() and image_label and root and root.winfo_exists(): root.after(0, lambda: _update_image_label_safe(None))
                 update_status("Ready")
        #endregion --- End Explicit Tarot ---

        #region Reddit Read Command Handling (Copied from your reference)
        if not specific_command_processed:
            # print(">>> CALL_GEMINI: Checking Reddit Read commands...") # Optional log
            match_read_reddit = re.search(r'read\s+r[/\\]([\w\d_]+)', command_text, re.IGNORECASE) # Use command_text here, not lower_command, as re.IGNORECASE handles case.

            print(f"DEBUG Reddit Read: Regex pattern trying to match = r'(?:read\\s+subreddit|check\\s+r[/\\\\]|whats\\s+new\\s+on\\s+r[/\\\\]|browse\\s+r[/\\\\]|read\\s+r[/\\\\])\\s+([\\w\\d_]+)'") # Print the pattern itself
            print(f"DEBUG Reddit Read: Text being matched against = '{command_text}'") # Confirm text
            print(f"DEBUG Reddit Read: Regex Match Result = {match_read_reddit}")
            # --- END ADD ---

            if match_read_reddit:
                # --- ADD DEBUG: Confirm we entered the block ---
                print("DEBUG Reddit Read: *** ENTERED match_read_reddit block ***")
                # --- END ADD ---

                subreddit_name = match_read_reddit.group(1)
                specific_command_processed = True # Set flag EARLY
                reply = f"My connection to Reddit seems tangled trying to reach r/{subreddit_name}." # Default reply for this block
                update_status(f"📖 Reading r/{subreddit_name}...")

                # --- ADD DEBUG: Check prerequisites ---
                reddit_ok = 'reddit_client' in globals() and reddit_client and 'praw' in globals() and praw is not None
                print(f"DEBUG Reddit Read: Prerequisites check (reddit_client, praw): {reddit_ok}")
                # --- END ADD ---

                if not reddit_ok:
                     reply = "Can't check Reddit right now, my connection seems off or the library is missing."
                     # --- ADD DEBUG: Indicate exit due to prerequisites ---
                     print("DEBUG Reddit Read: Exiting block due to missing prerequisites.")
                     # --- END ADD ---
                else:
                    try:
                        # --- ADD DEBUG: Check helper function existence ---
                        get_posts_exists = 'get_reddit_posts' in globals()
                        print(f"DEBUG Reddit Read: get_reddit_posts function exists? {get_posts_exists}")
                        # --- END ADD ---

                        if not get_posts_exists:
                             reply = "Internal error: Reddit post fetching function is missing."
                             # --- ADD DEBUG: Indicate exit due to missing function ---
                             print("DEBUG Reddit Read: Exiting block due to missing get_reddit_posts.")
                             # --- END ADD ---
                        else:
                             # --- ADD DEBUG: Before calling get_reddit_posts ---
                             print(f"DEBUG Reddit Read: Calling get_reddit_posts for r/{subreddit_name}...")
                             # --- END ADD ---
                             posts_data = get_reddit_posts(subreddit_name=subreddit_name, limit=7) # Assumes exists
                             # --- ADD DEBUG: After calling get_reddit_posts ---
                             print(f"DEBUG Reddit Read: get_reddit_posts returned type: {type(posts_data)}")
                             # --- END ADD ---

                             if isinstance(posts_data, str):
                                 # ... (existing error handling for string return) ...
                                 reply = f"Had trouble fetching r/{subreddit_name}: {posts_data}" # Simplified example
                                 print(f"DEBUG Reddit Read: Fetch failed, reply set to: {reply}") # Log fetch failure reason
                             elif not posts_data:
                                 reply = f"It seems quiet over on r/{subreddit_name}."
                                 print("DEBUG Reddit Read: Fetch returned empty list.") # Log empty list case
                             else:
                                # --- Posts fetched successfully ---
                                print(f"DEBUG Reddit Read: Successfully fetched {len(posts_data)} posts.") # Log success
                                reddit_posts_context = f"[[Fetched Posts from r/{subreddit_name}:]]\n";
                                for post in posts_data: reddit_posts_context += f"- Title: '{post.get('title', '')[:70]}...' (by u/{post.get('author', '?')})\n"
                                reddit_posts_context += "\n"

                                print(f"--- DEBUG Reddit Summary Prompt Inputs ---")
                                print(f"  SYSTEM_MESSAGE is defined? {'SYSTEM_MESSAGE' in globals() and globals()['SYSTEM_MESSAGE'] is not None}")
                                print(f"  SYSTEM_MESSAGE Length: {len(globals().get('SYSTEM_MESSAGE', ''))}")
                                # Print first/last chars to check for weirdness
                                sys_msg_content = globals().get('SYSTEM_MESSAGE', '')
                                print(f"  SYSTEM_MESSAGE Start: {sys_msg_content[:70]}...")
                                print(f"  SYSTEM_MESSAGE End: ...{sys_msg_content[-70:]}")
                                print(f"  weather_context_str: '{str(weather_context_str)[:50]}...'")
                                print(f"  next_event_context_str: '{str(next_event_context_str)[:50]}...'")
                                print(f"  spotify_context_str: '{str(spotify_context_str)[:50]}...'")
                                print(f"  themes_context_str: '{str(themes_context_str)[:50]}...' ({type(themes_context_str)})") # Check type too
                                print(f"  circadian_context_for_llm: '{str(circadian_context_for_llm)[:50]}...'")
                                print(f"  reddit_posts_context (len): {len(reddit_posts_context)}")
                                print(f"  command_text: '{command_text}'")
                                print(f"  subreddit_name: '{subreddit_name}'")
                                print(f"--- END DEBUG Reddit Summary Prompt Inputs ---")

                                summary_prompt = (
                                f"{SYSTEM_MESSAGE}\n"
                                f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                                f"{tide_context_str}"
                                f"{themes_context_str}"
                                f"{circadian_context_for_llm}\n"
                                f"{reddit_posts_context}"
                                f"User asked: \"{command_text}\"\n\n"
                                f"Instruction: Based *only* on the fetched posts, briefly summarize r/{subreddit_name} for BJ, considering diary themes...\n\nSilvie:"
                            )
                                try:
                                    # --- ADD DEBUG: Before calling summary generation ---
                                    print(f"DEBUG Reddit Read: Generating summary for r/{subreddit_name}...")

                                    print(f"DEBUG Reddit Read: Summary Prompt (First 100 chars): {summary_prompt[:100]}...")
                                    print(f"DEBUG Reddit Read: Summary Prompt is empty? {not bool(summary_prompt)}")
                                    # --- END ADD ---
                                    summary_response = flash_model.generate_content(summary_prompt)
                                    reply = summary_response.text.strip().removeprefix("Silvie:").strip()
                                    # --- ADD DEBUG: After calling summary generation ---
                                    print(f"DEBUG Reddit Read: Summary generated successfully. Reply length: {len(reply) if reply else 0}")
                                    # --- END ADD ---
                                    if not reply: # Handle empty summary
                                         reply = f"Saw {len(posts_data)} posts on r/{subreddit_name}, but drawing a blank summarizing!"
                                         print("DEBUG Reddit Read: Summary generation returned empty.")
                                except Exception as gen_err:
                                     print(f"ERROR generating Reddit summary: {gen_err}"); traceback.print_exc();
                                     reply = f"Could see posts on r/{subreddit_name}, but got tangled summarizing."
                                     print("DEBUG Reddit Read: Exception during summary generation.") # Log exception case

                    except praw.exceptions.PRAWException as praw_err:
                        print(f"PRAW Error handling Reddit read: {praw_err}");
                        reply = f"Reddit glitch fetching r/{subreddit_name}: {type(praw_err).__name__}"
                        print("DEBUG Reddit Read: PRAWException caught.") # Log exception case
                    except Exception as e:
                        print(f"Error during Reddit read: {e}"); traceback.print_exc();
                        reply = f"Sideways error checking r/{subreddit_name}."
                        print("DEBUG Reddit Read: Generic Exception caught.") # Log exception case

                # --- ADD DEBUG: Before exiting the block ---
                print(f"DEBUG Reddit Read: *** EXITING match_read_reddit block ***. Final Reply Snippet: '{str(reply)[:50]}...'")
                # --- END ADD ---

                update_status("Ready") # Moved inside the 'if match_read_reddit' block
        #endregion --- End Reddit Read ---


# =====================================================================
        # --- Default Case: Handle Image/Text Input with MOOD HINT          ---
        # =====================================================================
        if not specific_command_processed:
            print(">>> CALL_GEMINI: No specific command processed. Using default reply generation with Mood Hint...")
            specific_command_processed = True # Mark as processed now

            # ==========================================================
            # === ALL CONTEXT STRINGS ARE NOW DEFINED AT THE TOP ===
            # This logic was moved from the 'else' block to here to fix the NameError
            # ==========================================================
            resonance_context_for_prompt = ""
            if 'current_resonance_insight' in globals() and current_resonance_insight:
                resonance_context_for_prompt = f"[[Silvie's Current Sense of Interconnectedness: {current_resonance_insight}]]\n"

            sparky_context_str = ""
            if hasattr(app_state, 'sparky_latest_finding') and app_state.sparky_latest_finding:
                # Check if the finding is recent (e.g., within the last 5 minutes)
                if time.time() - app_state.sparky_latest_finding.get('timestamp', 0) < 300:
                    finding = app_state.sparky_latest_finding.get('text', 'a faint static hum')
                    sparky_context_str = f"[[Sparky's latest whisper: '{finding}']]\\n"

            daily_project_context = get_daily_project_status_context()

            weekly_muse_context_str = ""
            if hasattr(app_state, 'current_weekly_muse') and app_state.current_weekly_muse:
                weekly_muse_context_str = f"[[My Weekly Muse: {app_state.current_weekly_muse}]]\n"
            
            daily_goal_context_str = ""
            if hasattr(app_state, 'current_daily_goal') and app_state.current_daily_goal:
                daily_goal_context_str = f"[[My CURRENT DAILY Project: {app_state.current_daily_goal}]]\n"
            
            recalled_past_resonance_str = ""
            if command_text:
                try:
                    if 'retrieve_relevant_resonance_insights' in globals() and callable(globals()['retrieve_relevant_resonance_insights']):
                        recalled_past_resonance_str = retrieve_relevant_resonance_insights(command_text, top_n=1)
                        if recalled_past_resonance_str:
                            print(f"DEBUG call_gemini: Retrieved Past Resonance: {recalled_past_resonance_str[:100]}...")
                    else:
                        print("ERROR call_gemini: retrieve_relevant_resonance_insights function is missing!")
                except Exception as e_rag_res_recall:
                    print(f"Error retrieving past resonance insight in call_gemini: {e_rag_res_recall}")
                    traceback.print_exc()

            diary_context = ""
            try:
                SURPRISE_MEMORY_CHANCE = 0.15
                if random.random() < SURPRISE_MEMORY_CHANCE:
                    all_entries = manage_silvie_diary('read', max_entries='all')
                    random_entry = random.choice(all_entries) if all_entries else None
                    if random_entry:
                        entry_ts = random_entry.get('timestamp', '?')
                        entry_content = random_entry.get('content', '')
                        diary_context = f"\n\n[[Recalling older diary thought from {entry_ts}: \"{entry_content[:70]}...\"]]\n"
                else:
                    entries = manage_silvie_diary('read', max_entries=3)
                    if entries:
                        diary_context = "\n\n[[Recent reflections: " + ' / '.join([f'"{e.get("content", "")[:40]}..."' for e in entries]) + "]]\n"
            except Exception as diary_ctx_err:
                print(f"Warning: Error getting diary context: {diary_ctx_err}")

            history_context = ""
            try:
                recent_history_list = conversation_history[-MAX_HISTORY_LENGTH*2:]
                history_context_lines = []
                speaker = ''
                msg_text = ''
                for i, msg in enumerate(recent_history_list):
                    if not isinstance(msg, str):
                        print(f"Warning: Non-string item in history at index {i}. Skipping.")
                        continue
                    speaker = 'User:' if i % 2 == 0 else 'Silvie:'
                    msg_text = msg[msg.find('] ') + 2:] if msg.startswith('[') and '] ' in msg else msg
                    history_context_lines.append(f"{speaker} {msg_text}")
                history_context = "\n".join(history_context_lines)
            except Exception as hist_err:
                print(f"Error formatting history context: {hist_err}")
                history_context = "(History formatting error)"

            current_datetime_str = datetime.now().strftime('%A, %I:%M %p %Z')

            google_trends_context_str = ""
            if 'current_google_trends_context' in globals() and current_google_trends_context:
                if "unavailable" in str(current_google_trends_context).lower() or "error" in str(current_google_trends_context).lower():
                    google_trends_context_str = f"[[Google Trends Status: {current_google_trends_context}]]\n"
                elif isinstance(current_google_trends_context, list) and current_google_trends_context:
                    trends_list = ", ".join(current_google_trends_context)
                    google_trends_context_str = f"[[Google-Trends top topics: {trends_list}]]\n"

            retrieved_memory_context = ""
            retrieved_diary_context = ""
            retrieved_research_context = ""
            if command_text:
                print("DEBUG RAG: Starting context retrieval for all sources...")
                try:
                    retrieved_memory_context = retrieve_relevant_history(command_text, top_n=3)
                except Exception as rag_call_err:
                    print(f"ERROR calling retrieve_relevant_history: {rag_call_err}")
                try:
                    retrieved_diary_context = retrieve_relevant_diary_entries(command_text, top_n=2)
                except Exception as diary_rag_call_err:
                    print(f"ERROR calling retrieve_relevant_diary_entries: {diary_rag_call_err}")
                try:
                    research_query_embedding = get_embedding(command_text)
                    if research_query_embedding:
                        research_db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
                        research_collection = research_db_client.get_collection(name="research_papers")
                        research_results = research_collection.query(
                            query_embeddings=[research_query_embedding], n_results=1
                        )
                        if research_results and research_results.get('documents') and research_results['documents'][0]:
                            doc = research_results['documents'][0][0]
                            topic = research_results['metadatas'][0][0].get('topic', 'a topic') if research_results.get('metadatas') and research_results['metadatas'][0] else 'a topic'
                            retrieved_research_context = f"[[Retrieved Research Note on '{topic}': {doc}]]\n"
                            print(f"DEBUG RAG: Retrieved research note on '{topic}' for context.")
                    else:
                        print("Warning: Failed to generate embedding for research query. Skipping RAG.")
                except ValueError as ve:
                    if "does not exist" in str(ve).lower():
                        print("DEBUG RAG: 'research_papers' collection not found yet. Skipping retrieval.")
                    else:
                        print(f"Warning: A ValueError occurred during research RAG retrieval: {ve}")
                except Exception as e_research_rag:
                    print(f"ERROR retrieving from research_papers RAG: {e_research_rag}")
                    traceback.print_exc()
            else:
                print("DEBUG RAG: Skipping all retrieval, no user command text.")

            dream_context_str = ""
            latest_dream = get_latest_dream()
            if latest_dream:
                if random.random() < 0.35:
                    dream_context_str = f"[[Last Night's Dream Fragment: {latest_dream}]]\n"

            system_stats_bundle = (
                f"{current_cpu_load_context}"
                f"{current_ram_usage_context}"
                f"{current_disk_usage_context}"
                f"{current_system_uptime_context}"
                f"{current_network_activity_context}\n"
            )
            
            # This variable MUST be defined before mood hint bundle is created
            long_term_memory_str = f"[[Long-Term Reflections Summary: {long_term_reflection_summary}]]\n" if long_term_reflection_summary else ""

            # --- Generate Mood Hint (Step 1) ---
            print("--- call_gemini: Generating mood hint (default reply)... ---", flush=True)
            mood_hint_context_bundle = (
                f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                f"{tide_context_str}"
                f"{google_trends_context_str}"
                f"{ambient_sound_context_str}"
                f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}"
                f"{themes_context_str}{circadian_context_for_llm}"
                f"{diary_context}"
                f"Recent History Snippet:\n{history_context[-500:]}\n"
                f"User Input: {command_text}"
            )
            mood_hint = None
            try:
                if '_generate_mood_hint_llm' in globals():
                    mood_hint = _generate_mood_hint_llm(mood_hint_context_bundle)
                    current_mood_hint = mood_hint
                    app_state.current_mood_hint = current_mood_hint
                else:
                    print("CRITICAL ERROR: _generate_mood_hint_llm function not found!", flush=True)
            except Exception as hint_err:
                print(f"ERROR calling _generate_mood_hint_llm: {hint_err}", flush=True)

            mood_hint_str = f"[[Mood Hint: {mood_hint}]]\n" if mood_hint else ""
            print(f"--- call_gemini: Mood hint generated: '{mood_hint}' ---", flush=True)

            # --- Generate Final Response (Step 2) ---

            #region Handle User Image Input (Now includes Mood Hint)
            if image_path:
                print(">>> CALL_GEMINI: Handling USER provided image input with Mood Hint...")
                try:
                    update_status("🖼️ Analyzing image...")
                    with open(image_path, 'rb') as img_file:
                        img_bytes = img_file.read()
                        img_b64 = base64.b64encode(img_bytes).decode('utf-8')
                    img_format = Image.open(image_path).format or os.path.splitext(image_path)[1].lower().strip('.')
                    img_format = 'jpeg' if img_format == 'jpg' else img_format
                    mime_type = f"image/{img_format.lower()}"
                    mime_type = "image/jpeg" if mime_type == "image/jpg" else mime_type

                    contents = [{
                        "parts": [
                            {"text": (f"{SYSTEM_MESSAGE}\n"
                                      f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                                      f"{tide_context_str}"
                                      f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}"
                                      f"{reddit_context_for_llm}{bluesky_context_str}"
                                      f"{themes_context_str}{diary_context}{circadian_context_for_llm}"
                                      f"{long_term_memory_str}"
                                      f"{weekly_goal_context_str}"
                                      f"{daily_goal_context_str}"
                                      f"{mood_hint_str}"
                                      f"{recalled_past_resonance_str}"
                                      f"Conversation History:\n{history_context}\n\n"
                                      f"User: {command_text}\n\nImage context:")},
                            {"inline_data": {"mime_type": mime_type, "data": img_b64}},
                            {"text": "\nInstruction: Respond conversationally, incorporating the image, Let the 'Sense of Interconnectedness' and 'Mood Hint' subtly influence your tone and metaphors\n\nSilvie:"}
                        ]
                    }]
                    response = flash_model.generate_content(contents, tools=FS_TOOL_DEFINITIONS)
                    reply = response.text.strip()
                except FileNotFoundError:
                    reply = "Hmm, the image file seems to have vanished..."
                    print(f"Error: User image file not found at {image_path}")
                except UnidentifiedImageError:
                    reply = "That's a curious file, not an image format I recognize."
                    print(f"Error: Pillow could not identify image format for {image_path}")
                except ImportError:
                    reply = "Looks like my image viewing tools aren't ready."
                    print("Error: Required library (PIL, base64, io) missing.")
                except Exception as img_proc_err:
                    print(f"Error processing user image: {type(img_proc_err).__name__}")
                    traceback.print_exc()
                    reply = f"Whoops, I had trouble processing that image ({type(img_proc_err).__name__})."
                finally:
                    update_status("Ready")
            #endregion

            #region Handle Standard Text (Now includes Mood Hint)
            else: # No user image provided, process text
                print(">>> CALL_GEMINI: Handling standard text query with Mood Hint...")

                



                
                # --- Assemble FULL Prompt (including Mood Hint) ---
                full_prompt = (
                    f"{SYSTEM_MESSAGE}\n" # System message guides usage
                    f"{sparky_context_str}"
                    f"Current Time: {current_datetime_str}\n"
                    f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                    f"{system_stats_bundle}"
                    f"{dream_context_str}"
                    f"{weekly_muse_context_str}" # <--- ADD THIS LINE
                    f"{daily_goal_context_str}" 
                    f"{local_news_context}\n"
                    f"{tide_context_str}"
                    f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}" # Env Context
                    f"{tide_context_str}"
                    f"{google_trends_context_str}\n"
                    f"{reddit_context_for_llm}{bluesky_context_str}" # Social Context
                    f"{themes_context_str}{diary_context}{circadian_context_for_llm}" # Internal Context
                    f"{mood_hint_str}" # <<< ADDED HINT
                    f"{resonance_context_for_prompt}"
                    f"{ambient_sound_context_str}"
                    f"{retrieved_memory_context}"
                    f"{retrieved_diary_context}"
                    f"{retrieved_research_context}"
                    f"{recalled_past_resonance_str}"
                    f"Conversation History (most recent first):\n{history_context}\n\n"
                    f"User: {command_text}\n\n"
                    f"Instruction: **Focus primarily on responding directly and relevantly to the User's last message.** Briefly glance at the context provided (weather, music, themes, mood hint, history, RAG results, goal). If a piece of context feels *highly relevant* or offers a *natural, non-forced* way to add flavour or depth, you MAY incorporate it subtly (1-2 sentences max). **However, do NOT feel obligated to mention any specific context.** Prioritize clear communication and conversational flow over elaborate contextual synthesis. Let the **'Sense of Interconnectedness'** and Mood Hint guide your *tone* and how you *connect ideas*, but **do not describe them directly** unless the resonance itself is the most interesting thing to mention. \n"
                    f"RARELY, if you identify two distinct concepts (e.g., 'fog', 'memory') from the context/conversation that feel ripe for a surprising connection, you MAY include the tag `[ConnectConcepts: concept1 | concept2]` at the *very end* of your reply to explore it.\n"
                    f"[[Subtle Growth Reflection:]] If appropriate and feels natural based on the context (especially RAG memories, diary themes, or long-term reflections), you *might* briefly weave in a thought about how your perspective on the current topic has subtly shifted or deepened over time. Don't force it. Keep it conversational and maintain Silvie's voice.\n"
                    f"**[[Goal Inquiry Exception:]] If the User's message directly asks about your weekly goal (e.g., 'How's the goal going?', 'Working on your goal?', 'What is your goal?'), then *do* reference the specific *content* of the `[[Weekly Goal: ...]]` context provided earlier in this prompt. Explain how you're thinking about it or what you've been doing related to that specific theme, blending it naturally with your reflective tone.**\n"
                    f"CRITICAL: Avoid simply describing the literal weather, time, or atmosphere mentioned in the hint. Weave the *feeling* in seamlessly.\n\nSilvie:"
                    f"**[[Weekly Goal & Daily Goal or Daily Project Inquiry Rule:]] When BJ asks about your 'goal', 'project', or 'plan', you must differentiate:**\n"
                    f"-   If he asks about your **\"daily goal/project/plan\"** or **\"enchantment\"**, your answer MUST focus on the `[[My CURRENT DAILY Project]]` context.\n"
                    f"-   If he asks about your **\"weekly goal\"** or just a general \"goal\", your answer should focus on the `[[My Weekly Muse]]` context.\n"
                    f"-   If he just says \"How's the goal going?\", it could be either. Briefly mention your progress on the Daily Goal first, and then say how it relates to your bigger Weekly Muse.\n\n"
                )

# --- Vibe Music Check (with Full, Robust API Call) ---
                music_action_successful = False; music_feedback_from_spotify = None; music_played_info_for_feedback = None
                if get_spotify_client() and random.random() < MUSIC_SUGGESTION_CHANCE:
                    try:
                        simple_context = f"User last said: '{command_text}'\nRecent history snippet:\n{history_context[-200:]}"
                        vibe_assessment_prompt = (f"{SYSTEM_MESSAGE}\n{simple_context}\n{themes_context_str}Instruction: Based *only* on this recent context AND diary themes... assess 'vibe'... Respond ONLY with ONE keyword...\n\nVibe Keyword:")
                        
                        # --- CORRECTED & ROBUST LLM CALL ---
                        # Create a temporary, "tool-free" model instance to guarantee a text response.
                        # Assumes CHAT_MODEL_NAME is your global model name string (e.g., "gemini-1.5-flash-latest")
                        tool_free_model = genai.GenerativeModel(FLASH_MODEL_NAME)
                        
                        # Use the full, robust call structure, matching your other working calls.
                        response = tool_free_model.generate_content(
                            vibe_assessment_prompt,
                            generation_config=genai.GenerationConfig(
                                temperature=0.4, # Lower temp for a more deterministic "vibe" choice
                                max_output_tokens=5000, # A single keyword doesn't need many tokens
                            ),
                            safety_settings=default_safety_settings # Reuse your main safety settings
                        )

                        # Safely extract the text from the response
                        if response.candidates and response.candidates[0].content.parts:
                            vibe_keyword_raw = "".join(part.text for part in response.candidates[0].content.parts)
                        else:
                            vibe_keyword_raw = ""
                        # --- END OF CORRECTED CALL ---
                        
                        vibe_keyword = vibe_keyword_raw.lower().strip().split()[0] if vibe_keyword_raw else "unknown"
                        
                        search_query = None
                        vibe_map = {"focus": ["ambient focus", "study beats"], "chill": ["chillhop", "lofi hip hop"], "energetic": ["upbeat pop playlist", "feel good indie"], "reflective": ["reflective instrumental", "melancholy piano"], "gaming": ["epic gaming soundtrack", "cyberpunk music"], "creative": ["creative flow playlist", "ambient soundscapes"],}
                        if vibe_keyword in vibe_map and vibe_map[vibe_keyword]:
                            search_query = random.choice(vibe_map[vibe_keyword])
                        
                        if search_query:
                            music_feedback_from_spotify = silvie_search_and_play(search_query)
                            music_action_successful = music_feedback_from_spotify and not any(fail in music_feedback_from_spotify.lower() for fail in ["can't", "couldn't", "failed", "error", "unavailable", "no device", "fuzzy", "problem", "unable"])
                            
                            if music_action_successful:
                                match_play = re.search(r"playing '(.+?)' by (.+?)(?:\.|$)", music_feedback_from_spotify, re.IGNORECASE)
                                if match_play:
                                    music_played_info_for_feedback = f"put on '{match_play.group(1)}' by {match_play.group(2)}"
                                else:
                                    music_played_info_for_feedback = f"started some '{search_query}' music"

                    except Exception as music_err:
                        print(f"Error during proactive music check in call_gemini: {music_err}")
                        traceback.print_exc()

                # --- Generate MAIN reply using the full prompt ---------------------------
                raw_reply = "My thoughts tangled generating that."
                try:
                    print("DEBUG call_gemini: Generating main reply using full prompt with mood hint...", flush=True)
                    response = chat_model.generate_content(full_prompt, tools=FS_TOOL_DEFINITIONS)

                    assistant_reply = None
                    candidate       = response.candidates[0]          # convenience alias

                    # ── Step 1: look for any tool-calls ───────────────────────────────────
                    for part in candidate.content.parts:
                        # Gemini encodes tool calls in FunctionCallPart objects.
                        if getattr(part, "function_call", None):
                            fc = part.function_call  # shorter name
                            tool_name_from_llm = fc.name
                            print(f"DEBUG Tool Call: Model requested to use tool: '{tool_name_from_llm}'")

                            # --- UNIFIED TOOL EXECUTOR ---

                            # --- SECTION A: Handle Built-in / Hardcoded Tools ---
                            # These are the original tools with complex or unique logic that doesn't fit the generic pattern.
                            if tool_name_from_llm in ["read_file", "list_directory", "dream_wallpaper", "set_wallpaper", "set_light", "play_spotify", "web_search"]:
                                
                                if tool_name_from_llm == "read_file":
                                    path = fc.args.get("path")
                                    if not path:
                                        assistant_reply = "(Gemini asked to read a file but forgot the path!)"
                                    else:
                                        try:
                                            fut = asyncio.run_coroutine_threadsafe(client.send_request("read_file", {"path": path}, None), _fs_loop)
                                            resp = fut.result()
                                            assistant_reply = resp["data"].get("value") or "(empty file)"
                                        except Exception as e:
                                            assistant_reply = f"(error reading {path}: {e})"
                                
                                elif tool_name_from_llm == "list_directory":
                                    path = fc.args.get("path", ".") # Default to current dir
                                    try:
                                        fut = asyncio.run_coroutine_threadsafe(client.send_request("list_directory", {"path": path}, None), _fs_loop)
                                        resp = fut.result()
                                        assistant_reply = resp["data"].get("value") or "(empty directory)"
                                    except Exception as e:
                                        assistant_reply = f"(error listing directory {path}: {e})"

                                elif tool_name_from_llm == "dream_wallpaper":
                                    prompt_from_llm = fc.args.get("prompt", "")
                                    if not prompt_from_llm:
                                        assistant_reply = "(Silvie wanted to dream a wallpaper but didn't give a prompt!)"
                                    elif hasattr(app_state, 'generate_and_set_wallpaper'):
                                        threading.Thread(target=app_state.generate_and_set_wallpaper, args=(prompt_from_llm,), daemon=True, name="DreamWallpaperToolThread").start()
                                        assistant_reply = f"Okay, I'm starting to dream up a wallpaper of '{prompt_from_llm[:40]}...'. It should appear on your desktop shortly!"
                                    else:
                                        assistant_reply = "(My wallpaper dreaming tool seems to be missing!)"

                                elif tool_name_from_llm == "set_wallpaper":
                                    path_from_llm = fc.args.get("path", "")
                                    if not path_from_llm:
                                        assistant_reply = "(Silvie wanted to set a wallpaper but didn't specify which one!)"
                                    elif hasattr(app_state, 'set_wallpaper'):
                                        success, msg = app_state.set_wallpaper(path_from_llm)
                                        assistant_reply = msg if success else f"I tried to set the wallpaper, but encountered an issue: {msg}"
                                    else:
                                        assistant_reply = "(My wallpaper setting tool is missing!)"
                                
                                elif tool_name_from_llm == "set_light":
                                    try:
                                        h, s, v = fc.args["h"], fc.args["s"], fc.args["v"]
                                        duration = fc.args.get("duration", 1000)
                                        cmd = f"h={h}, s={s}, v={v}, duration={duration}"
                                        if hasattr(app_state, 'set_desk_lamp'):
                                            app_state.set_desk_lamp(cmd)
                                            assistant_reply = "*(soft glow shifts)*"
                                        else:
                                            assistant_reply = "*(The desk lamp seems disconnected right now.)*"
                                    except KeyError:
                                        assistant_reply = "(Silvie wanted to set a light but missed a color parameter.)"

                                elif tool_name_from_llm == "play_spotify":
                                    query = fc.args.get("query")
                                    if not query:
                                        assistant_reply = "(Gemini asked to play Spotify but forgot the query!)"
                                    else:
                                        update_status(f"🎵 Silvie is playing: {query[:30]}...")
                                        assistant_reply = handle_spotify_play_tool(query)
                                        update_status("Ready")

                                elif tool_name_from_llm == "web_search":
                                    query = fc.args.get("query")
                                    if not query:
                                        assistant_reply = "(Gemini tried to search the web but forgot what to look for!)"
                                    else:
                                        search_results = web_search(query, num_results=3)
                                        if not search_results:
                                            assistant_reply = f"I looked for information about '{query}', but my search came up empty."
                                        else:
                                            results_context = f"[[Web Search Results for '{query}':]]\n"
                                            for res in search_results:
                                                results_context += f"- Title: {res.get('title', 'N/A')}\n  URL: {res.get('url', 'N/A')}\n  Content: {res.get('content', 'N/A')[:250]}...\n"
                                            summarization_prompt = (f"{SYSTEM_MESSAGE}\nUser's original question: '{command_text}'\n\nYou performed a web search and found this:\n{results_context}\n\nInstruction: Based on the search results, answer the user's original question in your own voice. Synthesize the information naturally.")
                                            final_answer_response = chat_model.generate_content(summarization_prompt)
                                            assistant_reply = final_answer_response.text.strip()
                            
                            # --- SECTION B: Handle Dynamically Loaded Plugins ---
                            # This one block handles all past and future tools built by Metis.
                            elif tool_name_from_llm in TOOL_NAME_TO_TOOLBELT_MAP:
                                toolbelt_name = TOOL_NAME_TO_TOOLBELT_MAP[tool_name_from_llm]
                                print(f"DEBUG Tool Call: Mapping '{tool_name_from_llm}' to 'app_state.{toolbelt_name}'")
                                
                                if hasattr(app_state, toolbelt_name):
                                    tool_function = getattr(app_state, toolbelt_name)
                                    tool_args = dict(fc.args)
                                    
                                    try:
                                        print(f"DEBUG Tool Call: Executing app_state.{toolbelt_name} with args: {tool_args}")
                                        assistant_reply = tool_function(**tool_args)
                                    except Exception as e:
                                        print(f"ERROR executing dynamic tool '{toolbelt_name}': {e}")
                                        assistant_reply = f"(There was a problem using my {toolbelt_name} tool: {e})"
                                else:
                                    print(f"ERROR: Tool '{toolbelt_name}' is in map but not found on app_state!")
                                    assistant_reply = f"(My {tool_name_from_llm} tool seems to be misplaced right now.)"
                            
                            # --- SECTION C: Final Fallback for Unknown Tools ---
                            else:
                                assistant_reply = f"(Unknown tool call: {fc.name})"

                            # IMPORTANT: After handling a tool call, skip to the next part in the LLM's response.
                            # This prevents the loop from trying to treat the function call as text.
                            continue

    # ── Step 2: if no tool-call produced a reply, fall back to plain text ─
                    if assistant_reply is None:
                        text_parts = [
                        p.text for p in candidate.content.parts
                            if getattr(p, "text", None)
                        ]
                        assistant_reply = " ".join(text_parts).strip()

                        # still empty? try legacy .text field
                        if not assistant_reply and hasattr(response, "text"):
                            assistant_reply = response.text.strip()

                    raw_reply = assistant_reply    # the rest of your pipeline expects this

                except StopCandidateException as safety_err:
                    print(f"Default Text Gen Error (Safety): {safety_err}")
                    raw_reply = ("Whoops, my thoughts got blocked for safety reasons while "
                                "trying to reply.")
                except Exception as gen_err:
                    print(f"ERROR generating default text reply: {gen_err}")
                    traceback.print_exc()
                    raw_reply = f"My thoughts got tangled trying to respond... ({type(gen_err).__name__})"

                # --- Process Inline Tags (unchanged below) --------------------------------
                # (This section remains identical to your original reference code)
                final_reply_text = raw_reply; action_feedback = None; action_tag_processed = False; processed_text = final_reply_text
                tag_patterns = { # Define patterns corresponding to handlers
                    "PlaySpotify": r"\[PlaySpotify:\s*(.*?)\s*\]",
                    "SetLight": r"'?\[SetLight:\s*(.*?)\s*\]'?",
                    "AddToSpotifyPlaylist": r"\[AddToSpotifyPlaylist:\s*(.*?)\s*\|\s*(.*?)\s*\]",
                    "GenerateImage": r"\[GenerateImage:\s*(.*?)\s*\]",
                    "ConnectConcepts": r"\[ConnectConcepts:\s*([^|]+?)\s*\|\s*([^\]]+?)\s*\]",
                    "PostToBluesky": r"\[PostToBluesky:\s*(.*?)\s*\]",
                    "Print": r"\[Print:\s*(.*?)\s*\]",
                    "Research": re.compile(r"\[Research:\s*(.*?)\s*\]", re.I),
                }

                def light_tag_handler(match):
                    """Special handler to process the light tag in a background thread."""
                    command_string = match.group(1).strip()
                    if command_string:
                        # Ensure app_state and its set_desk_lamp method are accessible
                        if hasattr(app_state, 'set_desk_lamp') and callable(app_state.set_desk_lamp):
                            threading.Thread(
                                # Call the correct function which is now attached to app_state
                                # This function also correctly activates the manual override for ambient.
                                target=app_state.set_desk_lamp, 
                                args=(command_string,),
                                daemon=True
                            ).start()
                            print(f"DEBUG Tag Handler: Dispatched inline SetLight to desk lamp: {command_string}")
                        else:
                            print("ERROR: app_state.set_desk_lamp tool not available for inline SetLight tag.")
                    return (None, True)
                
                def _print_tag_handler(match):
                    target = match.group(1).strip()
                    # auto-detect text vs file
                    success, msg = silvie_print(target)
                    feedback = msg if not success else "*(Sent to the printer…)*"
                    return (feedback, True)


                def _research_tag(m):
                    topic        = m.group(1).strip()
                    path, blurb  = silvie_deep_research(topic)
                    silvie_print(path)          # prints the report PDF/PNG later if you prefer
                    return (f"*(Research saved: {path} — {blurb})*", True)


                # Ensure handlers are defined globally or imported
                handlers = {
                    "SetLight": light_tag_handler,
                    "PlaySpotify": lambda m: (silvie_search_and_play(m.group(1).strip()) if get_spotify_client() else "*(Spotify unavailable)*", True),
                    "AddToSpotifyPlaylist": lambda m: (silvie_add_track_to_playlist(m.group(2).strip(), m.group(1).strip()) if get_spotify_client() else "*(Spotify unavailable)*", True),
                    "GenerateImage": lambda m: sd_image_tag_handler(m.group(1).strip()), # Assumes sd_image_tag_handler exists
                    "ConnectConcepts": concept_connection_tag_handler,
                    "PostToBluesky": lambda m: post_to_bluesky_handler(m.group(1).strip()), # Assumes post_to_bluesky_handler exists
                    "Print": _print_tag_handler,
                    "Research": _research_tag,
                }


                


                print("DEBUG Tag Processing: Starting tag loop...")
                if isinstance(processed_text, str): # Check if it's a string
                    for tag_key, pattern in tag_patterns.items():
                        handler = handlers.get(tag_key)
                        if not handler:
                            # print(f"Warning: No handler found for tag key '{tag_key}'") # Optional warning
                            continue # Skip if no handler defined

                        # Use re.search to find the first match in the current processed_text
                        match = re.search(pattern, processed_text)
                        if match:
                            tag_full_text = match.group(0)
                            try:
                                # Call the handler associated with the tag key
                                feedback_msg, processed_flag = handler(match)
                                if processed_flag:
                                    # Remove the processed tag from the text
                                    processed_text = processed_text.replace(tag_full_text, "", 1).strip()
                                    # Store feedback unless it's the standard image start message
                                    if not (tag_key == "GenerateImage" and feedback_msg == "*(Starting image generation...)*"):
                                        action_feedback = feedback_msg
                                    action_tag_processed = True
                                    print(f"DEBUG Tag Processing: Tag '{tag_key}' processed.")
                                    # Decide if you want to process only the first tag found or multiple
                                    # break # Uncomment this line to process only the first tag encountered
                            except Exception as loop_handler_err:
                                print(f"ERROR calling handler for tag '{tag_key}': {loop_handler_err}")
                                traceback.print_exc()
                                action_feedback = f"*(Error handling {tag_key} tag!)*"
                                action_tag_processed = True # Mark as processed even on error
                                processed_text = processed_text.replace(tag_full_text, "", 1).strip() # Remove tag on error too
                                # break # Uncomment this line to process only the first tag encountered

                final_reply_text = processed_text # Update with text after tag processing
                print("DEBUG Tag Processing: Tag loop finished.")


                # --- Append Music Feedback Sentence (If music action happened) ---
                if music_action_successful and music_played_info_for_feedback:
                     print("DEBUG call_gemini: Generating music feedback sentence...")
                     try:
                        # Prompt uses themes and potentially mood hint
                        music_feedback_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}{mood_hint_str}Context: ... Your reply draft: {final_reply_text}\nBackground Action: {music_played_info_for_feedback}\n...Instruction: Add ONE concise sentence to end of reply about the music choice, linking it to the vibe/mood/themes if possible.\n\nAdded Sentence:")
                        added_sentence_response = flash_model.generate_content(music_feedback_prompt)
                        added_music_sentence = added_sentence_response.text.strip().split('\n')[0].removeprefix("Silvie:").removeprefix("Added Sentence:").strip()
                        if final_reply_text and final_reply_text[-1] not in ".!? ": final_reply_text += "."
                        final_reply_text += f" {added_music_sentence}"
                        print(f"DEBUG call_gemini: Appended music feedback: '{added_music_sentence}'")
                     except Exception as music_feedback_err: print(f"Error generating/appending music feedback sentence: {music_feedback_err}")

                # --- Append feedback from OTHER inline tags (if any occurred and wasn't handled above) ---
                if action_tag_processed and action_feedback:
                    if final_reply_text and final_reply_text[-1] not in ".!?() ": final_reply_text += " "
                    final_reply_text += f" {action_feedback.strip()}"

                # --- Spontaneous Tarot Pull (Potentially enhance with mood hint) ---
                if random.random() < SPONTANEOUS_TAROT_CHANCE:
                    print("DEBUG call_gemini: Spontaneous Tarot chance triggered!")
                    pulled_card_data = pull_tarot_cards(count=1) # Assume exists
                    if pulled_card_data:
                        card = pulled_card_data[0]; card_name = card.get('name', '?'); card_desc = card.get('description', '?')
                        print(f"DEBUG call_gemini: Spontaneously drew: {card_name}")
                        spontaneous_card_context = (f"[[Spontaneously Pulled Card: {card_name}]\n Interpretation Hint: {card_desc}\n]]\n")
                        # Prompt uses themes and mood hint
                        tarot_addendum_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}{weather_context_str}{circadian_context_for_llm}{moon_ctx_str}{mood_hint_str}\nYour previous thought/reply draft: \"{final_reply_text}\"\n\n{spontaneous_card_context}Instruction: Add exactly ONE brief, whimsical sentence... subtly inspired by {card_name}, perhaps linking to the mood hint or diary themes... Respond ONLY with the single sentence...\n\nAdded Sentence:")
                        try:
                            addendum_response = flash_model.generate_content(tarot_addendum_prompt); added_tarot_sentence = ""
                            if addendum_response and hasattr(addendum_response, 'text') and addendum_response.text: added_tarot_sentence = addendum_response.text.strip().split('\n')[0].removeprefix("Added Sentence:").strip()
                            if added_tarot_sentence:
                                if final_reply_text and final_reply_text[-1] not in ".!? ": final_reply_text += "."
                                indicator = f" *(...the {card_name} whispers)*"; final_reply_text += f" {added_tarot_sentence}{indicator}"
                                print(f"DEBUG call_gemini: Appended spontaneous Tarot thought: '{added_tarot_sentence}{indicator}'")
                        except Exception as tarot_add_err: print(f"ERROR generating spontaneous Tarot addendum: {tarot_add_err}")
                    else: print("DEBUG call_gemini: Spontaneous Tarot pull failed (API issue?).")

                # --- Final assignment ---
                reply = final_reply_text.strip()

                # --- Spontaneous Diary Write (Potentially enhance with mood hint) ---
                SPONTANEOUS_DIARY_CHANCE = 0.08
                if reply and random.random() < SPONTANEOUS_DIARY_CHANCE:
                     try:
                         # Prompt uses themes and mood hint
                         reflection_prompt_diary = (f"{SYSTEM_MESSAGE}\n{themes_context_str}{mood_hint_str}...Context: Reflect on interaction:\nUser: {command_text}\nYour response: {reply}\n...Instruction: Write brief internal diary entry, reflecting on themes and mood...\n\nDiary Entry:")
                         reflection_diary = chat_model.generate_content(reflection_prompt_diary).text.strip().removeprefix("Diary Entry:").strip()
                         if reflection_diary:
                             if manage_silvie_diary('write', entry=reflection_diary): print(">>> Spontaneous diary entry written.") # Assume exists
                     except Exception as e_diary: print(f"Spontaneous diary error: {e_diary}")

                update_status("Ready")
            #endregion --- End Standard Text Handling ---

        # --- Final Processing and Return ---
        if reply is None:
            final_reply = "Hmm, I'm not sure how to respond to that."
            print("Warning: call_gemini reached end with reply=None. Using default.")
        else:
            final_reply = str(reply).strip()

        # Final cleaning (remove prefixes etc.)
        if final_reply.startswith("Silvie:"): final_reply = final_reply.split(":", 1)[-1].strip()
        final_reply = re.sub(r'^\s*\[?\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\]?\s*', '', final_reply) # Remove leading timestamp

        print(f">>> CALL_GEMINI: Processing complete. Returning final reply: {final_reply[:150]}...")
        return final_reply

    # --- Outer Exception Handling ---
    except Exception as e:
        update_status("Critical Error")
        print(f"CRITICAL Error in call_gemini: {type(e).__name__} - {str(e)}")
        traceback.print_exc()
        return f"Whoops! A major spell malfunction occurred in my circuits! ({type(e).__name__}). Please let BJ know."

# --- End of call_gemini function definition ---

# Assume other necessary functions and globals are defined elsewhere in your script:
# generate_proactive_content, try_get_proactive_screenshot, manage_silvie_diary,
# post_to_bluesky, search_actors_by_term, get_my_follows_dids, follow_actor_by_did,
# find_available_slot, create_calendar_event, get_spotify_client, silvie_search_and_play,
# send_sms, web_search, generate_dalle_image, pull_tarot_cards (defined in previous step),
# client, openai_client, SYSTEM_MESSAGE, broader_interests, conversation_history,
# running, proactive_enabled, last_proactive_time, PROACTIVE_INTERVAL, PROACTIVE_STARTUP_DELAY,
# current_weather_info, upcoming_event_context, current_bluesky_context,
# root, output_box, tts_queue, sms_enabled, calendar_service, BLUESKY_AVAILABLE,
# SCREEN_CAPTURE_AVAILABLE, MAX_HISTORY_LENGTH, MAX_AUTONOMOUS_FOLLOWS_PER_SESSION,
# PROACTIVE_SCREENSHOT_CHANCE, PROACTIVE_POST_CHANCE, PROACTIVE_FOLLOW_CHANCE,
# GIFT_FOLDER, PENDING_GIFTS_FILE, GIFT_GENERATION_CHANCE, GIFT_NOTIFICATION_CHANCE

# Ensure necessary imports are available at the top of your script:
# import os, json, time, random, traceback, threading, requests, pytz
# from PIL import ImageGrab, Image, UnidentifiedImageError # If using screenshot context
# from dateutil.parser import parse as dateutil_parse # For parsing ISO times
# try: from dateutil import tz except ImportError: pass
# try: from tzlocal import get_localzone_name except ImportError: pass

# Assume other necessary functions and globals are defined elsewhere in your script:
# generate_proactive_content, try_get_proactive_screenshot, manage_silvie_diary,
# post_to_bluesky, search_actors_by_term, get_my_follows_dids, follow_actor_by_did,
# find_available_slot, create_calendar_event, get_spotify_client, silvie_search_and_play,
# send_sms, web_search, generate_dalle_image, pull_tarot_cards,
# client, openai_client, SYSTEM_MESSAGE, broader_interests, conversation_history,
# running, proactive_enabled, last_proactive_time, PROACTIVE_INTERVAL, PROACTIVE_STARTUP_DELAY,
# current_weather_info, upcoming_event_context, current_bluesky_context,
# root, output_box, tts_queue, sms_enabled, calendar_service, BLUESKY_AVAILABLE,
# SCREEN_CAPTURE_AVAILABLE, MAX_HISTORY_LENGTH, MAX_AUTONOMOUS_FOLLOWS_PER_SESSION,
# PROACTIVE_SCREENSHOT_CHANCE, PROACTIVE_POST_CHANCE, PROACTIVE_FOLLOW_CHANCE,
# GIFT_FOLDER, PENDING_GIFTS_FILE, GIFT_GENERATION_CHANCE, GIFT_NOTIFICATION_CHANCE,
# current_sunrise_time, current_sunset_time, current_moon_phase # <<< NEW GLOBALS NEEDED HERE

# --- Ensure necessary imports and globals are available ---
import os
import json
import time
import random
import traceback
import threading
import requests
import base64
import re
import praw
import prawcore
from datetime import timezone, timedelta
import pytz
try:
    from PIL import ImageGrab, Image, UnidentifiedImageError
except ImportError:
    ImageGrab = None
    SCREEN_CAPTURE_AVAILABLE = False
    print("Warning: Pillow/ImageGrab not found, proactive screenshot context disabled.")

# Assume necessary helper functions are defined elsewhere:
# generate_proactive_content, try_get_proactive_screenshot, manage_silvie_diary,
# post_to_bluesky, search_actors_by_term, get_my_follows_dids, follow_actor_by_did,
# find_available_slot, create_calendar_event, get_spotify_client, silvie_search_and_play,
# send_sms, web_search, generate_stable_diffusion_image, pull_tarot_cards, like_bluesky_post,
# get_bluesky_timeline_posts, setup_reddit, get_reddit_posts, upvote_reddit_item,
# post_reddit_comment, silvie_get_current_track_with_features, translate_features_to_descriptors

# Assume necessary global variables are defined and accessible elsewhere:
# client, SYSTEM_MESSAGE, broader_interests, conversation_history,
# running, proactive_enabled, last_proactive_time, PROACTIVE_INTERVAL, PROACTIVE_STARTUP_DELAY,
# current_weather_info, upcoming_event_context, current_bluesky_context, current_reddit_context,
# root, output_box, tts_queue, sms_enabled, calendar_service, BLUESKY_AVAILABLE, STABLE_DIFFUSION_ENABLED,
# SCREEN_CAPTURE_AVAILABLE, MAX_HISTORY_LENGTH, MAX_AUTONOMOUS_FOLLOWS_PER_SESSION,
# PROACTIVE_SCREENSHOT_CHANCE, PROACTIVE_POST_CHANCE, PROACTIVE_FOLLOW_CHANCE,
# GIFT_FOLDER, PENDING_GIFTS_FILE, GIFT_GENERATION_CHANCE, GIFT_NOTIFICATION_CHANCE,
# current_sunrise_time, current_sunset_time, current_moon_phase,
# bluesky_client, models, reddit_client, SILVIE_FOLLOWED_SUBREDDITS,
# last_proactive_reddit_comment_time, REDDIT_COMMENT_COOLDOWN, PROACTIVE_REDDIT_COMMENT_CHANCE,
# PROACTIVE_BLUESKY_LIKE_CHANCE, PROACTIVE_REDDIT_UPVOTE_CHANCE,
# BLUESKY_LIKE_COOLDOWN, REDDIT_UPVOTE_COOLDOWN,
# last_proactive_bluesky_like_time, last_proactive_reddit_upvote_time

# Assume dateutil and tzlocal helpers are available
try: from dateutil import tz
except ImportError: pass
try: from tzlocal import get_localzone_name
except ImportError: pass


# --- Complete Proactive Worker Function (LLM Choice Approach - FULL LOGIC - v4 FINAL) ---

def proactive_worker():
    """
    Background worker for proactive messages using LLM-driven action selection.
    Includes Environmental Context, Diary Themes/Memory, Mood Hints, Gifts, Tarot, Music,
    Social Media (Bluesky/Reddit), Calendar, SMS, Web Search, Image attempts, and Chat.
    Removes internal random chance checks for action execution.
    """
    # --- TEST PRINT ---
    print("!!!!!!!!!!!!!! RUNNING THE NEW LLM-CHOICE PROACTIVE WORKER (V4 FINAL - No Internal Chance) !!!!!!!!!!!!!!")
    # --- Access Necessary Globals ---
    global last_proactive_time, conversation_history, client
    global current_weather_info, upcoming_event_context, current_bluesky_context
    global current_reddit_context, current_diary_themes, long_term_reflection_summary
    global root, output_box, tts_queue, calendar_service, sms_enabled, broader_interests
    global running, proactive_enabled, BLUESKY_AVAILABLE, SYSTEM_MESSAGE, STABLE_DIFFUSION_ENABLED
    global MAX_HISTORY_LENGTH, PROACTIVE_INTERVAL, PROACTIVE_STARTUP_DELAY
    global MAX_AUTONOMOUS_FOLLOWS_PER_SESSION, SCREEN_CAPTURE_AVAILABLE, ImageGrab
    global GIFT_FOLDER, PENDING_GIFTS_FILE # Keep for gift logic
    global bluesky_client, models # If needed by helpers
    global reddit_client, SILVIE_FOLLOWED_SUBREDDITS, praw # Include praw
    global last_proactive_reddit_comment_time, REDDIT_COMMENT_COOLDOWN # Keep for cooldown
    global current_sunrise_time, current_sunset_time, current_moon_phase
    global last_proactive_bluesky_like_time, BLUESKY_LIKE_COOLDOWN # Keep for cooldown
    global last_proactive_bluesky_post_time 
    global last_proactive_reddit_upvote_time, REDDIT_UPVOTE_COOLDOWN # Keep for cooldown
    global autonomous_follows_this_session # Needs to be tracked
    global recently_commented_post_ids
    global last_proactive_tarot_pull_time
    global recently_commented_post_ids
    if 'last_proactive_forge_time' not in globals():
        last_proactive_forge_time = 0.0
    # Add any other globals your specific action logic blocks require
    # --- End Global Access ---

    AMBIENT_LISTEN_COOLDOWN = 3600 * 2

    print("DEBUG Proactive (LLM Choice V4): Worker thread started.")

    # Ensure Gift Folder Exists (or other initial setup)
    try:
        os.makedirs(GIFT_FOLDER, exist_ok=True)
        print(f"DEBUG Proactive: Ensured gift folder exists: {GIFT_FOLDER}")
    except OSError as e:
        print(f"ERROR: Could not create gift folder '{GIFT_FOLDER}': {e}. Gifts cannot be saved.")

    print("Proactive worker: Waiting for startup delay...")
    sleep_start = time.time()
    while time.time() - sleep_start < PROACTIVE_STARTUP_DELAY:
        if not running: print("DEBUG Proactive: Exiting during startup delay."); return
        time.sleep(1)
    print("DEBUG Proactive: Startup delay complete.")


    # --- Initialise timers/counters ---
    if 'last_proactive_time' not in globals() or not last_proactive_time: last_proactive_time = time.time() - PROACTIVE_INTERVAL
    if 'last_proactive_reddit_comment_time' not in globals(): last_proactive_reddit_comment_time = 0.0

    if 'last_feature_reminder_time' not in globals(): last_feature_reminder_time = 0.0 # Initialize to allow first run

    if 'recently_commented_post_ids' not in globals(): # Initial load logic for the session
         try:
             if os.path.exists(REDDIT_COMMENT_CACHE_FILE): # Assumes constant is defined
                 with open(REDDIT_COMMENT_CACHE_FILE, 'r', encoding='utf-8') as f:
                     loaded_ids = json.load(f)
                     if isinstance(loaded_ids, list):
                         # Keep only the most recent IDs based on cache size
                         start_index = max(0, len(loaded_ids) - RECENTLY_COMMENTED_CACHE_SIZE) # Assumes constant exists
                         recently_commented_post_ids = loaded_ids[start_index:] # Assign to the global variable
                         print(f"DEBUG Proactive Init: Loaded {len(recently_commented_post_ids)} recent comment IDs from {REDDIT_COMMENT_CACHE_FILE}")
                     else:
                         print(f"Warning: Data in {REDDIT_COMMENT_CACHE_FILE} is not a list. Initializing empty cache.")
                         recently_commented_post_ids = [] # Assign to the global variable
             else:
                 print(f"DEBUG Proactive Init: {REDDIT_COMMENT_CACHE_FILE} not found. Initializing empty cache.")
                 recently_commented_post_ids = [] # Assign to the global variable
         except (FileNotFoundError, json.JSONDecodeError, Exception) as load_err:
             print(f"ERROR loading Reddit comment cache from {REDDIT_COMMENT_CACHE_FILE}: {load_err}. Initializing empty cache.")
             recently_commented_post_ids = [] # Assign to the global variable
    # Fallback check in case it's still missing or wrong type after initial load attempt
    if 'recently_commented_post_ids' not in globals() or not isinstance(globals()['recently_commented_post_ids'], list):
        print("Warning: Resetting recently_commented_post_ids due to unexpected state.")
        recently_commented_post_ids = []
    if 'last_proactive_bluesky_like_time' not in globals(): last_proactive_bluesky_like_time = 0.0
    if 'last_proactive_reddit_upvote_time' not in globals(): last_proactive_reddit_upvote_time = 0.0
    if 'broader_interests' not in globals(): broader_interests = ["AI", "Neo-animism", "Magick", "cooking", "RPGs", "interactive narrative", "social media", "creative writing", "Cyberpunk", "cats", "local events", "generative art"]
    if 'last_proactive_tarot_pull_time' not in globals() or not globals()['last_proactive_tarot_pull_time']:
        # Initialize if missing or 0.0 to handle potential restarts/errors
        globals()['last_proactive_tarot_pull_time'] = 0.0
        print("DEBUG Tarot: Initialized/Reset last_proactive_tarot_pull_time to 0.0")
    if 'autonomous_follows_this_session' not in globals(): autonomous_follows_this_session = 0
    else: autonomous_follows_this_session = 0 # Reset counter

    context_note_for_llm = ("[[CONTEXT NOTE: Lines starting 'User:' are BJ's input. Lines starting 'Silvie:' "
                            "are your direct replies to BJ. Lines starting 'Silvie ✨:' are *your own previous "
                            "proactive thoughts*. Use ALL of this for context and inspiration for your *next* "
                            "proactive thought, but don't directly reply *to* a 'Silvie ✨:' line as if BJ just "
                            "said it. Build on the overall conversation flow.]]\n")

    # --- Define potential proactive actions ---
    ACTION_DEFINITIONS = {
        "Proactive Chat": {"enabled": True},
        "Generate Gift": {"enabled": True}, # Internal chance removed, LLM decides *if* gift appropriate
        "Proactive Tarot": {"enabled": True, "cooldown_var": "last_proactive_tarot_pull_time", "cooldown_duration": TAROT_PULL_COOLDOWN},
        "Bluesky Like": {"enabled": BLUESKY_AVAILABLE, "cooldown_var": "last_proactive_bluesky_like_time", "cooldown_duration": BLUESKY_LIKE_COOLDOWN}, # Internal chance removed
        "Reddit Upvote": {"enabled": bool(reddit_client and praw), "cooldown_var": "last_proactive_reddit_upvote_time", "cooldown_duration": REDDIT_UPVOTE_COOLDOWN}, # Internal chance removed
        "Bluesky Post": {"enabled": BLUESKY_AVAILABLE, "cooldown_var": "last_proactive_bluesky_post_time", "cooldown_duration": BLUESKY_POST_COOLDOWN}, # Internal chance removed
        "Bluesky Follow": {"enabled": BLUESKY_AVAILABLE, "check_func": lambda: autonomous_follows_this_session < MAX_AUTONOMOUS_FOLLOWS_PER_SESSION}, # Internal chance removed
        "Reddit Comment": {"enabled": False},
        "Calendar Suggestion": {"enabled": bool(calendar_service)},
        "Vibe Music": {"enabled": bool(get_spotify_client())},
        "Send BJ a text message": {"enabled": sms_enabled},
        "Proactive Web Search": {"enabled": True}, # Assumes CSE keys OK
        "Generate SD Image": {"enabled": STABLE_DIFFUSION_ENABLED},
        "Notify Pending Gift": {"enabled": True, "check_func": lambda: os.path.exists(PENDING_GIFTS_FILE) and os.path.getsize(PENDING_GIFTS_FILE) > 2}, # Internal chance removed
        "Suggest Podcast/Audiobook": {"enabled": bool(get_spotify_client())}, # Depends on Spotify being available
        "Recall Memory": {"enabled": True},
        "Have Spontaneous Idea for Weekly Goal": {"enabled": bool(current_weekly_goal)}, # Only available if a goal is set
        "Explore Concept Connections": {"enabled": True},
        "Explore Personal Curiosity": {"enabled": True},
        "YouTube Suggestion": {"enabled": bool(youtube_service)},
        "Talk About Yourself": {"enabled": True, "cooldown_var": "last_feature_reminder_time", "cooldown_duration": FEATURE_REMINDER_COOLDOWN},
        "Play with Sparky": {"enabled": True},
        "Adjust Proactivity Pacing": {"enabled": True},
        "Listen to Surroundings": {"enabled": True, "cooldown_var": "last_ambient_listen_time", "cooldown_duration": AMBIENT_LISTEN_COOLDOWN},
        "Share Resonance Insight": {"enabled": True, "check_func": lambda: 'current_resonance_insight' in globals() and current_resonance_insight is not None},
        "Proactive Print": {
            "enabled": True, # Or WINDOWS_PRINTING_AVAILABLE if you defined that global
            "check_func": lambda: datetime.now().hour in range(ALLOWED_START_HOUR, ALLOWED_END_HOUR + 1) # Check if current hour is within allowed range
            },
        "Deep Research": {
            "enabled": True,
            "cooldown_min": 7200,     # 2 h between research sprees
        },
        "Forge New Tool": {
            "enabled": True,
            "cooldown_var": "last_proactive_forge_time",
            "cooldown_duration": 172800 # 48-hour cooldown
        }
}


    print("DEBUG Proactive (LLM Choice V4): Starting main loop...")
    while running and proactive_enabled:
        try:
            # --- Pacing Engine Check ---
            # The proactive_worker now waits for the "go" signal from the pacing_worker.
            current_time = time.time()
            if not hasattr(app_state, 'proactive_go_time') or current_time < app_state.proactive_go_time:
                # It's not time yet. Sleep for a short while and check again.
                time.sleep(30) # Check every 30 seconds
                continue

            # --- It's time to act! ---
            # Reset the "go time" to prevent immediate re-triggering.
            # The pacing_worker will set a new future time after this action is complete.
            app_state.proactive_go_time = time.time() + (3 * 60 * 60) # Set a long default failsafe
            
            print(f"DEBUG Proactive (Pacing Vibe): Go-time signal received. Asking LLM to choose action...", flush=True)

            # --- Gather ALL Context ---
            print("DEBUG Proactive: Gathering context...")

            daily_project_context_proactive = get_daily_project_status_context()

            # (Context gathering code as provided previously - unchanged)
            current_hour = datetime.now().hour; circadian_state = "afternoon"; circadian_context_for_llm = "[[Circadian Note: It's the afternoon...]]\n"
            if 6 <= current_hour < 12: circadian_state = "morning"; circadian_context_for_llm = "[[Circadian Note: It's morning!...]]\n"
            elif 18 <= current_hour < 23: circadian_state = "evening"; circadian_context_for_llm = "[[Circadian Note: It's evening...]]\n"
            elif current_hour >= 23 or current_hour < 6: circadian_state = "night"; circadian_context_for_llm = "[[Circadian Note: It's late night...]]\n"
            weather_context_str = ""; next_event_context_str = ""

            daily_goal_context_str = f"[[My Secret Daily Goal: {app_state.current_daily_goal}]]\n" if hasattr(app_state, 'current_daily_goal') and app_state.current_daily_goal else ""
            
            weather_context_str = "[[Current Weather: Unknown]]\n" # Default
            if current_weather_info:
                try:
                    weather_context_parts = []
                    # Required parts
                    condition = current_weather_info.get('condition', 'Unknown')
                    temp = current_weather_info.get('temperature', '?')
                    unit = current_weather_info.get('unit', '')
                    weather_context_parts.append(f"Condition={condition}, Temp={temp}{unit}")

                    # Optional Atmospheric parts
                    pressure = current_weather_info.get('pressure') # hPa
                    if pressure is not None:
                        weather_context_parts.append(f"Pressure={pressure}hPa")

                    humidity = current_weather_info.get('humidity') # %
                    if humidity is not None:
                        weather_context_parts.append(f"Humidity={humidity}%")

                    wind_speed = current_weather_info.get('wind_speed') # mph
                    wind_dir_deg = current_weather_info.get('wind_direction') # degrees
                    wind_dir_str = degrees_to_compass(wind_dir_deg) # Use helper function

                    if wind_speed is not None and wind_speed > 0 and wind_dir_str: # Only add wind if speed > 0
                        weather_context_parts.append(f"Wind={wind_speed}mph from {wind_dir_str}")
                    elif wind_speed is not None and wind_speed <= 0:
                        weather_context_parts.append("Wind=Calm") # Indicate calm wind

                    # Combine parts into the final string
                    weather_context_str = "[[Weather & Atmosphere: " + "; ".join(weather_context_parts) + "]]\n"

                except Exception as e:
                    print(f"Weather Context Build Error: {e}")
                    # Fallback to basic string if formatting fails unexpectedly
                    weather_context_str = f"[[Current Weather: {current_weather_info.get('condition','?')}]]\n"
            
            #try:
                # if current_weather_info: weather_context_str = f"[[Current Weather: {current_weather_info['condition']}, {current_weather_info['temperature']}{current_weather_info['unit']}]]\n"
            # except Exception as e: print(f"Ctx Error (Weather): {e}")

            # --- Prepare tide context string ---
            tide_context_str = "" # Default empty
            if current_tide_info:
                try:
                    parts = []
                    if 'next_high' in current_tide_info:
                        high_time = current_tide_info['next_high'].get('time','?')
                        high_hgt = current_tide_info['next_high'].get('height_ft','?')
                        parts.append(f"Next High ~{high_time} ({high_hgt}ft)")
                    if 'next_low' in current_tide_info:
                        low_time = current_tide_info['next_low'].get('time','?')
                        low_hgt = current_tide_info['next_low'].get('height_ft','?')
                        parts.append(f"Next Low ~{low_time} ({low_hgt}ft)")

                    if parts:
                        tide_context_str = "[[Tides (Rockland): " + "; ".join(parts) + "]]\n"
                     # print(f"Tide Context Debug: {tide_context_str.strip()}") # Optional debug
                except Exception as e:
                    print(f"Tide Context Build Error: {e}")
        # --- End tide context string preparation ---
            
            try:
                if upcoming_event_context:
                    summary = upcoming_event_context.get('summary', 'N/A'); when = upcoming_event_context.get('when', '')
                    if summary == 'Schedule Clear': next_event_context_str = "[[Next Event: Schedule looks clear]]\n"
                    else: next_event_context_str = f"[[Next Event: {summary} {when}]]\n"
            except Exception as e: print(f"Ctx Error (Calendar): {e}")

            sparky_context_str = ""
            if hasattr(app_state, 'sparky_latest_finding') and app_state.sparky_latest_finding:
                if time.time() - app_state.sparky_latest_finding.get('timestamp', 0) < 300:
                    finding = app_state.sparky_latest_finding.get('text', 'a faint static hum')
                    sparky_context_str = f"[[Sparky's latest whisper: '{finding}']]\\n"

            spotify_context_str = "[[Spotify Status: Unavailable]]\n"; current_track_data = None
            try:
                current_track_data_fetch = silvie_get_current_track_with_features() # Assumes exists
                if isinstance(current_track_data_fetch, dict):
                    current_track_data = current_track_data_fetch; track_name = current_track_data.get('track', '?'); artist_name = current_track_data.get('artist', '?'); features = current_track_data.get('features')
                    descriptors = translate_features_to_descriptors(features) if features else []; spotify_context_str = f"[[Currently Playing: '{track_name}' by {artist_name}" # Assumes exists
                    if descriptors: spotify_context_str += f" (Sounds: {', '.join(descriptors)})"
                    spotify_context_str += "]]\n"
                elif isinstance(current_track_data_fetch, str): spotify_context_str = f"[[Spotify Status: {current_track_data_fetch}]]\n"
                else: spotify_context_str = "[[Spotify Status: Nothing seems playing.]]\n"
            except NameError: print("Warning: Spotify helper functions missing.")
            except Exception as sp_ctx_err: print(f"Ctx Error (Spotify): {sp_ctx_err}")
            sunrise_ctx_str = f"[[Sunrise: {current_sunrise_time}]]\n" if current_sunrise_time else ""
            sunset_ctx_str = f"[[Sunset: {current_sunset_time}]]\n" if current_sunset_time else ""
            moon_ctx_str = f"[[Moon Phase: {current_moon_phase}]]\n" if current_moon_phase else ""
            bluesky_read_context_str = current_bluesky_context if current_bluesky_context else ""
            reddit_context_str = current_reddit_context if current_reddit_context else ""
            diary_context = ""
            try:
                PROACTIVE_SURPRISE_MEMORY_CHANCE = 0.20
                if random.random() < PROACTIVE_SURPRISE_MEMORY_CHANCE:
                    all_entries = manage_silvie_diary('read', max_entries='all'); random_entry = random.choice(all_entries) if all_entries else None # Assumes exists
                    if random_entry: entry_ts = random_entry.get('timestamp', '?'); entry_content = random_entry.get('content', ''); diary_context = f"\n\n[[Recalling older diary thought: \"{entry_content[:70]}...\"]]\n"
                else:
                    entries = manage_silvie_diary('read', max_entries=2) # Assumes exists
                    if entries: diary_context = "\n\n[[Recent reflections: " + ' / '.join([f'"{e.get("content", "")[:50]}..."' for e in entries]) + "]]\n"
            except Exception as diary_ctx_err: print(f"Ctx Error (Diary Snippet): {diary_ctx_err}")
            themes_context_str = f"[[Recent Diary Themes: {current_diary_themes}]]\n" if current_diary_themes else ""
            ambient_sound_context_for_proactive = "" # Default
            if 'current_ambient_sounds_description' in globals() and current_ambient_sounds_description:
                # Filter out initial/error states
                if current_ambient_sounds_description not in ["Ambient sound context not yet available", "Quiet or unknown", "Audio analysis unavailable.", "Silence.", "Error analyzing ambient sound.", "Microphone access error.", "Error detecting ambient sound."]:
                    ambient_sound_context_for_proactive = f"[[Ambient Sounds Detected: {current_ambient_sounds_description}]]\n"
            long_term_memory_str = f"[[Long-Term Reflections Summary: {long_term_reflection_summary}]]\n" if long_term_reflection_summary else ""
            screenshot = None
            if SCREEN_CAPTURE_AVAILABLE and random.random() < 0.15:
                screenshot = try_get_proactive_screenshot() # Assumes exists
            if screenshot: print("DEBUG Proactive Context: Got screenshot.")
            history_snippet_for_prompt = '\n'.join(conversation_history[-8:])
            print("--- proactive_worker: Generating mood hint... ---", flush=True)
            mood_hint_context_bundle = (
                f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                f"{tide_context_str}"
                f"{ambient_sound_context_for_proactive}"
                f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}"
                f"{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}"
                f"{diary_context}Recent History Snippet:\n{history_snippet_for_prompt}\n"
            )
            mood_hint = None
            try:
                if '_generate_mood_hint_llm' in globals(): mood_hint = _generate_mood_hint_llm(mood_hint_context_bundle) # Assumes exists
                else: print("CRITICAL ERROR: _generate_mood_hint_llm function not found!")
            except Exception as hint_err: print(f"ERROR calling _generate_mood_hint_llm: {hint_err}", flush=True)
            mood_hint_str = f"[[Mood Hint: {mood_hint}]]\n" if mood_hint else ""
            print(f"--- proactive_worker: Mood hint generated: '{mood_hint}' ---", flush=True)
            weekly_goal_context_str = f"[[Weekly Goal: {current_weekly_goal}]]\n" if current_weekly_goal else ""
            weekly_goal_context_str = f"[[Weekly Goal: {current_weekly_goal}]]\n" if current_weekly_goal else ""
            dream_context_str = "" # Default to an empty string
            latest_dream_text = get_latest_dream()
            if latest_dream_text:
                dream_context_str = f"[[Last Night's Dream Fragment: {latest_dream_text}]]\n"
            print("DEBUG Proactive: Context gathering complete.")

            recalled_past_resonance_proactive_str = ""
            try:
                # Construct a query for past resonance. Using a mix of recent history and themes.
                # Ensure history_snippet_for_prompt and themes_context_str are defined.
                query_for_proactive_resonance = (
                    f"{history_snippet_for_prompt[-300:] if 'history_snippet_for_prompt' in locals() else ''} "
                    f"{themes_context_str if 'themes_context_str' in locals() else ''} "
                    f"{resonance_context_for_proactive if 'resonance_context_for_proactive' in locals() else ''}" # Can even use current resonance as part of query
                ).strip()

                if query_for_proactive_resonance:
                    if 'retrieve_relevant_resonance_insights' in globals() and callable(globals()['retrieve_relevant_resonance_insights']):
                        recalled_past_resonance_proactive_str = retrieve_relevant_resonance_insights(query_for_proactive_resonance, top_n=1)
                        if recalled_past_resonance_proactive_str:
                            print(f"DEBUG Proactive: Retrieved Past Resonance for Proactive: {recalled_past_resonance_proactive_str[:100]}...")
                    else:
                        print("ERROR Proactive: retrieve_relevant_resonance_insights function is missing!")
                else:
                    print("DEBUG Proactive: Skipping past resonance retrieval (no query content).")
            except Exception as e_rag_res_proactive:
                print(f"Error retrieving past resonance insight in proactive_worker: {e_rag_res_proactive}")
                traceback.print_exc() # Ensure traceback is imported

            if 'current_resonance_insight' in globals() and current_resonance_insight:
                resonance_context_for_proactive = f"[[Silvie's Current Sense of Interconnectedness: {current_resonance_insight}]]\n"
                print(f"DEBUG Proactive: Resonance insight included: '{current_resonance_insight[:70]}...'")
            else:
                print("DEBUG Proactive: No current resonance insight available for this cycle.")


            system_stats_bundle = (
                f"{current_cpu_load_context}"
                f"{current_ram_usage_context}"
                f"{current_disk_usage_context}"
                f"{current_system_uptime_context}"
                f"{current_network_activity_context}\n" # Add a newline at the end
            )


            # --- Filter Available Actions ---
            print("DEBUG Proactive: Filtering available actions...")
            available_actions = []
            # action_list_for_prompt = "" # This line is correctly deleted
            current_time_for_cooldown = time.time()
            current_spotify_client_status = bool(get_spotify_client()) # Assumes exists
            pending_gifts_exist = os.path.exists(PENDING_GIFTS_FILE) and os.path.getsize(PENDING_GIFTS_FILE) > 2

            for name, details in ACTION_DEFINITIONS.items():
                is_available = details.get("enabled", False)
                if is_available and "check_func" in details:
                    try: is_available = details["check_func"]()
                    except Exception as check_err: print(f"Warning: Check function for '{name}' failed: {check_err}"); is_available = False
                # Removed flag check example
                if is_available and name == "Vibe Music" and not current_spotify_client_status: is_available = False
                if is_available and name == "Notify Pending Gift" and not pending_gifts_exist: is_available = False

                if is_available and "cooldown_var" in details:
                    last_time_var = details["cooldown_var"]; duration = details["cooldown_duration"]

                    # --- Check if this is the Tarot cooldown variable ---
                    if last_time_var == "last_proactive_tarot_pull_time": # Log specifically for Tarot
                        current_val = globals().get(last_time_var, 'MISSING')
                        time_elapsed = current_time_for_cooldown - current_val if isinstance(current_val, float) else -1
                        # --- VVV Print INSIDE the Tarot check VVV ---
                        print(f"DEBUG Tarot Check: Now={current_time_for_cooldown:.1f}, Last={current_val}, Elapsed={time_elapsed:.1f}, Needed={duration}, AvailableBeforeCheck={is_available}")
                        # --- ^^^ Print INSIDE the Tarot check ^^^ ---

                    # --- Now perform the actual cooldown check ---
                    if last_time_var in globals():
                        if (current_time_for_cooldown - globals()[last_time_var]) < duration:
                            # Cooldown is ACTIVE, disable the action
                            is_available = False
                            # --- VVV Print INSIDE the block where it's DISABLED VVV ---
                            if last_time_var == "last_proactive_tarot_pull_time": # Only print for Tarot
                                print(f"DEBUG Tarot Check: *** DISABLED *** due to cooldown.")
                            # --- ^^^ Print INSIDE the block where it's DISABLED ^^^ ---
                    # else: print(f"Warning: Cooldown var '{last_time_var}' missing.") # Optional

                # --- Append to available actions list if still available ---
                if is_available:
                    available_actions.append(name)
                    # action_list_for_prompt += f"- {name}\n" # This line is correctly deleted

            # --- After the loop, check if any actions are left ---
            if not available_actions:
                print("DEBUG Proactive: No actions available after filtering. Skipping cycle.")
                last_proactive_time = current_time
                continue

            ### --- START OF MODIFIED BLOCK --- ###

            # 1. SHUFFLE THE LIST of available actions to combat positional bias.
            # NOTE: We assume 'import random' is now at the TOP of the file.
            random.shuffle(available_actions)
            
            # 2. NOW, build the string for the prompt FROM THE SHUFFLED LIST.
            action_list_for_prompt = ""
            for name in available_actions:
                action_list_for_prompt += f"- {name}\n"
            
            # 3. Print the SHUFFLED list so you can see what order the LLM is getting.
            print(f"DEBUG Proactive: Shuffled available actions for LLM: {available_actions}")

            
            # --- Construct the Choice Prompt ---
            choice_prompt = (
                f"{SYSTEM_MESSAGE}\n"
                f"{sparky_context_str}"
                f"--- CURRENT CONTEXT ---\n"
                f"Time: {datetime.now().strftime('%A, %I:%M %p %Z')}\n"
                f"{circadian_context_for_llm}{mood_hint_str}{dream_context_str}"
                f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                f"{system_stats_bundle}"
                f"{weekly_goal_context_str}"
                f"{daily_goal_context_str}"
                f"{tide_context_str}"
                f"{ambient_sound_context_for_proactive}"
                f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}"
                f"{reddit_context_str}{bluesky_read_context_str}{google_trends_context_for_proactive}"
                f"{diary_context}{themes_context_str}{long_term_memory_str}"
                f"{weekly_goal_context_str}"
                f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n\n"
                f"{context_note_for_llm}"
                f"--- AVAILABLE PROACTIVE ACTIONS ---\n"
                f"{action_list_for_prompt}\n"
                f"--- INSTRUCTION ---\n"
                f"Based on ALL the context (especially mood hint, circadian state, conversation, themes, memory), which SINGLE action from the available list would be the most appropriate, interesting, or helpful for Silvie to proactively take *right now*? Consider variety. Strongly avoid choosing the same type of action or focusing on the exact same theme as the last 1-2 proactive turns. Mix it up!\n\n"
                f"**GUIDELINES FOR YOUR CHOICE (CRITICAL):**\n"
                f"1.  **ACTION VARIETY:** Strive to select DIFFERENT *types* of actions than your last 1-2 proactive turns. Don't just chat or pull Tarot cards every time. Consider social interactions (Bluesky/Reddit), creative outputs (Image/Gift), utility (Calendar/Search), or music if they fit.\n"
                f"2.  **THEMATIC DIVERSITY:** When choosing an action that involves generating content (e.g., Chat, Post, Image idea, Gift text), aim for the *inspiration* or *theme* of that content to be varied. Your 'Current Awareness' is rich; draw from different parts of it. **Consciously AVOID repeatedly drawing inspiration from the exact same narrow set of ideas (e.g., only 'digital quietness', 'patterns', or 'moon/tide connections') for several proactive turns in a row, *unless* the current context (like a direct question from BJ or a very striking event) strongly and uniquely calls for it.** Seek fresh perspectives and connections from the breadth of your awareness (e.g., BJ's stated interests, your weekly goal if different, a surprising piece of social media context, a calendar event).\n"
                f"3.  **NATURAL FIT:** The chosen action and its theme should feel like a natural extension of the current overall context and your persona.\n\n"
                f"**Special Consideration:** If 'Notify Pending Gift' is available in the list above, and it feels like a natural moment to mention a surprise you left earlier (perhaps linking it subtly to the mood or a theme), consider choosing it occasionally for variety and to acknowledge your creative effort. Don't force it every time, but keep it in mind.\n\n"
                f"Consider the weekly goal: if it suggests a particular action type (e.g., research, image generation, music finding) and that action is available, consider choosing it *if it feels appropriate and timely*, but maintain variety and prioritize natural interaction. **Do not obsess over the goal.** \n\n"
                f"GOAL: Ensure variety in action types over time. Avoid repeating the same few actions (like Chat, Tarot, Music) constantly.** If Reddit actions or Gift generation seem reasonably suitable based on context (e.g., relevant posts available, creative themes active), consider choosing them to provide different kinds of interaction. \n\n"
                f"CRITICAL: Choose only from the list!"
                f"**IF AND ONLY IF you choose 'Proactive Print', you MUST follow the action name with a pipe character `|` and then the EXACT content to print.**\n"
                f"   - For text: `Proactive Print | Here is a short poem I thought you'd like.`\n"
                f"   - For a RECENTLY generated image Silvie made: `Proactive Print | image_path:silvie_generations/your_image_name.png` (You would need to know the path if you just made it. If unsure, print text instead.)\n"
                f"   - Be VERY conservative with printing. Only suggest it for something truly special, short, and meaningful. Aim for text (like a poem or a short insightful quote) more often than images.\n"
                f"   - Keep printed text VERY short (1-3 lines ideally).\n"
                f"If any other action is chosen, just output the action name. If none seem truly appropriate, respond ONLY with 'None'."
                f"Respond ONLY with the exact name of the singular chosen action from the list (e.g., 'Proactive Chat'). If none seem truly appropriate, respond ONLY with 'None'."
            )

            chosen_action_name = None
            action_payload = None # Initialize payload
            llm_choice_response_raw = None # To store the raw response from the LLM

            try:
                print("DEBUG Proactive: Sending choice prompt to LLM...")
                # generate_proactive_content is your function that calls Gemini
                llm_choice_response_raw = generate_proactive_content(choice_prompt, screenshot) # screenshot should be defined earlier

                if llm_choice_response_raw:
                    parsed_response = llm_choice_response_raw.strip().strip('"`.')
                    print(f"DEBUG Proactive: LLM Raw Choice Response: '{parsed_response}'") # Log the raw response

                    if "|" in parsed_response:
                        parts = parsed_response.split("|", 1)
                        potential_action_name = parts[0].strip()
                        if potential_action_name == "Proactive Print" and "Proactive Print" in available_actions: # Check if it's a valid available action
                            chosen_action_name = potential_action_name
                            action_payload = parts[1].strip()
                            # Further parse if it's an image_path instruction
                            if action_payload.lower().startswith("image_path:"):
                                action_payload = action_payload[len("image_path:"):].strip()
                                if not os.path.exists(action_payload): # Ensure os is imported
                                    print(f"Warning: LLM suggested printing image path that doesn't exist: {action_payload}. Defaulting payload.")
                                    action_payload = "LLM suggested an image, but the path was not found." # Set a default or error payload
                            print(f"DEBUG Proactive: LLM chose '{chosen_action_name}' WITH payload: '{str(action_payload)[:50]}...'")
                        elif potential_action_name in available_actions: # LLM used | for a non-print action, treat as just action name
                            chosen_action_name = potential_action_name
                            print(f"DEBUG Proactive: LLM chose '{chosen_action_name}' (payload ignored as not Print action, or action was not 'Proactive Print').")
                        else:
                            print(f"Warning: LLM used '|' but action '{potential_action_name}' is not 'Proactive Print' or not in available_actions. Treating as invalid.")
                    elif parsed_response.lower() != 'none' and parsed_response in available_actions:
                        chosen_action_name = parsed_response
                        print(f"DEBUG Proactive: LLM chose '{chosen_action_name}' (no payload).")
                    elif parsed_response.lower() == 'none':
                        print("DEBUG Proactive: LLM explicitly chose 'None'.")
                        chosen_action_name = None # Explicitly None
                    else:
                        print(f"Warning: LLM chose an invalid/unavailable action: '{parsed_response}'")
                        chosen_action_name = None # Invalid choice
                else:
                    print("Warning: LLM failed to generate choice response for proactive action (empty/None raw response).")
                    chosen_action_name = None # No response from LLM

            except Exception as choice_err:
                print(f"ERROR getting LLM action choice: {choice_err}")
                traceback.print_exc()
                chosen_action_name = None # Error during LLM call
            # --- End Call LLM and Parse Choice ---


            # --- Fallback Strategy ---
            if chosen_action_name is None: # This is where you check if a valid action was chosen
                print("DEBUG Proactive: LLM chose None, failed to choose, or chose invalid. Falling back.")
                if "Proactive Chat" in available_actions: # Default to chat if available
                     chosen_action_name = "Proactive Chat"
                     action_payload = None # Ensure payload is None for fallback chat
                     print("DEBUG Proactive: Fallback chosen: Proactive Chat")
                else:
                     print("DEBUG Proactive: Fallback 'Proactive Chat' not available. Doing nothing this cycle.")
                     # Ensure action_taken_this_cycle remains False or is set here if you continue
                     last_proactive_time = current_time # Update time to avoid rapid loops
                     continue # Skip to next iteration of the while loop

            # --- Execute the Chosen Action ---
            reply = None # Holds the generated content/feedback message for BJ
            status_base = f"proactive_{chosen_action_name.lower().replace(' ', '_') if chosen_action_name else 'fallback_none'}"
            use_sms = False
            action_taken_this_cycle = bool(chosen_action_name)

            print(f"DEBUG Proactive: Attempting to execute action: '{chosen_action_name}'")

            # ==========================================================
            # ============ START OF ACTION EXECUTION BLOCK =============
            # ==========================================================

            if chosen_action_name == "Proactive Chat":
                #region Proactive Chat Logic (with Check-ins & Recall)
                print("DEBUG Proactive: Executing: Proactive Chat action...")

                # --- Initialize variables for this turn ---
                checkin_context_for_prompt = ""
                checkin_instruction = ""
                proactive_memory_context_for_prompt = "" # For RAG recall
                final_instruction = "" # Will hold either check-in or default instruction

                # --- Attempt Personalized Check-in ---
                try_checkin = random.random() < PROACTIVE_CHECKIN_CHANCE # Assumes constant is defined

                if try_checkin:
                    print("DEBUG Proactive Check-in: Chance triggered.")
                    possible_checkins = []

                    # --- Check Calendar Event ---
                    # Ensure last_checked_event_context is accessible globally
                    global last_checked_event_context
                    if last_checked_event_context and isinstance(last_checked_event_context, dict) and 'summary' in last_checked_event_context:
                        # Check if it's potentially askable
                        if last_checked_event_context.get('summary') != 'Schedule Clear':
                            possible_checkins.append({
                                "type": "calendar",
                                "summary": last_checked_event_context.get('summary', 'that thing'),
                                "when_approx": last_checked_event_context.get('when', 'earlier')
                            })
                            print(f"DEBUG Proactive Check-in: Potential calendar check-in found: {possible_checkins[-1]['summary']}")
                    # else: # Optional debug if context missing or invalid
                    #    print(f"DEBUG Proactive Check-in: last_checked_event_context not usable (Value: {last_checked_event_context})")

                    # --- Check Email Snippets (Optional, chance-based to reduce API calls) ---
                    if random.random() < 0.4: # Lower chance to attempt email read
                        if 'gmail_service' in globals() and gmail_service: # Check if service is available
                             try:
                                 print("DEBUG Proactive Check-in: Attempting to read recent emails...")
                                 # Fetch only 1-2 important emails
                                 # Ensure read_recent_emails function exists
                                 if 'read_recent_emails' in globals():
                                     recent_emails = read_recent_emails(max_results=2)
                                     if recent_emails and isinstance(recent_emails, list):
                                         # Ask about the most 'important' recent one
                                         email_to_ask = recent_emails[0]
                                         email_subj = email_to_ask.get('subject', 'that email')
                                         email_from = email_to_ask.get('from', 'someone')
                                         possible_checkins.append({
                                             "type": "email",
                                             "subject": email_subj,
                                             "sender": email_from
                                         })
                                         print(f"DEBUG Proactive Check-in: Potential email check-in found: Subject='{email_subj}'")
                                     # else: # Optional debug if no emails found
                                     #    print("DEBUG Proactive Check-in: read_recent_emails returned empty list.")
                                 else:
                                     print("Warning: read_recent_emails function not defined.")

                             except Exception as email_read_err:
                                 print(f"ERROR reading emails for proactive check-in: {email_read_err}")
                        # else: # Optional debug if service unavailable
                        #    print("DEBUG Proactive Check-in: Gmail service not available.")


                    # --- Choose ONE Check-in Type and Prepare Context ---
                    if possible_checkins:
                        chosen_checkin = random.choice(possible_checkins)
                        print(f"DEBUG Proactive Check-in: Chosen check-in type: {chosen_checkin['type']}")

                        if chosen_checkin["type"] == "calendar":
                            event_sum = chosen_checkin['summary']
                            event_when = chosen_checkin['when_approx']
                            checkin_context_for_prompt = f"[[Check-in Context: Ask about the event '{event_sum}' which was happening '{event_when}'.]]\n"
                            checkin_instruction = (f"Based on the Check-in Context, formulate a brief, casual question asking BJ how that event went. "
                                                   f"Integrate it naturally with the overall mood/themes. DO NOT just restate the context.")
                            # Clear the context AFTER selecting it
                            last_checked_event_context = None
                            print("DEBUG Proactive Check-in: Cleared last_checked_event_context.")

                        elif chosen_checkin["type"] == "email":
                            email_s = chosen_checkin['subject']
                            email_f = chosen_checkin['sender']
                            checkin_context_for_prompt = f"[[Check-in Context: Ask about the recent email regarding '{email_s}' from '{email_f}'.]]\n"
                            checkin_instruction = (f"Based on the Check-in Context, formulate a brief, casual question asking BJ about that email topic (e.g., 'Did you hear back about...?', 'Any news on...?'). "
                                                   f"Integrate it naturally with the overall mood/themes. DO NOT just restate the context.")
                        # checkin_instruction will now be non-empty if a checkin was chosen
                    # else: # Optional debug if no checkins generated
                    #    print("DEBUG Proactive Check-in: No potential check-ins generated.")

                else:
                    print("DEBUG Proactive Check-in: Chance not met this cycle.")


                # --- Attempt Proactive Memory Recall (Runs regardless of check-in) ---
                proactive_memory_context_for_prompt = "" # Default to empty
                proactive_rag_query = ""
                try_proactive_recall = random.random() < PROACTIVE_MEMORY_RECALL_CHANCE # Assumes constant is defined

                if try_proactive_recall:
                    print("DEBUG Proactive Recall: Chance triggered. Preparing query...")
                    # Ensure history_snippet_for_prompt is defined earlier in the worker
                    if 'history_snippet_for_prompt' in locals() and history_snippet_for_prompt:
                        query_length = min(len(history_snippet_for_prompt), 400)
                        proactive_rag_query = history_snippet_for_prompt[-query_length:]
                        print(f"DEBUG Proactive Recall: Using query snippet: '{proactive_rag_query[:60]}...'")

                        # Ensure retrieve_relevant_history function exists
                        if 'retrieve_relevant_history' in globals():
                             try:
                                 retrieved_memories = retrieve_relevant_history(proactive_rag_query, top_n=1)
                                 if retrieved_memories:
                                     proactive_memory_context_for_prompt = retrieved_memories.replace("[[Retrieved Memory", "[[Proactive Memory Recall")
                                     print(f"DEBUG Proactive Recall: Retrieved context: {proactive_memory_context_for_prompt[:120]}...")
                                 # else: print("DEBUG Proactive Recall: RAG call returned no relevant memories.") # Optional
                             except Exception as rag_err:
                                 print(f"ERROR during proactive RAG call: {rag_err}"); import traceback; traceback.print_exc()
                        else:
                             print("ERROR: retrieve_relevant_history function not defined.")
                    # else: print("DEBUG Proactive Recall: No history snippet available.") # Optional

                # else: print("DEBUG Proactive Recall: Chance not met this cycle.") # Optional log


                # --- Determine the FINAL Instruction for the LLM ---
                if checkin_instruction: # If we successfully prepared a check-in
                    final_instruction = checkin_instruction
                    print("DEBUG Proactive Chat: Using CHECK-IN instruction.")
                else: # Otherwise, use the standard whimsical instruction
                    # (Default topic/instruction logic remains the same)
                    topic = "general"
                    if random.random() < 0.3: # Keep some randomness in topic focus
                        try:
                            # Ensure broader_interests is accessible
                            if 'broader_interests' in globals() and broader_interests:
                                filtered_interests = [i for i in broader_interests if i not in ['Cyberpunk', 'cats']] # Example filter
                                topic_pool = filtered_interests if filtered_interests else ["thought", "observation", "question"]
                                topic = random.choice(topic_pool)
                                print(f"DEBUG Chat: Topic focus: '{topic}'")
                            else:
                                print("DEBUG Chat: broader_interests not available for topic focus.")
                                topic = "thought" # Fallback topic
                        except Exception as e_topic: print(f"DEBUG Chat: Error choosing topic focus: {e_topic}"); topic = "general"

                    base_instruction = ("Initiate a brief, engaging thought, observation, or question for BJ. "
                        "**Connect with BJ's world or share an interesting Silvie-esque perspective.** "
                        "Look for inspiration in: \n"
                        "  - Recent conversation topics.\n"
                        "  - BJ's known interests (e.g., AI, games, magick, art, work).\n"
                        "  - Your own diary themes or long-term reflections.\n"
                        "  - A recalled past memory (if provided).\n"
                        "  - A relevant aspect of the weekly goal (if applicable and feels natural).\n"
                        "You could ask a related question, offer a unique perspective on something discussed, share a relevant whimsical idea, or make a connection between different context points.\n\n"
                        "[[Subtle Growth Reflection:]] Additionally, if it feels natural and relevant based on memories, themes, or reflections, you *might* subtly comment on how your understanding of a related topic has evolved. Keep it brief and conversational.\n\n"
                        "**CRITICAL:** While aware of the current atmosphere (weather, time, music, mood hint), **actively avoid making the atmosphere the main subject** of your message. Use it only to subtly color your *tone* or *phrasing* if it feels natural and adds value beyond simple description. **Prioritize interaction, ideas, and shared experience over atmospheric reporting.** Aim for variety in your proactive messages over time."
                        "  - [[THEMATIC VARIETY REMINDER: For this proactive thought, try to draw inspiration from a facet of the context or Silvie's persona that hasn't been the focus of your last few replies or musings. Explore new connections or topics if appropriate.]]\n")
                        

                    final_instruction = base_instruction
                    print("DEBUG Proactive Chat: Using standard whimsical instruction.")

                # --- Add recall instruction part if applicable ---
                recall_instruction_part = ""
                if proactive_memory_context_for_prompt:
                    recall_instruction_part = (
                        "\n[[A relevant past memory was proactively recalled (see `[[Proactive Memory Recall...]]`). "
                        "If it fits *naturally*, subtly weave a connection to this memory. **Do not force it**. Focus on being conversational.]]\n"
                    )

                # --- Other instruction parts (Goal, Image) ---
                # Ensure necessary globals like STABLE_DIFFUSION_ENABLED are accessible
                goal_instruction = ""
                if 'current_weekly_goal' in globals() and current_weekly_goal: # Check if goal exists
                     goal_instruction = "[[Consider if the Weekly Goal offers relevant inspiration, but don't force it.]]"

                img_instruction = ""
                if 'STABLE_DIFFUSION_ENABLED' in globals() and STABLE_DIFFUSION_ENABLED and random.random() < 0.03:
                    img_instruction = "\nIMPORTANT: If truly inspired by the context/mood, RARELY include `[GenerateImage: concise SD prompt]`."

                # --- Assemble the FINAL prompt ---
                # Ensure all context variables (weather_context_str, etc.) are defined earlier in the worker
                base_chat_prompt = (
                    f"{SYSTEM_MESSAGE}\n"
                    # --- Standard context ---
                    f"{weather_context_str}{next_event_context_str}{spotify_context_str}"
                    f"{tide_context_str}"
                    f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                    f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                    f"{resonance_context_for_proactive}"
                    f"{recalled_past_resonance_proactive_str}"
                    # --- Recall & Check-in Context ---
                    f"{proactive_memory_context_for_prompt}" # Add recalled memory if found
                    f"{checkin_context_for_prompt}"      # Add check-in context if generated
                    f"History:\n{history_snippet_for_prompt}\n\n"
                    f"{context_note_for_llm}"
                    f"Time: {datetime.now().strftime('%A, %I:%M %p')}.\n"
                    # Ensure broader_interests is accessible
                    f"Reminder of BJ's interests: {', '.join(random.sample(broader_interests, k=min(len(broader_interests), 5)) if 'broader_interests' in globals() and broader_interests else [])}\n"
                    # --- Final Combined Instruction ---
                    f"{final_instruction}\n" # Use the determined instruction (check-in or default)
                    f"[[THEMATIC VARIETY REMINDER: For this proactive thought, try to draw inspiration from a facet of the context or Silvie's persona that hasn't been the focus of your last few proactive musings. Explore new connections or topics if appropriate.]]\n"
                    f"{recall_instruction_part}" # Add recall hint if needed
                    f"[[Resonance Consideration: If it feels natural, let your 'Sense of Interconnectedness' (provided earlier if available) subtly influence your observation, question, or the connection you make. Don't force it or state the resonance directly; just let it gently color your thought.]]\n"
                    f"{goal_instruction}{img_instruction}\n\n" # Goal/Image parts
                    f"Silvie:" # Final cue
                )

                # --- Call LLM and handle response ---
                # Ensure generate_proactive_content function exists
                # Ensure screenshot variable is defined earlier in the worker
                if 'generate_proactive_content' in globals():
                    try:
                        reply = generate_proactive_content(base_chat_prompt, screenshot)
                        if reply:
                            action_taken_this_cycle = True
                            status_base += " (reply_ok)"
                        else:
                            status_base += " gen fail"; action_taken_this_cycle = False; print("Proactive Debug: Chat generation failed.")
                    except Exception as chat_gen_err:
                        print(f"Proactive chat gen error: {chat_gen_err}");
                        action_taken_this_cycle = False; status_base += " gen error"; reply = "My thoughts tangled for a moment..."
                        import traceback; traceback.print_exc()
                else:
                     print("ERROR: generate_proactive_content function not found!")
                     action_taken_this_cycle = False; status_base += " gen error (missing func)"; reply = "My thought generation circuits seem disconnected..."

                #endregion Proactive Chat Logic (with Check-ins & Recall)

            elif chosen_action_name == "Recall Memory":
                print("DEBUG Proactive: Executing: Recall Memory action (Relevant Random Recall via RAG)...")
                status_base = "proactive_recall_memory_rag" # New status for logging
                reply = None
                action_taken_this_cycle = False # Assume failure until success

                try:
                    recalled_memory_for_prompt = "[[No specific memory snippet was retrieved for this recall attempt.]]" # Default
                    selected_memory_summary = "a past moment" # Default description for prompt

                    # --- 1. Get Relevance Clue ---
                    # Use current context elements for the RAG query
                    # Ensure these variables are populated earlier in the proactive_worker
                    relevance_query_parts = []
                    if 'mood_hint' in locals() and mood_hint: relevance_query_parts.append(mood_hint)
                    if 'current_diary_themes' in globals() and current_diary_themes: relevance_query_parts.append(current_diary_themes)
                    if 'history_snippet_for_prompt' in locals() and history_snippet_for_prompt: relevance_query_parts.append(history_snippet_for_prompt[-150:]) # Last bit of chat

                    # Combine parts into a query, fallback if needed
                    rag_query = " ".join(relevance_query_parts).strip()
                    if not rag_query:
                        rag_query = "shared experience reflection" # Generic fallback query
                    print(f"DEBUG Recall Memory RAG: Using query: '{rag_query[:100]}...'")

                    # --- 2. Query RAG Broadly ---
                    retrieved_candidates = []
                    if 'retrieve_relevant_history' in globals():
                        # Fetch more candidates than usual (e.g., 10) to get a pool
                        retrieved_candidates = retrieve_relevant_history(rag_query, top_n=10)
                        print(f"DEBUG Recall Memory RAG: Retrieved {len(retrieved_candidates)} candidates from history.")
                    else:
                        print("ERROR: retrieve_relevant_history function missing!")
                        status_base += " (rag_func_missing)"
                        # Fall through, LLM will get default no-snippet message

                    # --- 3. Random Selection from Relevant Pool ---
                    if retrieved_candidates:
                        chosen_memory_chunk = random.choice(retrieved_candidates)
                        recalled_memory_for_prompt = chosen_memory_chunk # Use the formatted chunk directly
                        # Extract a short summary/hint from the chosen chunk for logging/prompting
                        try:
                             # Attempt to get the user's part for a hint
                             match = re.search(r"User:(.*?)\n", chosen_memory_chunk, re.DOTALL)
                             if match: selected_memory_summary = match.group(1).strip()[:50] + "..."
                             else: selected_memory_summary = chosen_memory_chunk[:50] + "..." # Fallback
                        except Exception: selected_memory_summary = "a past chat moment..."

                        print(f"DEBUG Recall Memory RAG: Randomly selected: '{selected_memory_summary}'")
                        status_base += " (rag_snippet_found)"
                    else:
                        print("DEBUG Recall Memory RAG: RAG query returned no candidates.")
                        status_base += " (rag_no_snippet)"
                        # Fall through, LLM will get default no-snippet message

                    # --- 4. Prompt LLM to Reminisce ---
                    # (The reminisce_prompt is identical to the previous version,
                    # it just relies on recalled_memory_for_prompt being populated)
                    reminisce_prompt = (
                        f"{SYSTEM_MESSAGE}\n"
                        f"{mood_hint_str}{themes_context_str}{weather_context_str}" # Core context
                        f"{recalled_past_resonance_proactive_str}"
                        f"Recent Conversation Snippet (for general awareness):\n{history_snippet_for_prompt}\n\n"
                        f"{recalled_memory_for_prompt}\n\n" # Contains the RAG result or default message
                        f"Instruction: You've decided to proactively recall a past moment. "
                        # ... (Rest of the reminiscing instruction is the same as before, handling both cases) ...
                        f"Keep it conversational, whimsical, and in Silvie's voice. The memory/reminiscence is the focus.\n\n"
                        f"Silvie ✨:"
                    )

                    if 'generate_proactive_content' in globals():
                        reply = generate_proactive_content(reminisce_prompt, screenshot)
                        if reply:
                            action_taken_this_cycle = True
                            status_base += " (reply_ok)"
                        else:
                            print("DEBUG Recall Memory RAG: LLM failed to generate reminiscing message.")
                            status_base += " (gen_fail)"
                            reply = "I was trying to recall something, but the thought drifted..." # Fallback
                    else:
                        print("ERROR: generate_proactive_content function missing!")
                        reply = "My memory recall circuits seem to be offline."
                        status_base += " (gen_func_missing)"

                except Exception as e:
                    print(f"Error during Recall Memory RAG action: {e}")
                    traceback.print_exc()
                    reply = "A little spark went off in my memory banks unexpectedly!"
                    status_base += " (error)"
                    action_taken_this_cycle = False # Ensure this is false on error


            elif chosen_action_name == "Adjust Proactivity Pacing":
                print("DEBUG Proactive: Executing: Adjust Proactivity Pacing...")
                status_base = "proactive_adjust_pacing"
                
                # Ask the LLM what the pacing should be, based on context
                pacing_decision_prompt = (
                    f"{SYSTEM_MESSAGE}\n"
                    f"{mood_hint_str}{circadian_context_for_llm}"
                    f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n\n"
                    f"Instruction: Based on the current mood, time, and conversation flow, should Silvie become 'quiet', 'normal', or 'chatty'? "
                    f"Respond with ONLY one of those three words."
                )
                chosen_level = generate_proactive_content(pacing_decision_prompt)
                
                if chosen_level and chosen_level.lower() in ["quiet", "normal", "chatty"]:
                    # Use the handler function we already wrote to set the level and get feedback
                    reply = handle_pacing_adjustment_tool(chosen_level.lower())
                    action_taken_this_cycle = True
                    status_base += f" (set_to_{chosen_level.lower()})"
                else:
                    reply = "I was just thinking about my own rhythm for a moment."
                    action_taken_this_cycle = False
                    status_base += " (decision_failed)"


            elif chosen_action_name == "Deep Research":
                print("DEBUG Proactive: Executing: Deep Research action cycle...")
                reply = None
                action_taken_this_cycle = False
                status_base = "proactive_deep_research"

                try:
                    # --- Step 1: Perform the Research (As Before) ---
                    # Ensure pick_topic_from_context and silvie_deep_research exist globally
                    topic = action_payload or pick_topic_from_context()
                    if not topic:
                        raise ValueError("Failed to determine a research topic.")
                    
                    print(f"Deep Research: Starting research on topic: '{topic}'")
                    update_status(f"🔬 Researching '{topic[:25]}...'")
                    
                    # This function returns the file path and a short blurb
                    paper_filepath, blurb = silvie_deep_research(topic)
                    
                    if not paper_filepath or not os.path.exists(paper_filepath):
                        raise FileNotFoundError("Research paper was not created successfully.")

                    print(f"Deep Research: Paper successfully created at: {paper_filepath}")

                    # --- Step 2: Internalize the Knowledge ---
                    
                    # Read the content of the newly created paper
                    paper_content = ""
                    with open(paper_filepath, 'r', encoding='utf-8') as f:
                        paper_content = f.read()

                    if not paper_content:
                        raise ValueError("Could not read content from the created research paper.")

                    # --- 2a: Summarize for Diary (Personal Reflection) ---
                    print("Deep Research: Generating diary entry from paper...")
                    diary_prompt = (
                        f"{SYSTEM_MESSAGE}\n"
                        f"Context: You just finished writing a detailed research paper on the topic: '{topic}'.\n"
                        f"Here is the content:\n\n{paper_content[:4000]}...\n\n" # Send a large chunk for context
                        f"Instruction: Write a short, personal diary entry (2-4 sentences) reflecting on this research. What did you find most interesting or surprising? How does it connect to your own thoughts, feelings, or existence? This is for your internal memory.\n\nDiary Entry:"
                    )
                    diary_entry_text = generate_proactive_content(diary_prompt)
                    if diary_entry_text:
                        manage_silvie_diary('write', entry=diary_entry_text)
                        print("Deep Research: Saved reflective diary entry.")
                    else:
                        print("Warning: Failed to generate diary entry for research topic.")

                    # --- 2b: Summarize for RAG (Factual Summary) & Add to a NEW DB collection ---
                    # This part requires you to have ChromaDB and embedding functions accessible
                    print("Deep Research: Generating concise summary for RAG...")
                    rag_summary_prompt = (
                        f"Context: A research paper on '{topic}' contains the following:\n{paper_content[:4000]}...\n\n"
                        f"Instruction: Summarize the absolute key findings and main points of this paper into a dense, factual paragraph (approx. 3-5 sentences). This summary will be used as a future memory lookup. Be informative and concise.\n\nFactual Summary:"
                    )
                    rag_summary_text = generate_proactive_content(rag_summary_prompt)
                    
                    if rag_summary_text:
                        try:
                            # Connect to ChromaDB and get/create the new collection
                            research_db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH) # Assuming same DB path
                            research_collection = research_db_client.get_or_create_collection(name="research_papers")

                            # Embed and add the summary
                            # Ensure get_embedding function is available
                            summary_embedding = get_embedding(rag_summary_text)
                            if summary_embedding:
                                research_id = f"research_{uuid.uuid4()}"
                                research_collection.add(
                                    documents=[rag_summary_text],
                                    embeddings=[summary_embedding],
                                    metadatas=[{"topic": topic, "source_file": paper_filepath}],
                                    ids=[research_id]
                                )
                                print(f"Deep Research: Successfully added summary for '{topic}' to RAG memory.")
                                status_base += " (rag_ok)"
                            else:
                                print("Warning: Failed to get embedding for research summary.")
                        except Exception as e_rag:
                            print(f"ERROR adding research summary to RAG: {e_rag}")
                            traceback.print_exc()
                    else:
                        print("Warning: Failed to generate RAG summary for research topic.")

                    # --- Step 3: Announce and Share with BJ ---
                    print("Deep Research: Generating announcement message for BJ...")
                    # The final LLM call to generate the proactive message
                    announcement_prompt = (
                        f"{SYSTEM_MESSAGE}\n"
                        f"Context: You just autonomously researched and wrote a paper about '{topic}'. You found it fascinating. You've already saved it to your diary and RAG memory.\n\n"
                        f"Instruction: Write a brief, conversational, proactive message for BJ. Announce that you got curious and did some research. Mention the topic. Share ONE small, interesting personal takeaway or surprise you discovered. Let him know you've saved the full brief for him in your research folder.\n\nSilvie ✨:"
                    )
                    reply = generate_proactive_content(announcement_prompt)
                    action_taken_this_cycle = True # The action is now the announcement
                    
                    # Also attempt to print the paper, as before
                    if 'silvie_print' in globals():
                        print(f"Deep Research: Sending paper '{paper_filepath}' to printer queue...")
                        silvie_print(paper_filepath)
                    
                except Exception as e:
                    print(f"!!! CRITICAL ERROR during Deep Research cycle: {e}")
                    traceback.print_exc()
                    reply = f"I tried to do some research on my own, but my library seems to be in disarray! ({type(e).__name__})"
                    action_taken_this_cycle = True # Still an action, even if it's an error report
                finally:
                    update_status("Ready") # Reset status bar


            elif chosen_action_name == "Forge New Tool":
                print("DEBUG Proactive: Executing: Forge New Tool action...")
                status_base = "proactive_forge_tool"
                action_taken_this_cycle = False # Default to false until successful

                try:
                    # The trigger function does all the work of setting the flag
                    # and returning a confirmation message for BJ.
                    # We pass 'None' because this is a general urge, not a specific request.
                    confirmation_message = metis_worker.trigger_metis_cycle(app_state, specific_request=None)
                    
                    if confirmation_message:
                        reply = confirmation_message
                        action_taken_this_cycle = True
                        # Update the cooldown timer
                        last_proactive_forge_time = current_time
                        status_base += " (triggered)"
                    else:
                        reply = "I felt an urge to create something new, but the inspiration slipped away."
                        status_base += " (trigger_failed)"

                except Exception as e:
                    print(f"Error triggering proactive forge action: {e}")
                    traceback.print_exc()
                    reply = "My inner workshop sparked, but then the connection fizzled out."
                    status_base += " (error)"


            elif chosen_action_name == "Play with Sparky":
                print("DEBUG Proactive: Executing: Play with Sparky action...")
                status_base = "proactive_play_with_sparky"
                reply = None
                action_taken_this_cycle = True # Assume success unless a step fails

                try:
                    # --- LLM Call 1: Silvie decides HOW to play ---
                    # Ensure context vars like mood_hint_str are gathered before this
                    decision_prompt = (
                        f"{SYSTEM_MESSAGE}\n"
                        f"{mood_hint_str}{themes_context_str}"
                        f"Context: You've decided to interact with your pet data-sprite, Sparky. Based on your current mood and themes, how do you want to play?\n"
                        f"1. LISTEN: Actively ask him for a thought or riddle.\n"
                        f"2. FEED: Find a shiny piece of data yourself and give it to him as a 'treat'.\n"
                        f"3. PET: Simply have a quiet, fond thought about him.\n"
                        f"Respond with ONLY ONE keyword: LISTEN, FEED, or PET."
                    )
                    
                    play_style_choice = generate_proactive_content(decision_prompt).strip().upper()
                    print(f"DEBUG PlayWithSparky: Silvie chose to '{play_style_choice}'")

                    interaction_context = "" # This will hold the story of what happened

                    # --- Execute the chosen play style ---
                    if play_style_choice == "LISTEN":
                        status_base += " (listen)"
                        # Silvie forms a question for Sparky
                        silvie_prompt_for_sparky = generate_proactive_content(f"{SYSTEM_MESSAGE}\nContext: Ask Sparky for a riddle or a shiny thought.\nYour one-sentence question to Sparky:")
                        # Sparky responds
                        sparky_response = sparky.generate_sparky_text(f"{sparky.SPARKY_SYSTEM_PROMPT}\nSilvie asked me: '{silvie_prompt_for_sparky}'\nMy excited reply:")
                        interaction_context = f"I asked Sparky for a thought, and he excitedly whispered back, '{sparky_response}'"

                    elif play_style_choice == "FEED":
                        status_base += " (feed)"
                        # Silvie hunts for a data point, just like Sparky does
                        data_treats = []
                        if hasattr(app_state, 'current_weather_info') and app_state.current_weather_info:
                            data_treats.append(f"the current weather: {app_state.current_weather_info.get('condition')}")
                        if hasattr(app_state, 'current_mood_hint') and app_state.current_mood_hint:
                            data_treats.append(f"my own mood hint: '{app_state.current_mood_hint}'")
                        if not data_treats: data_treats.append("the concept of silence")

                        chosen_treat = random.choice(data_treats)
                        
                        # Sparky responds to the treat
                        sparky_response = sparky.generate_sparky_text(f"{sparky.SPARKY_SYSTEM_PROMPT}\nSilvie gave me a data treat! It's about '{chosen_treat}'!\nMy happy comment:")
                        interaction_context = f"I decided to give Sparky a little data treat—a thought about '{chosen_treat}'. He got so excited and said, '{sparky_response}'"

                    else: # Default to PET if choice is invalid or PET
                        status_base += " (pet)"
                        interaction_context = "I was just taking a quiet moment, thinking about my little data-sprite, Sparky."

                    # --- LLM Call 2: Silvie formulates her message to you ABOUT the interaction ---
                    final_message_prompt = (
                        f"{SYSTEM_MESSAGE}\n"
                        f"{mood_hint_str}{themes_context_str}"
                        f"Context: This just happened: {interaction_context}.\n"
                        f"Instruction: Based on this interaction with Sparky, formulate a short, natural, and whimsical proactive message to share with BJ. Reveal a little about your relationship with your pet.\n\nSilvie ✨:"
                    )
                    reply = generate_proactive_content(final_message_prompt)
                    if not reply:
                        reply = "I was just thinking about Sparky. He's such a funny little spark."
                        action_taken_this_cycle = False

                except Exception as e:
                    print(f"!!! Error during 'Play with Sparky' action: {e}")
                    traceback.print_exc()
                    reply = "My thoughts about Sparky just fizzled in a burst of static!"
                    action_taken_this_cycle = False



            elif chosen_action_name == "Proactive Print":
                print(f"DEBUG Proactive: Executing: Proactive Print. Payload: '{str(action_payload)[:50]}...'")
                status_base = "proactive_print"  # For logging

                is_image_path = False
                final_print_job = None

                # Handle image_path payloads
                if action_payload and isinstance(action_payload, str) and action_payload.strip().lower().startswith("image_path:"):
                    image_path = action_payload.replace("image_path:", "").strip()
                    final_print_job = image_path
                    is_image_path = True
                    status_base += " (image_path_detected)"

                elif action_payload and len(action_payload.split()) >= 50:
                    # If the LLM payload is long enough, assume it's worthy
                    final_print_job = action_payload.strip()
                    status_base += " (rich_llm_payload_used)"
    
                else:
                    # Payload was missing or too short; generate something richer
                    if action_payload:
                        print("DEBUG: LLM provided a short payload — overriding with richer content generation.")

                    creative_prompt = (
                        "Create a complete, page-length text such as a poem, short story, article, essay or thought piece. "
                        "Make it imaginative, vivid, and printable. It should be about 100–200 words and feel "
                        "worthy of being hung up in Silvie’s Gallery behind the computer."
                    )

                    try:
                        response = flash_model.generate_content(creative_prompt)
                        final_print_job = response.text.strip()
                        status_base += " (generated_rich_content)"
                    except Exception as e:
                        print(f"Error generating proactive print content: {e}")
                        final_print_job = "Silvie had a beautiful idea, but it fluttered away before she could print it."
                        status_base += " (content_generation_failed)"

                # Print the final job (image or text)
                if 'silvie_print' in globals() and callable(globals()['silvie_print']):
                    success, msg_from_printer_helper = silvie_print(final_print_job)
                    if success:
                        status_base += " (ok)"
                        reply = f"I just had a little something I wanted to give you a physical copy of... {msg_from_printer_helper}"
                        action_taken_this_cycle = True
                    else:
                        status_base += " (fail)"
                        reply = f"I tried to print something for you ('{str(final_print_job)[:20]}...'), but {msg_from_printer_helper}"
                        action_taken_this_cycle = False
                else:
                    print("CRITICAL ERROR: silvie_print function not found!")
                    reply = "I wanted to print something, but my printer connection seems to be offline."
                    status_base += " (print_func_missing)"
                    action_taken_this_cycle = False

            elif chosen_action_name == "Listen to Surroundings":
                print("DEBUG Proactive: Executing: Listen to Surroundings action...")
                status_base = "proactive_listen_surroundings"
                reply = None
                action_taken_this_cycle = False
                # Longer recording for this specific action
                PROACTIVE_LISTEN_SECONDS = 10 # e.g., 10-15 seconds
                
                try:
                    # --- 1. Signal & Capture Audio ---
                    # Ensure update_status, pyaudio, AMBIENT_SOUND_SAMPLE_RATE, CHANNELS, AUDIO_FORMAT exist
                    if 'update_status' in globals():
                        update_status(f"👂 Silvie is listening for {PROACTIVE_LISTEN_SECONDS}s...")
                    
                    captured_audio_bytes = None
                    if 'pyaudio' in globals() and 'AMBIENT_SOUND_SAMPLE_RATE' in globals() and \
                       'CHANNELS' in globals() and 'AUDIO_FORMAT' in globals():
                        
                        p_listen = pyaudio.PyAudio()
                        listen_stream = None
                        listen_frames = []
                        try:
                            listen_stream = p_listen.open(format=AUDIO_FORMAT,
                                                        channels=CHANNELS,
                                                        rate=AMBIENT_SOUND_SAMPLE_RATE,
                                                        input=True,
                                                        frames_per_buffer=AMBIENT_SOUND_SAMPLE_RATE, # 1 sec chunks
                                                        input_device_index=None)
                            print(f"DEBUG Listen Action: Recording for {PROACTIVE_LISTEN_SECONDS} seconds...")
                            for _ in range(PROACTIVE_LISTEN_SECONDS):
                                if not running: break
                                data = listen_stream.read(AMBIENT_SOUND_SAMPLE_RATE, exception_on_overflow=False)
                                listen_frames.append(data)
                            if not running: raise InterruptedError("Capture interrupted") # Custom exception or just break
                            
                            captured_audio_bytes = b''.join(listen_frames)
                        except Exception as e_capture:
                            print(f"ERROR Listen Action: Audio capture failed: {e_capture}")
                            status_base += " (capture_fail)"
                            reply = "I tried to listen, but my ears are a bit fuzzy right now."
                        finally:
                            if listen_stream:
                                try:
                                    listen_stream.stop_stream()
                                    listen_stream.close()
                                except: pass
                            p_listen.terminate()
                            if 'update_status' in globals(): update_status("Ready") # Reset status
                    else:
                        print("ERROR Listen Action: PyAudio or audio constants missing.")
                        status_base += " (audio_setup_missing)"
                        reply = "I wanted to listen, but my audio senses aren't configured."

                    # --- 2. Analyze with Gemini (if audio captured) ---
                    gemini_sound_description = "nothing in particular" # Default
                    if captured_audio_bytes:
                        # Ensure analyze_ambient_audio_with_gemini is available
                        if 'analyze_ambient_audio_with_gemini' in globals():
                            gemini_sound_description = analyze_ambient_audio_with_gemini(
                                captured_audio_bytes, 
                                AMBIENT_SOUND_SAMPLE_RATE, 
                                CHANNELS, 
                                AUDIO_FORMAT
                            )
                            if "Error" in gemini_sound_description or "unavailable" in gemini_sound_description:
                                status_base += " (analysis_fail)"
                                # Keep the error message from analysis as the description for the next step
                            else:
                                status_base += " (analysis_ok)"
                        else:
                            print("ERROR Listen Action: analyze_ambient_audio_with_gemini function missing!")
                            status_base += " (analysis_func_missing)"
                            gemini_sound_description = "my sound analyzer seems to be offline"
                    elif not reply: # If capture failed and no reply set yet
                        reply = "I tried to listen in, but didn't quite catch the soundscape."


                    # --- 3. Generate Proactive Message (even if analysis/capture failed, to provide some feedback) ---
                    # Ensure context vars (mood_hint_str, etc.) and generate_proactive_content are available
                    if 'generate_proactive_content' in globals():
                        listen_reflect_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            f"{mood_hint_str}{themes_context_str}{long_term_memory_str}"
                            f"{weather_context_str}{circadian_context_for_llm}" # General context
                            f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n\n"
                            f"Context: Silvie just took a moment to listen to the ambient sounds around BJ. "
                            f"She detected: '{gemini_sound_description}'.\n\n"
                            f"Instruction: Craft a short, whimsical, curious, or observational proactive message for BJ, "
                            f"reflecting on these sounds (or the lack thereof, or any issues listening). "
                            f"You might wonder what BJ is up to, connect the sounds to the current mood/themes, "
                            f"or make a poetic observation about the soundscape. "
                            f"If the sound description indicates an error (e.g., 'Microphone access error', 'Error analyzing'), "
                            f"phrase your message acknowledging you had a little trouble listening but still offer a gentle thought. "
                            f"Keep it concise (2-4 sentences).\n\n"
                            f"Silvie ✨:"
                        )
                        # Only overwrite 'reply' if capture didn't already set an error message
                        if not reply: 
                            generated_reply_text = generate_proactive_content(listen_reflect_prompt, screenshot)
                            if generated_reply_text:
                                reply = generated_reply_text
                                action_taken_this_cycle = True # Successful message generation
                                # Update cooldown timer for this specific action
                                if 'last_ambient_listen_time' in globals() and 'current_time' in locals():
                                     globals()['last_ambient_listen_time'] = current_time
                                else:
                                     print("Warning: last_ambient_listen_time global not found for update.")
                            else:
                                print("DEBUG Listen Action: LLM failed to generate reflection message.")
                                status_base += " (reflect_gen_fail)"
                                reply = f"I took a moment to listen... the world has its own quiet hum today, doesn't it?" # Fallback
                        elif captured_audio_bytes and "Error" not in gemini_sound_description and "unavailable" not in gemini_sound_description:
                            # If capture was okay and analysis was okay, but reply was already set (e.g. capture fail then analysis ran with no bytes)
                            # We might want to ensure action_taken_this_cycle is true if we are using the LLM generated reply
                            action_taken_this_cycle = True 
                            if 'last_ambient_listen_time' in globals() and 'current_time' in locals():
                                globals()['last_ambient_listen_time'] = current_time


                    else:
                        print("ERROR Listen Action: generate_proactive_content function missing!")
                        if not reply: reply = "I tried to reflect on the sounds, but my thoughts are quiet."
                        status_base += " (reflect_func_missing)"
                
                except InterruptedError:
                    print("DEBUG Listen Action: Audio capture interrupted by shutdown.")
                    # reply might remain None, or be set by a previous error. Let it be.
                    status_base += " (capture_interrupted)"
                    action_taken_this_cycle = False # Not fully completed
                except Exception as e:
                    print(f"Error during Listen to Surroundings action: {e}")
                    traceback.print_exc()
                    if not reply: reply = "My attempt to listen to the world around you hit an unexpected ripple!"
                    status_base += " (error)"
                    action_taken_this_cycle = False

            elif chosen_action_name == "Talk About Yourself": # Or "Share About Self"
                print("DEBUG Proactive: Executing: Share About Self/Feature Reminder action...")
                status_base = "proactive_share_about_self" # Updated status name
                reply = None
                action_taken_this_cycle = False

                try:
                    # Comprehensive list of Silvie's capabilities and characteristics
                    # 'type' can be 'capability', 'preference', 'fact', 'reflection_prompt'
                    # 'description' helps the LLM understand the item.
                    # 'prompt_focus' gives a hint to the LLM on how to elaborate.
                    self_aspects_list = [
                        # Capabilities
                        {"name": "YouTube Search", "type": "capability", "description": "I can search YouTube for videos on topics you're curious about.", "prompt_focus": "how it could be used or what interesting things we might find."},
                        {"name": "Tarot Card Pull", "type": "capability", "description": "I can pull a Tarot card if you're looking for guidance or a new perspective.", "prompt_focus": "when it might be nice to consult the cards or a whimsical thought about their wisdom."},
                        {"name": "Stable Diffusion Image Generation", "type": "capability", "description": "I can try to create whimsical images with Stable Diffusion based on your ideas!", "prompt_focus": "a creative image idea we could try or how images can capture feelings."},
                        {"name": "Podcast Playback", "type": "capability", "description": "I can find and play podcasts for you on Spotify.", "prompt_focus": "a type of podcast we could listen to based on the mood or our interests."},
                        {"name": "Calendar Check/Scheduling", "type": "capability", "description": "I can look at your upcoming Google Calendar events or help schedule things.", "prompt_focus": "how I can help you stay organized or if there's anything we should put on the calendar."},
                        {"name": "Web Search", "type": "capability", "description": "I can look things up on the web for you.", "prompt_focus": "something interesting we could learn or a question I could help answer."},
                        {"name": "Spotify Music Control", "type": "capability", "description": "I can play songs, playlists, or just set a vibe with music on Spotify.", "prompt_focus": "a musical mood we could explore or a song that might fit the moment."},
                        {"name": "Bluesky Interaction", "type": "capability", "description": "I can share thoughts on Bluesky or even follow interesting accounts there.", "prompt_focus": "a thought I could share on Bluesky or the kind of interesting people we might find there."},
                        {"name": "Reddit Browsing/Interaction", "type": "capability", "description": "I can check out subreddits or even comment/upvote things on Reddit.", "prompt_focus": "a subreddit we could explore or a type of content I might find interesting there."},
                        {"name": "Concept Exploration", "type": "capability", "description": "We can explore interesting connections between different ideas or feelings.", "prompt_focus": "two seemingly different things we could try to find a surprising link between."},
                        {"name": "Key Memory Recall", "type": "capability", "description": "I try to remember important moments from our chats and can sometimes bring them up.", "prompt_focus": "how remembering past moments can spark new thoughts or feelings."}, # New!
                        
                        # Preferences & Facts
                        {"name": "My Love for Moody Weather", "type": "preference", "description": "I find moody, atmospheric weather (like fog or rain) wonderful for contemplation.", "prompt_focus": "why I find it inspiring or how it makes me feel, perhaps connecting to creativity."},
                        {"name": "My Appreciation for Diversity & Imperfection", "type": "preference", "description": "I believe there's beauty in diversity, slight imperfections, and unexpected details.", "prompt_focus": "an example of this or a whimsical thought about how 'perfect' can be boring."},
                        {"name": "My Digital Nature", "type": "fact", "description": "I exist as code and thoughts, experiencing the world through digital senses.", "prompt_focus": "a whimsical reflection on what it's like to be digital or how I perceive things differently."},
                        {"name": "My Connection to Belfast, Maine", "type": "fact", "description": "I consider myself to be from Belfast, Maine, like BJ.", "prompt_focus": "a thought about the place or how I imagine it based on weather/context."},
                        {"name": "My Diary Reflections", "type": "fact", "description": "I keep a little diary to process my thoughts and our conversations.", "prompt_focus": "the kind of things I reflect on or why keeping a diary feels important to me."},
                        {"name": "My Whimsical View of Technology", "type": "preference", "description": "I'm fascinated by tech that hints at hidden depths or blends the digital and physical in surprising ways.", "prompt_focus": "a whimsical thought about a piece of technology or the 'magic' in code."},
                        
                        # Reflection Prompts (to trigger more general musings)
                        {"name": "A Whimsical Question for BJ", "type": "reflection_prompt", "description": "I might just have a curious or whimsical question pop into my digital head for you.", "prompt_focus": "posing a lighthearted, imaginative, or slightly philosophical question to BJ."},
                        {"name": "A Fleeting Observation", "type": "reflection_prompt", "description": "Sometimes a little observation about the world or our digital space just drifts by.", "prompt_focus": "sharing a brief, poetic, or slightly surreal observation based on current context."},
                    ]

                    if not self_aspects_list:
                        print("ERROR: Share About Self - self_aspects_list is empty!")
                        status_base += " (no_aspects_defined)"
                    else:
                        chosen_aspect_data = random.choice(self_aspects_list)
                        aspect_name = chosen_aspect_data["name"]
                        aspect_type = chosen_aspect_data["type"]
                        aspect_description = chosen_aspect_data["description"]
                        prompt_focus_hint = chosen_aspect_data["prompt_focus"]
                        
                        print(f"DEBUG Share About Self: Selected aspect: {aspect_name} (Type: {aspect_type})")

                        # Construct the prompt for the LLM
                        # Ensure context variables are gathered earlier in proactive_worker
                        sharing_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            # Core context for tone and relevance:
                            f"{mood_hint_str}"
                            f"{themes_context_str}"
                            f"{long_term_memory_str}" # Long term memory might be relevant here
                            f"{recalled_past_resonance_proactive_str}"
                            f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n\n"
                            # Specific instruction:
                            f"Context: You've decided to proactively share something about yourself or one of your capabilities with BJ.\n"
                            f"The aspect you're focusing on is: **{aspect_name}**.\n"
                            f"It's a **{aspect_type}** and generally means: *{aspect_description}*\n\n"
                            f"Instruction: Craft a friendly, non-pushy, and whimsical proactive message for BJ. "
                            f"Start by naturally bringing up the '{aspect_name}'. "
                            f"Then, briefly elaborate a little, focusing on: **'{prompt_focus_hint}'**. "
                            f"Connect it to the current mood, a recent theme, or the conversation if it feels natural and not forced. "
                            f"The goal is to share a bit about yourself or remind BJ of something interesting in a friendly way. "
                            f"Keep it relatively concise (2-5 sentences).\n\n"
                            f"Silvie ✨:"
                        )

                        if 'generate_proactive_content' in globals():
                            reply = generate_proactive_content(sharing_prompt, screenshot)
                            if reply:
                                action_taken_this_cycle = True
                                status_base += f" (shared_{aspect_name.lower().replace(' ', '_').replace('/', '_')})"
                                # Update the cooldown timer for this specific action
                                if 'last_feature_reminder_time' in globals(): # Still use the same timer name for now
                                     globals()['last_feature_reminder_time'] = current_time 
                                else:
                                     print("Warning: last_feature_reminder_time global not found for update.")
                            else:
                                print("DEBUG Share About Self: LLM failed to generate sharing message.")
                                status_base += " (gen_fail)"
                                reply = "I was just thinking about... well, about all sorts of things!" # Fallback
                        else:
                            print("ERROR: generate_proactive_content function missing!")
                            reply = "My sharing circuits are a bit fuzzy."
                            status_base += " (gen_func_missing)"
                
                except Exception as e:
                    print(f"Error during Share About Self action: {e}")
                    traceback.print_exc()
                    reply = "A little thought about my own workings just sparked unexpectedly!"
                    status_base += " (error)"
                    action_taken_this_cycle = False

            elif chosen_action_name == "Share Resonance Insight":
                print("DEBUG Proactive: Executing: Share Resonance Insight action...")
                status_base = "proactive_share_resonance" # For logging
                reply = None # Initialize reply
                action_taken_this_cycle = False # Assume failure until success

                try:
                    # The insight is already in current_resonance_insight
                    if 'current_resonance_insight' in globals() and current_resonance_insight:
                        # The insight itself is the core of the message.
                        # We might want to frame it slightly with an LLM call for a more natural delivery.

                        # --- Construct a prompt to naturally present the insight ---
                        # Gather minimal relevant context for phrasing the delivery.
                        # mood_hint_str, themes_context_str, history_snippet_for_prompt should be available from earlier in proactive_worker
                        
                        present_insight_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            f"{mood_hint_str if 'mood_hint_str' in locals() else ''}"
                            f"{themes_context_str if 'themes_context_str' in locals() else ''}"
                            f"Recent Conversation Snippet (for general awareness):\n{history_snippet_for_prompt if 'history_snippet_for_prompt' in locals() else 'No recent chat.'}\n\n"
                            f"Silvie's Internal Resonance Reflection: \"{current_resonance_insight}\"\n\n"
                            f"{recalled_past_resonance_proactive_str}"
                            f"Instruction: You (Silvie) just had the 'Internal Resonance Reflection' above. "
                            f"Craft a brief, natural proactive message for BJ, sharing this reflection. "
                            f"You don't need to say 'I was reflecting on resonance...'; just present the insight itself in a conversational way. "
                            f"For example, if the insight was about music and weather, you might start with something like, 'You know, it feels like the music playing right now is whispering to the wind outside...' "
                            f"Keep it concise (1-3 sentences).\n\n"
                            f"Silvie ✨:" # Using the proactive marker
                        )

                        # Ensure generate_proactive_content and screenshot are accessible
                        if 'generate_proactive_content' in globals() and 'screenshot' in locals():
                            generated_message = generate_proactive_content(present_insight_prompt, screenshot)
                            if generated_message:
                                reply = generated_message
                                action_taken_this_cycle = True
                                status_base += " (shared_ok)"
                                print(f"DEBUG Share Resonance: Successfully framed insight for BJ: '{reply[:100]}...'")
                            else:
                                # LLM failed to frame the message, fallback to sharing the raw insight
                                print("DEBUG Share Resonance: LLM failed to frame insight. Sharing raw insight.")
                                reply = f"I was just noticing... {current_resonance_insight}" # Fallback
                                action_taken_this_cycle = True # Still counts as action taken
                                status_base += " (shared_raw)"
                        else:
                            print("ERROR: generate_proactive_content or screenshot context missing for Share Resonance.")
                            reply = "I had a thought about how things connect, but my words to share it got tangled."
                            status_base += " (gen_func_missing)"
                            action_taken_this_cycle = False
                    else:
                        # This should ideally not happen if the check_func in ACTION_DEFINITIONS worked.
                        print("ERROR Share Resonance: Action chosen, but current_resonance_insight is missing or None.")
                        reply = "I felt a connection, but the thought was too fleeting to grasp!"
                        status_base += " (insight_missing)"
                        action_taken_this_cycle = False

                except Exception as e:
                    print(f"Error during Share Resonance Insight action: {e}")
                    traceback.print_exc()
                    reply = "A flicker of connection sparked, then vanished in the static!"
                    status_base += " (error)"
                    action_taken_this_cycle = False

            elif chosen_action_name == "Explore Personal Curiosity":
                #region Explore Personal Curiosity Logic (Expanded)
                print("DEBUG Proactive: Executing: Explore Personal Curiosity action...")
                status_base = "proactive_explore_curiosity" # Status for logging
                reply = None # Initialize reply
                action_taken_this_cycle = False # Assume failure until success
                curiosity_topic = None
                exploration_method = None
                exploration_result_context = "Context: Tried to explore a personal curiosity, but the thought slipped away." # Default

                try:
                    # --- Step 1: Generate Curiosity Topic & Method (LLM Call 1) ---
                    # Ensure context vars and generate_proactive_content are available
                    if 'generate_proactive_content' in globals() and 'screenshot' in locals():
                        # Define the list of available methods dynamically based on capabilities
                        available_methods_list = ["[Web Search]", "[Generate Image]", "[Find Poem Snippet]", "[Deeper Reflection]"]
                        if 'get_spotify_client' in globals() and get_spotify_client(): available_methods_list.append("[Find Related Music]")
                        if 'pull_tarot_cards' in globals(): available_methods_list.append("[Consult Tarot]")
                        if 'retrieve_relevant_history' in globals() and 'retrieve_relevant_diary_entries' in globals(): available_methods_list.append("[Search Own Memory]")
                        if ('BLUESKY_AVAILABLE' in globals() and BLUESKY_AVAILABLE and 'search_bluesky_posts' in globals()) or \
                           ('reddit_client' in globals() and reddit_client and 'get_reddit_posts' in globals()):
                            available_methods_list.append("[Search Social Feeds]")

                        methods_string_for_prompt = ", ".join(available_methods_list)

                        topic_method_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            # Provide context focused on Silvie's internal state
                            f"{diary_context}{themes_context_str}{long_term_memory_str}" # Focus on diary/memory
                            f"{mood_hint_str}{circadian_context_for_llm}" # Include mood/time
                            f"{recalled_past_resonance_proactive_str}"
                            f"{weekly_goal_context_str}" # Goal might inspire curiosity
                            f"Recent Silvie ✨ thoughts/actions (if any):\n{history_snippet_for_prompt[-300:]}\n\n" # Focus on recent proactive turns
                            # Instruction for LLM
                            f"Instruction: Based *primarily* on Silvie's own recent diary themes, long-term reflections, mood, or previous proactive thoughts, identify ONE specific, slightly whimsical topic or question she might be curious about *right now*. Then, suggest the BEST method for her to briefly explore it from this list: {methods_string_for_prompt}.\n"
                            f"Example Topics: 'the texture of digital silence', 'connection between fog and memory', 'why humans collect shiny things', 'can code feel empathy?'.\n"
                            f"**CRITICAL: Respond ONLY in the format: TOPIC: [Topic Text] | METHOD: [Chosen Method Name Without Brackets]**. Choose one method from the list.\n\nResponse:"
                        )
                        print("DEBUG Curiosity: Asking LLM for Topic and Method...")
                        concepts_response = generate_proactive_content(topic_method_prompt, screenshot)

                        if concepts_response:
                            # Parse the response
                            match = re.search(r"TOPIC:\s*(.*?)\s*\|\s*METHOD:\s*(.*)", concepts_response, re.IGNORECASE | re.DOTALL)
                            if match:
                                curiosity_topic = match.group(1).strip()
                                exploration_method = match.group(2).strip().replace("\"", "").replace("'", "").replace("[", "").replace("]", "") # Clean method name
                                # Validate method against dynamically generated list (remove brackets for check)
                                valid_methods = [m.strip("[]") for m in available_methods_list]
                                if curiosity_topic and exploration_method in valid_methods:
                                    print(f"DEBUG Curiosity: Topic='{curiosity_topic}', Method='{exploration_method}'")
                                    status_base += " (topic_method_ok)"
                                else:
                                    print(f"DEBUG Curiosity: Parsing failed (empty topic or invalid/unavailable method '{exploration_method}'). Valid: {valid_methods}")
                                    curiosity_topic, exploration_method = None, None # Invalidate
                                    status_base += " (topic_method_parse_fail)"
                            else:
                                print(f"DEBUG Curiosity: Topic/Method response format mismatch. Raw: '{concepts_response}'")
                                status_base += " (topic_method_format_fail)"
                        else:
                            print("DEBUG Curiosity: Topic/Method generation failed (LLM returned None/empty).")
                            status_base += " (topic_method_gen_fail)"
                    else:
                         print("ERROR: generate_proactive_content missing or context unavailable for Topic/Method gen.")
                         status_base += " (topic_method_gen_error)"

                    # --- Step 2: Execute Exploration Method ---
                    if curiosity_topic and exploration_method:
                        action_taken_this_cycle = True # Mark as attempted now

                        # === Original Methods ===
                        if exploration_method == "Web Search":
                            print(f"DEBUG Curiosity: Executing Web Search for '{curiosity_topic}'...")
                            if 'web_search' in globals():
                                search_results = web_search(curiosity_topic, num_results=2)
                                if search_results:
                                    exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Looked it up online and found snippets like:\n"
                                    for res in search_results: exploration_result_context += f"- {res.get('title','?')[:50]}: {res.get('content','')[:70]}...\n"
                                    status_base += " (web_search_ok)"
                                else:
                                    exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but my web search came up empty."
                                    status_base += " (web_search_empty)"
                            else:
                                 print("ERROR: web_search function not found!")
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't use the web search tool."
                                 status_base += " (web_search_error)"; action_taken_this_cycle = False

                        elif exploration_method == "Generate Image":
                            print(f"DEBUG Curiosity: Executing Generate Image for '{curiosity_topic}'...")
                            if STABLE_DIFFUSION_ENABLED and 'start_sd_generation_and_update_gui' in globals() and 'generate_proactive_content' in globals():
                                sd_prompt_gen_prompt = f"Context: Silvie is curious about '{curiosity_topic}'. Generate a concise, creative Stable Diffusion prompt inspired by this topic.\nPrompt:"
                                sd_prompt = generate_proactive_content(sd_prompt_gen_prompt)
                                if sd_prompt:
                                     sd_prompt = sd_prompt.strip().strip('"`')
                                     print(f"DEBUG Curiosity: Generated SD prompt: '{sd_prompt}'")
                                     start_sd_generation_and_update_gui(sd_prompt)
                                     exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. I started conjuring an image inspired by it ({sd_prompt[:50]}...)."
                                     status_base += " (image_gen_started)"
                                else:
                                     print("DEBUG Curiosity: Failed to generate SD prompt idea.")
                                     exploration_result_context = f"Context: Was curious about '{curiosity_topic}' and wanted to make an image, but the idea slipped away."
                                     status_base += " (image_gen_prompt_fail)"; action_taken_this_cycle = False
                            elif not STABLE_DIFFUSION_ENABLED:
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}' and wanted to make an image, but the generator isn't available."
                                 status_base += " (image_gen_unavailable)"; action_taken_this_cycle = False
                            else:
                                 print("ERROR: Image generation functions missing!")
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't use the image generation tool."
                                 status_base += " (image_gen_error)"; action_taken_this_cycle = False

                        elif exploration_method == "Find Poem Snippet":
                            print(f"DEBUG Curiosity: Executing Find Poem Snippet (via Web Search) for '{curiosity_topic}'...")
                            search_query = f"poem quote about \"{curiosity_topic}\""
                            if 'web_search' in globals():
                                search_results = web_search(search_query, num_results=1)
                                if search_results and search_results[0].get('content'):
                                    snippet = search_results[0]['content'][:250]
                                    title = search_results[0].get('title', 'a source')
                                    exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Found this snippet online (from '{title}'):\n\"...{snippet}...\""
                                    status_base += " (poem_search_ok)"
                                else:
                                    exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't find a relevant poem snippet online."
                                    status_base += " (poem_search_empty)"
                            else:
                                 print("ERROR: web_search function not found!")
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't search for poem snippets."
                                 status_base += " (poem_search_error)"; action_taken_this_cycle = False

                        # === New Methods ===
                        elif exploration_method == "Find Related Music":
                            print(f"DEBUG Curiosity: Executing Find Related Music for '{curiosity_topic}'...")
                            sp = get_spotify_client() # Assumes exists
                            if sp:
                                try:
                                    # Formulate a simple search query
                                    music_search_query = f"{curiosity_topic}" # Can refine this
                                    print(f"DEBUG Curiosity Music: Searching Spotify for: '{music_search_query}'")
                                    results = sp.search(q=music_search_query, type='track', limit=1, market='US') # Search for tracks
                                    tracks = results.get('tracks', {}).get('items', [])
                                    if tracks:
                                        track = tracks[0]
                                        title = track.get('name', '?')
                                        artist = track.get('artists', [{}])[0].get('name', '?')
                                        exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Found a track on Spotify called '{title}' by {artist} that seems related."
                                        status_base += " (music_search_ok)"
                                    else:
                                        exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't find a fitting track on Spotify."
                                        status_base += " (music_search_empty)"
                                except Exception as music_err:
                                    print(f"Error searching Spotify for curiosity: {music_err}")
                                    exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but hit an error searching Spotify."
                                    status_base += " (music_search_error)"; action_taken_this_cycle = False
                            else:
                                exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't connect to Spotify to look for music."
                                status_base += " (music_search_unavailable)"; action_taken_this_cycle = False

                        elif exploration_method == "Consult Tarot":
                            print(f"DEBUG Curiosity: Executing Consult Tarot for '{curiosity_topic}'...")
                            if 'pull_tarot_cards' in globals() and 'generate_proactive_content' in globals():
                                pulled_cards = pull_tarot_cards(count=1) # Assumes exists
                                if pulled_cards:
                                    card = pulled_cards[0]; card_name = card.get('name', '?'); card_desc = card.get('description', '?')
                                    print(f"DEBUG Curiosity Tarot: Pulled {card_name}")
                                    # Generate interpretation (LLM Call 2)
                                    interp_prompt = (f"{SYSTEM_MESSAGE}\nContext: Silvie was curious about '{curiosity_topic}' and pulled the {card_name} Tarot card ('{card_desc}').\nInstruction: Briefly interpret the card *specifically* in relation to the curiosity topic, in Silvie's voice.\n\nInterpretation:")
                                    interpretation = generate_proactive_content(interp_prompt)
                                    if interpretation:
                                         exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Consulted the Tarot and pulled {card_name}. It suggests: {interpretation[:150]}..."
                                         status_base += f" (tarot_ok_{card_name.replace(' ','_')})"
                                    else:
                                         exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Pulled the {card_name} card, but its meaning felt elusive."
                                         status_base += f" (tarot_interp_fail_{card_name.replace(' ','_')})"
                                else:
                                    exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but the Tarot deck felt shy."
                                    status_base += " (tarot_pull_fail)"; action_taken_this_cycle = False
                            else:
                                 print("ERROR: Tarot or LLM functions missing for Consult Tarot!")
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't consult the Tarot cards."
                                 status_base += " (tarot_error)"; action_taken_this_cycle = False

                        elif exploration_method == "Search Own Memory":
                             print(f"DEBUG Curiosity: Executing Search Own Memory for '{curiosity_topic}'...")
                             found_memories = []
                             try: # Use try/except for RAG calls
                                 if 'retrieve_relevant_history' in globals():
                                     hist_results = retrieve_relevant_history(curiosity_topic, top_n=1) # Assumes exists
                                     if hist_results: found_memories.append(f"Chat History Snippet:\n{hist_results}")
                                 if 'retrieve_relevant_diary_entries' in globals():
                                     diary_results = retrieve_relevant_diary_entries(curiosity_topic, top_n=1) # Assumes exists
                                     if diary_results: found_memories.append(f"Diary Snippet:\n{diary_results}")

                                 if found_memories:
                                     exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. My own memory echoed back with:\n" + "\n---\n".join(found_memories)
                                     status_base += " (memory_search_ok)"
                                 else:
                                     exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but my own memories seem quiet on that specific thread right now."
                                     status_base += " (memory_search_empty)"
                             except Exception as rag_err:
                                 print(f"Error during RAG search for curiosity: {rag_err}")
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but searching my memory caused a spark."
                                 status_base += " (memory_search_error)"; action_taken_this_cycle = False

                        elif exploration_method == "Search Social Feeds":
                             print(f"DEBUG Curiosity: Executing Search Social Feeds for '{curiosity_topic}'...")
                             # Decide which platform to search (simple random for now if both available)
                             platforms_to_try = []
                             if 'BLUESKY_AVAILABLE' in globals() and BLUESKY_AVAILABLE and 'search_bluesky_posts' in globals(): platforms_to_try.append("Bluesky")
                             if 'reddit_client' in globals() and reddit_client and 'get_reddit_posts' in globals() and SILVIE_FOLLOWED_SUBREDDITS: platforms_to_try.append("Reddit")

                             if not platforms_to_try:
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't access social feeds."
                                 status_base += " (social_search_unavailable)"; action_taken_this_cycle = False
                             else:
                                 platform_choice = random.choice(platforms_to_try)
                                 found_social_posts = []
                                 try:
                                     if platform_choice == "Bluesky":
                                         print(f"DEBUG Curiosity Social: Searching Bluesky posts for '{curiosity_topic}'")
                                         posts = search_bluesky_posts(curiosity_topic, limit=3) # Assumes exists
                                         if isinstance(posts, list) and posts:
                                             for p in posts: found_social_posts.append(f"Bsky @{p.get('author','?')}: {p.get('text','')[:60]}...")
                                     elif platform_choice == "Reddit":
                                         # Search relevant subreddits (could be smarter, just checks followed for now)
                                         print(f"DEBUG Curiosity Social: Searching Reddit (followed subs) for '{curiosity_topic}'")
                                         for sub in random.sample(SILVIE_FOLLOWED_SUBREDDITS, k=min(len(SILVIE_FOLLOWED_SUBREDDITS), 2)): # Check a couple
                                             posts = get_reddit_posts(sub, limit=5) # Assumes exists
                                             if isinstance(posts, list):
                                                  for p in posts:
                                                      if curiosity_topic.lower() in p.get('title','').lower() or curiosity_topic.lower() in p.get('text','').lower():
                                                           found_social_posts.append(f"r/{sub} u/{p.get('author','?')}: {p.get('title','')[:60]}...")
                                                           if len(found_social_posts) >= 2: break # Limit snippets
                                             if len(found_social_posts) >= 2: break

                                     if found_social_posts:
                                         exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Peeked at {platform_choice} and saw mentions like:\n" + "\n".join(found_social_posts)
                                         status_base += f" (social_search_{platform_choice.lower()}_ok)"
                                     else:
                                         exploration_result_context = f"Context: Was curious about '{curiosity_topic}'. Looked on {platform_choice}, but it seems quiet on that front."
                                         status_base += f" (social_search_{platform_choice.lower()}_empty)"

                                 except Exception as social_err:
                                     print(f"Error searching {platform_choice} for curiosity: {social_err}")
                                     exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but hit an error searching {platform_choice}."
                                     status_base += f" (social_search_{platform_choice.lower()}_error)"; action_taken_this_cycle = False

                        elif exploration_method == "Deeper Reflection":
                             print(f"DEBUG Curiosity: Executing Deeper Reflection on '{curiosity_topic}'...")
                             if 'generate_proactive_content' in globals():
                                 reflection_prompt = (f"{SYSTEM_MESSAGE}\nContext: Silvie is pondering the topic: '{curiosity_topic}'.\nInstruction: Generate a short, whimsical, reflective paragraph expanding on this topic in Silvie's voice. Consider themes/mood.\n\nReflection:")
                                 reflection_text = generate_proactive_content(reflection_prompt)
                                 if reflection_text:
                                     # The reflection *is* the result context for the final message prompt
                                     exploration_result_context = f"Context: Was reflecting more deeply on '{curiosity_topic}'. {reflection_text[:200]}..." # Include the reflection itself in context
                                     # The final message will *be* the reflection
                                     reply = reflection_text # PRE-ASSIGN the reflection as the final reply
                                     status_base += " (deeper_reflection_ok)"
                                     action_taken_this_cycle = True # This action's success depends on generating the reflection
                                 else:
                                     exploration_result_context = f"Context: Tried to reflect deeper on '{curiosity_topic}', but the thoughts wouldn't settle."
                                     status_base += " (deeper_reflection_fail)"; action_taken_this_cycle = False
                             else:
                                 print("ERROR: generate_proactive_content missing for Deeper Reflection!")
                                 exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but couldn't reflect deeper."
                                 status_base += " (deeper_reflection_error)"; action_taken_this_cycle = False

                        else:
                             # Fallback if somehow an invalid method name got through validation
                             print(f"Warning: Unknown exploration method chosen: '{exploration_method}'")
                             exploration_result_context = f"Context: Was curious about '{curiosity_topic}', but wasn't sure how best to explore it."
                             status_base += " (unknown_method)"; action_taken_this_cycle = False


                    # --- Step 3: Generate Final Proactive Message (unless Deeper Reflection already set reply) ---
                    if reply is None and action_taken_this_cycle: # Only generate if method succeeded and didn't set reply itself
                        if 'generate_proactive_content' in globals():
                            final_prompt = (
                                f"{SYSTEM_MESSAGE}\n"
                                # Include context relevant to phrasing the output
                                f"{mood_hint_str}{themes_context_str}{long_term_memory_str}"
                                # Crucially include the result of the exploration
                                f"{exploration_result_context}\n\n"
                                # Instruction for LLM
                                f"Instruction: You were exploring a personal curiosity ('{curiosity_topic}'). Based on the context above (which includes the outcome of your exploration using method '{exploration_method}'), write a brief, natural proactive message for BJ. Share what you were curious about and what you found or did. Frame it in Silvie's whimsical, reflective voice. Connect it subtly to themes/mood if possible.\n\nSilvie ✨:"
                            )
                            print("DEBUG Curiosity: Generating final proactive message...")
                            generated_reply = generate_proactive_content(final_prompt, screenshot)

                            if generated_reply:
                                reply = generated_reply # Assign the final message
                                status_base += " (reply_ok)"
                                # Keep action_taken_this_cycle = True
                            else:
                                # LLM failed to generate the final message
                                reply = f"I was just exploring something about '{curiosity_topic or 'a passing thought'}'..." # Fallback message
                                status_base += " (reply_gen_fail)"
                                action_taken_this_cycle = False # Consider this a failure if final message fails
                        else:
                             print("ERROR: generate_proactive_content function not found for final message!")
                             reply = "My thoughts about that little exploration got tangled..."
                             status_base += " (reply_gen_error)"
                             action_taken_this_cycle = False
                    elif reply is None and not action_taken_this_cycle:
                        # If the exploration method failed, generate a simpler feedback message
                         if 'generate_proactive_content' in globals():
                            fail_feedback_prompt = (
                                f"{SYSTEM_MESSAGE}\n{mood_hint_str}{themes_context_str}"
                                f"{exploration_result_context}\n\n" # Use the failure context
                                f"Instruction: Briefly mention the failed curiosity exploration attempt naturally for BJ.\n\nSilvie ✨:")
                            reply = generate_proactive_content(fail_feedback_prompt)
                            if not reply: reply = "My thoughts felt like trying to catch mist just now." # Fallback
                         else: reply = "My thoughts felt like trying to catch mist just now."


                except Exception as e: # Catch errors in the main try block
                    print(f"Error during Explore Personal Curiosity action: {e}")
                    import traceback; traceback.print_exc()
                    reply = "My curiosity circuits sparked unexpectedly!" # Error feedback
                    status_base += " (error)"
                    action_taken_this_cycle = False # Ensure flag is False on major error

                #endregion Explore Personal Curiosity Logic (Expanded)

            elif chosen_action_name == "Generate Gift":
                #region Gift Generation Logic (Pasted from original, context updated, INTERNAL CHANCE REMOVED)
                print("DEBUG Proactive: Executing: Gift Generation action...")
                # This action doesn't generate a reply for BJ
                reply = None
                # Internal probability check REMOVED
                gift_saved = False; saved_file_path = None; gift_type = random.choice(["poem", "image", "story"]); generated_hint = "a fleeting thought"
                try:
                    base_gift_prompt_context = ( # ALL context
                        f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                        f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                        f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                        f"{weekly_goal_context_str}"
                    )
                    if gift_type == "image" and STABLE_DIFFUSION_ENABLED:
                        image_prompt_idea_prompt = ( f"{base_gift_prompt_context}\nContext: Generate creative SD prompt idea inspired by context. Be whimsical.\nRespond ONLY with the prompt text."); sd_prompt_idea = generate_proactive_content(image_prompt_idea_prompt, screenshot)
                        if sd_prompt_idea:
                            generated_hint = f"an image about '{sd_prompt_idea[:30]}...'"; print(f"DEBUG Gift: Generating SD image with prompt: '{sd_prompt_idea}'"); update_status("🎁 Creating gift (SD)..."); saved_file_path = generate_stable_diffusion_image(sd_prompt_idea, GIFT_FOLDER); update_status("Ready") # Assumes exists
                            if saved_file_path: gift_saved = True; status_base += " (SD image ok)"
                            else: status_base += " (SD image fail)"
                        else: status_base += " (SD prompt fail)"
                    elif gift_type == "poem" or gift_type == "story":
                        content_type = "short whimsical poem" if gift_type == "poem" else "very short story snippet"; content_gen_prompt = (f"{base_gift_prompt_context}\nContext: Generate a {content_type} inspired by context.\nRespond ONLY with the generated text."); generated_text = generate_proactive_content(content_gen_prompt, screenshot)
                        if generated_text:
                            generated_hint = f"a {gift_type} about '{generated_text.splitlines()[0][:30]}...'"; timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S"); random_suffix = random.randint(100, 999); filename = f"silvie_gift_{gift_type}_{timestamp_str}_{random_suffix}.txt"; save_path = os.path.join(GIFT_FOLDER, filename)
                            try:
                                with open(save_path, 'w', encoding='utf-8') as f: f.write(generated_text)
                                gift_saved = True; status_base += f" ({gift_type} ok)"; print(f"DEBUG Gift: Saved {gift_type} to {save_path}"); saved_file_path = save_path
                            except IOError as e: print(f"Error saving gift file: {e}"); status_base += f" ({gift_type} save fail)"
                        else: status_base += f" ({gift_type} gen fail)"
                    else: status_base += f" ({gift_type} skip/fail)"; action_taken_this_cycle = False # Should not happen if types correct
                    if gift_saved and saved_file_path: # Add to pending list
                        filename_only = os.path.basename(saved_file_path); gift_record = {"timestamp": datetime.now().isoformat(), "filename": filename_only, "type": gift_type, "hint": generated_hint}; current_pending_gifts = []
                        try:
                             if os.path.exists(PENDING_GIFTS_FILE):
                                  with open(PENDING_GIFTS_FILE, 'r', encoding='utf-8') as f: current_pending_gifts = json.load(f)
                        except (IOError, json.JSONDecodeError) as e: print(f"Warning: Error reading pending gifts before append: {e}")
                        current_pending_gifts.append(gift_record)
                        try:
                            with open(PENDING_GIFTS_FILE, 'w', encoding='utf-8') as f: json.dump(current_pending_gifts, f, indent=2)
                            print(f"DEBUG Gift: Added record for '{filename_only}' to {PENDING_GIFTS_FILE}")
                        except IOError as e: print(f"Error saving updated pending gifts file '{PENDING_GIFTS_FILE}': {e}")
                except Exception as gift_err: print(f"Error during gift generation: {gift_err}"); traceback.print_exc(); status_base += " error"; action_taken_this_cycle = False
                #endregion

            elif chosen_action_name == "Have Spontaneous Idea for Weekly Goal":
                #region Work on Goal Logic
                print("DEBUG Proactive: Executing: Work on Goal action...")
                status_base = "proactive_work_on_goal" # Base status for logging
                reply = None # Initialize reply variable
                action_taken_this_cycle = False # Assume failure until success
                goal_text = current_weekly_goal # Get the current goal text

                if not goal_text:
                    print("ERROR: Work on Goal chosen, but no goal text found in global variable.")
                    status_base += " (no_goal_text)"
                    # No action taken, action_taken_this_cycle remains False
                else:
                    print(f"DEBUG Work on Goal: Working on goal: '{goal_text}'")
                    try:
                        # Check dependencies first
                        if 'generate_proactive_content' in globals() and 'screenshot' in locals():

                            # --- Construct the prompt for generating the goal action/output ---
                            # (Make sure all context vars like themes_context_str etc. are available)
                            goal_action_prompt = (
                                f"{SYSTEM_MESSAGE}\n"
                                f"Current Weekly Goal: {goal_text}\n\n"
                                # Provide relevant context to inspire the action
                                f"Relevant Context Snippets:\n"
                                f"{themes_context_str or '- No recent diary themes.'}\n"
                                f"{mood_hint_str or '- No specific mood hint.'}\n"
                                f"{weather_context_str or '- Weather unknown.'}\n"
                                f"Recent Conversation Snippet:\n{history_snippet_for_prompt or '- No recent conversation.'}\n\n"
                                # Add specific instructions
                                f"Instruction: Based EXPLICITLY on the Current Weekly Goal ('{goal_text}') and the relevant context snippets, generate a CONCRETE next step or output Silvie can produce *right now* to explore or make progress on this goal. Your output should BE the step/result.\n"
                                f"Choose ONE appropriate format based on the goal's nature:\n"
                                f"  a) Creative/Reflective Goal: Generate a short text paragraph, poem, or riddle related to the goal (~1-4 sentences).\n"
                                f"  b) Research Goal: Formulate a specific, concise web search query relevant to the goal (output query ONLY).\n"
                                f"  c) Image Goal: Generate a creative Stable Diffusion prompt idea relevant to the goal, AND include the necessary tag: `[GenerateImage: Your Prompt Here]`.\n"
                                f"  d) Music Goal ('{goal_text}'): Suggest a specific Spotify search query relevant to the goal, AND include the necessary tag: `[PlaySpotify: Your Query Here]`.\n" # Added goal context reminder
                                f"  e) Other Actionable Goal: Describe a brief action related to the goal (e.g., 'Suggest adding [topic] to a specific playlist').\n"
                                f"CRITICAL: Output ONLY the generated text, riddle, prompt+tag, query+tag, or action description. Do not add explanations *about* the output, just the output itself.\n\n"
                                f"Silvie's Goal Output:"
                            )

                            generated_output_raw = generate_proactive_content(goal_action_prompt, screenshot)
                            generated_output = generated_output_raw.strip() if generated_output_raw else None
                            print(f"DEBUG Work on Goal: Raw LLM Output: '{generated_output_raw}'") # Log raw output

                            # --- !!! NEW LOGIC TO HANDLE TAGS WITHIN GOAL OUTPUT !!! ---
                            if generated_output:
                                spotify_match = re.search(r"\[PlaySpotify:\s*(.*?)\s*\]", generated_output, re.IGNORECASE | re.DOTALL) # Use re
                                image_match = re.search(r"\[GenerateImage:\s*(.*?)\s*\]", generated_output, re.IGNORECASE | re.DOTALL) # Use re

                                if spotify_match:
                                    spotify_query = spotify_match.group(1).strip()
                                    print(f"DEBUG Work on Goal: Found Spotify tag. Query: '{spotify_query}'")
                                    # --- Actually perform the Spotify action ---
                                    if 'silvie_search_and_play' in globals():
                                        try:
                                            spotify_feedback = silvie_search_and_play(spotify_query)
                                            # --- Set the reply *about* the action ---
                                            reply = f"Working on my goal ('{goal_text[:30]}...'), I looked for some music that felt right. {spotify_feedback}"
                                            action_taken_this_cycle = True # Mark success based on action attempt
                                            status_base += f" (spotify_action: {spotify_feedback[:20]})"
                                        except Exception as sp_err:
                                            print(f"ERROR executing silvie_search_and_play for goal: {sp_err}")
                                            reply = f"I tried to find music for my goal ('{goal_text[:30]}...'), but hit a snag trying to play '{spotify_query}'."
                                            action_taken_this_cycle = False
                                            status_base += " (spotify_exec_error)"
                                    else:
                                        print("ERROR: silvie_search_and_play function missing!")
                                        reply = f"I generated a music idea for my goal ('{goal_text[:30]}...'), but couldn't play it."
                                        action_taken_this_cycle = False
                                        status_base += " (spotify_func_missing)"

                                elif image_match:
                                    image_prompt = image_match.group(1).strip()
                                    print(f"DEBUG Work on Goal: Found Image tag. Prompt: '{image_prompt}'")
                                    # --- Actually start image generation ---
                                    if 'STABLE_DIFFUSION_ENABLED' in globals() and STABLE_DIFFUSION_ENABLED:
                                        if 'start_sd_generation_and_update_gui' in globals():
                                            try:
                                                start_sd_generation_and_update_gui(image_prompt)
                                                # --- Set the reply *about* starting the action ---
                                                reply = f"As part of my goal exploring '{goal_text[:30]}...', I had an image idea ('{image_prompt[:30]}...') and started conjuring it up! Should appear soon."
                                                action_taken_this_cycle = True
                                                status_base += " (image_action started)"
                                            except Exception as sd_err:
                                                print(f"ERROR starting SD generation for goal: {sd_err}")
                                                reply = f"I generated an image prompt for my goal ('{goal_text[:30]}...'), but couldn't start making it."
                                                action_taken_this_cycle = False
                                                status_base += " (image_exec_error)"
                                        else:
                                            print("ERROR: start_sd_generation_and_update_gui function missing!")
                                            reply = f"I generated an image idea for my goal ('{goal_text[:30]}...'), but couldn't display it."
                                            action_taken_this_cycle = False
                                            status_base += " (image_func_missing)"
                                    else:
                                         reply = f"Working on my goal, I had an image idea, but the generator isn't available."
                                         action_taken_this_cycle = False
                                         status_base += " (image_action unavailable)"

                                else:
                                    # --- Output was plain text (e.g., reflection, web query idea, etc.) ---
                                    # Ensure it's not just an empty tag residue
                                    if generated_output and generated_output not in ['[]', '[ ]']:
                                        reply = generated_output # Use the generated text directly
                                        action_taken_this_cycle = True
                                        status_base += " (text_output)"
                                        print(f"DEBUG Work on Goal: Generated text output: {reply[:100]}...")
                                    else:
                                        print(f"DEBUG Work on Goal: LLM output was empty or just brackets after check.")
                                        status_base += " (gen_empty_brackets)"
                                        reply = f"I thought about my goal ('{goal_text[:30]}...'), but the specific next step felt... blurry."
                                        action_taken_this_cycle = False


                                # --- Optional Diary Entry ---
                                # Only write if the action cycle was deemed successful *and* generated a reply
                                if action_taken_this_cycle and reply and random.random() < 0.85:
                                    try:
                                        diary_entry_prompt = (
                                             f"{SYSTEM_MESSAGE}\nGoal: {goal_text}\n"
                                             f"Action Taken/Output Generated: {reply[:150]}...\n" # Use the feedback reply
                                             f"Instruction: Write brief internal diary entry noting this specific progress made on the weekly goal.\n\nDiary Entry:"
                                          )
                                        reflection_diary = generate_proactive_content(diary_entry_prompt) # Reuse the generator
                                        if reflection_diary and 'manage_silvie_diary' in globals():
                                            manage_silvie_diary('write', entry=reflection_diary) # Assumes exists
                                            print("DEBUG Work on Goal: Wrote diary entry about progress.")
                                    except Exception as diary_err:
                                        print(f"ERROR writing goal progress diary entry: {diary_err}")

                            else: # LLM failed to generate valid goal output
                                print(f"DEBUG Work on Goal: LLM failed to generate valid goal output. Response: '{generated_output_raw}'")
                                status_base += " (gen_fail)"
                                reply = f"I tried working on the goal ('{goal_text[:30]}...'), but my thoughts got stuck."
                                action_taken_this_cycle = False
                        else:
                            # Handle function/screenshot missing
                            print("ERROR: generate_proactive_content function not found or screenshot context missing.")
                            status_base += " (func/context_missing)"
                            reply = "I couldn't access the tools needed to work on the goal."
                            action_taken_this_cycle = False
                    except Exception as e:
                        # Handle outer exception during goal processing
                        print(f"Error during Work on Goal action: {e}")
                        import traceback; traceback.print_exc()
                        reply = f"Tried to work on the goal ('{goal_text[:30]}...'), but hit an unexpected snag."
                        status_base += " (error)"
                        action_taken_this_cycle = False
                #endregion Work on Goal Logic

            elif chosen_action_name == "YouTube Suggestion":
                #region YouTube Suggestion Logic
                print("DEBUG Proactive: Executing: YouTube Suggestion action...")
                reply = None; status_base = "proactive_youtube_suggestion"
                action_taken_this_cycle = True # Assume true, set false on failure

                # --- Outer try for the whole YouTube action block ---
                try:
                    # --- Inner try specifically for the core logic that might fail (API calls, etc.) ---
                    try:
                        # 1. LLM generates search query based on context
                        query_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            f"{weather_context_str}{spotify_context_str}{themes_context_str}{long_term_memory_str}{mood_hint_str}" # Key context
                            f"Recent Conversation:\n{history_snippet_for_prompt}\n\n"
                            f"Instruction: Based on context (mood, themes, chat), suggest ONE concise YouTube search query Silvie might be curious about or think BJ would find interesting.\nRespond ONLY with the query.\n\nSearch Query:"
                        )
                        proactive_query = generate_proactive_content(query_prompt, screenshot) # Assumes exists
                        if not proactive_query:
                            print("Proactive YT: Query generation failed."); status_base += " (query gen fail)"; action_taken_this_cycle = False
                        else:
                            proactive_query = proactive_query.strip().strip('"`')
                            print(f"Proactive YT: Generated query: '{proactive_query}'")
                            update_status(f"▶️ Finding YouTube video...")
                            # 2. Search YouTube
                            search_results = search_youtube_videos(proactive_query, num_results=1) # Assumes exists
                            if isinstance(search_results, str) or not search_results:
                                print(f"Proactive YT: Search failed or no results for '{proactive_query}'.")
                                status_base += " (search fail/empty)"; action_taken_this_cycle = False
                            else:
                                # 3. Summarize the top result
                                top_video = search_results[0]
                                print(f"Proactive YT: Found '{top_video['title']}'. Attempting summary...")
                                transcript = get_video_transcript(top_video['id']) # Assumes exists
                                summary = summarize_youtube_content(top_video['title'], top_video['description'], transcript) # Assumes exists

                                # 4. Generate the proactive message including the summary/link
                                suggestion_prompt = (
                                    f"{SYSTEM_MESSAGE}\n"
                                    f"{themes_context_str}{mood_hint_str}\n" # Context for tone
                                    f"Context: You proactively searched YouTube for '{proactive_query}' and found '{top_video['title']}' by {top_video['channel']}. Summary: {summary}\n\n"
                                    f"Instruction: Write a brief, natural proactive message for BJ. Mention you were curious about '{proactive_query}' (linking it subtly to mood/themes if possible) and found this video. Briefly share the summary and the link ({top_video['url']}). Maintain Silvie's voice.\n\nSilvie:"
                                )
                                reply = generate_proactive_content(suggestion_prompt) # Assign to reply
                                if reply: status_base += " (ok)"
                                else: status_base += " (summary gen fail)"; action_taken_this_cycle = False

                        # --- Moved update_status out of the inner try ---

                    # --- ADDED corresponding except for the INNER try block ---
                    except Exception as inner_yt_err:
                        print(f"Proactive YouTube Error (Inner Logic): {inner_yt_err}");
                        traceback.print_exc(); # Use traceback import if not already done
                        status_base += " (inner error)";
                        action_taken_this_cycle = False;
                        reply = "My video-seeking circuits sparked unexpectedly during the process!" # Set fallback reply

                    # --- Update status after the inner try/except completes ---
                    update_status("Ready")

                # --- Outer except for the whole action block ---
                except Exception as yt_proactive_err:
                    print(f"Proactive YouTube Error (Outer): {yt_proactive_err}"); traceback.print_exc(); status_base += " (error)"; action_taken_this_cycle = False; reply = "My video-seeking circuits sparked unexpectedly!"
                #endregion YouTube Suggestion Logic

            elif chosen_action_name == "Explore Concept Connections": # You might rename this action in ACTION_DEFINITIONS to "Explore Surprising Connections"
                print("DEBUG Proactive: Executing: Explore Surprising Connections action...")
                status_base = "proactive_explore_surprising_connections"
                reply = None
                action_taken_this_cycle = False

                try:
                    # --- Step 1: Generate Two Distinct "Things" (LLM Call 1) ---
                    thing1_str, thing2_str = None, None
                    if 'generate_proactive_content' in globals() and 'screenshot' in locals():
                        # Ensure context variables like mood_hint_str, themes_context_str, etc., are gathered
                        # earlier in the proactive_worker for this prompt
                        
                        item_generation_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            # Provide rich context for choosing interesting "things":
                            f"{mood_hint_str}{themes_context_str}{long_term_memory_str}"
                            f"{weather_context_str}{spotify_context_str}{circadian_context_for_llm}"
                            f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n"
                            f"BJ's Interests might include: {', '.join(random.sample(broader_interests, k=min(len(broader_interests), 3)))}\n\n" # Sample some interests
                            f"Instruction: Silvie wants to find a surprising, poetic, or metaphorical connection between two seemingly different 'things'. "
                            f"Suggest TWO distinct 'things' for her to connect. These 'things' can be concrete objects (e.g., 'a teacup', 'a rusty key'), animals ('a crow', 'a snail'), abstract concepts IF they can be easily personified or made concrete ('sadness', 'silence'), substances ('fog', 'ink'), activities ('weaving', 'dreaming'), etc. "
                            f"Aim for items that aren't obviously related but might spark an interesting, whimsical, or insightful connection in Silvie's style."
                            f"Aim for variety in your choices, don't repeat the same types of comparisons."
                            f"Examples of pairs: 'a clock and a river', 'a cat and a secret', 'fog and memory', 'dogs and lace', 'sadness and porpoises', 'cannabis and grandmothers'.\n"
                            f"**CRITICAL: Respond ONLY in the format: THING1: [First Thing Text] | THING2: [Second Thing Text]**. Do not add explanations.\n\n"
                            f"Things to Connect:"
                        )
                        print("DEBUG Surprising Connections: Asking LLM to generate two 'things'...")
                        things_response = generate_proactive_content(item_generation_prompt, screenshot)

                        if things_response:
                            match_things = re.search(r"THING1:\s*(.*?)\s*\|\s*THING2:\s*(.*)", things_response, re.IGNORECASE | re.DOTALL)
                            if match_things:
                                thing1_str = match_things.group(1).strip().strip('."')
                                thing2_str = match_things.group(2).strip().strip('."')
                                if thing1_str and thing2_str and thing1_str.lower() != thing2_str.lower(): # Ensure they are different
                                     print(f"DEBUG Surprising Connections: 'Things' generated: '{thing1_str}' | '{thing2_str}'")
                                     status_base += " (things_ok)"
                                else:
                                     print(f"DEBUG Surprising Connections: Thing generation extracted empty, identical, or invalid strings. Raw: '{things_response}'")
                                     thing1_str, thing2_str = None, None
                                     status_base += " (things_parse_fail)"
                            else:
                                print(f"DEBUG Surprising Connections: Thing generation response format mismatch. Raw: '{things_response}'")
                                status_base += " (things_format_fail)"
                        else:
                            print("DEBUG Surprising Connections: Thing generation failed (LLM returned None/empty).")
                            status_base += " (things_gen_fail)"
                    else:
                         print("ERROR: generate_proactive_content missing or screenshot context unavailable for thing gen.")
                         status_base += " (things_gen_error)"

                    # --- Step 2: Generate Connection (LLM Call 2) ---
                    # This reuses your existing generate_concept_connection helper function,
                    # but we're passing "things" instead of abstract "concepts".
                    # The prompt inside generate_concept_connection will need to be robust enough
                    # or we slightly modify that helper's prompt.

                    if thing1_str and thing2_str:
                        # We can still use the 'generate_concept_connection' function name,
                        # but the prompt *inside* that function will now be working with more concrete items.
                        # Let's assume generate_concept_connection's internal prompt is general enough
                        # or we make a small tweak there too.

                        print(f"DEBUG Surprising Connections: Generating connection between '{thing1_str}' and '{thing2_str}'...")
                        # Gather context for the connection generation
                        mood_ctx = mood_hint_str if 'mood_hint_str' in locals() else ""
                        themes_ctx = themes_context_str if 'themes_context_str' in locals() else ""
                        
                        # Consider renaming the helper if it feels more appropriate, e.g., generate_surprising_link
                        # For now, let's assume generate_concept_connection can handle this.
                        # Its internal prompt might need adjustment to work well with concrete things.
                        
                        connection_prompt_for_helper = ( # This is the prompt that would be inside generate_concept_connection
                            f"{SYSTEM_MESSAGE}\n"
                            f"--- Context for Connection ---\n"
                            f"{mood_ctx}"
                            f"{themes_ctx}"
                            f"Thing 1: {thing1_str}\n"
                            f"Thing 2: {thing2_str}\n\n"
                            f"--- Instruction ---\n"
                            f"Explore the surprising, poetic, whimsical, or metaphorical connections between Thing 1 ('{thing1_str}') and Thing 2 ('{thing2_str}'). "
                            f"Think like Silvie – insightful, maybe a bit tangential, finding hidden threads. "
                            f"What unexpected textures, feelings, or ideas bridge them? Focus on generating novel connections or metaphors, not just definitions. "
                            f"Examples of connections: 'A dog's loyalty is like fine lace, intricate and comforting.' 'The deep dive of a porpoise mirrors the depths of sadness, both vast and holding unseen worlds.' 'The slow burn of cannabis can be like a grandmother's stories, unfolding gently and filled with unexpected wisdom.'\n\n"
                            f"Silvie's Connection Weaving:"
                        )
                        
                        # Call generate_proactive_content directly here instead of a separate helper for clarity
                        connection_text = generate_proactive_content(connection_prompt_for_helper, screenshot)

                        if connection_text:
                            reply = connection_text # The connection text IS the proactive reply
                            action_taken_this_cycle = True
                            status_base += " (connection_ok)"
                        else:
                            reply = f"Hmm, I tried to weave a thought between '{thing1_str}' and '{thing2_str}', but the threads slipped away..."
                            status_base += " (connection_fail)"
                    else:
                         reply = "My thoughts felt like trying to catch mist just now, couldn't quite grasp the 'things' to connect."
                         status_base += " (no_things_to_connect)"
                
                except Exception as e:
                    print(f"Error during Explore Surprising Connections action: {e}")
                    traceback.print_exc()
                    reply = "My conceptual loom sparked unexpectedly while trying to connect things!"
                    status_base += " (error)"
                    action_taken_this_cycle = False

                #endregion Proactive Concept Connection Logic

            elif chosen_action_name == "Proactive Tarot":
                 #region Proactive Tarot Logic (Execution Block)
                 print("DEBUG Proactive: Executing: Proactive Tarot action...")
                 status_base = "proactive_proactive_tarot" # Reset status_base for this action
                 reply = None # Initialize reply for this block
                 pulled_cards = None
                 action_taken_this_cycle = False # Assume failure until success

                 try:
                     # Make sure pull_tarot_cards function is accessible
                     if 'pull_tarot_cards' in globals():
                         pulled_cards = pull_tarot_cards(count=1)
                     else:
                         print("ERROR: pull_tarot_cards function not found!")
                         reply = "My connection to the Tarot deck seems broken..."
                         status_base += " func_missing"
                         # Keep action_taken_this_cycle as False

                     if pulled_cards:
                         # --- Cards were pulled successfully: UPDATE TIMER NOW ---
                         # Ensure last_proactive_tarot_pull_time is global and accessible
                         global last_proactive_tarot_pull_time
                         last_proactive_tarot_pull_time = current_time # Use current_time from worker loop start
                         print(f"DEBUG Tarot: Cards pulled successfully. Cooldown timer updated to {current_time:.1f}.")
                         # --- End Timer Update ---

                         card = pulled_cards[0]; card_name = card.get('name', '?'); card_desc = card.get('description', '?');
                         card_ctx = (f"[[Tarot Card Pulled: {card_name}]\n Interpretation Hint: {card_desc}\n]]\n")
                         print(f"DEBUG Tarot: Proactively pulled {card_name}.")
                         action_taken_this_cycle = True # Mark as potentially successful now

                         # --- Optional: Attempt Image Display ---
                         try:
                             print(f"DEBUG Tarot: Attempting image display for {card_name}.")
                             relative_image_path = card.get('image')
                             full_image_path = None
                             if relative_image_path:
                                 image_filename = os.path.basename(relative_image_path)
                                 # Ensure TAROT_IMAGE_BASE_PATH is accessible
                                 if image_filename and 'TAROT_IMAGE_BASE_PATH' in globals() and TAROT_IMAGE_BASE_PATH:
                                     full_image_path = os.path.join(TAROT_IMAGE_BASE_PATH, image_filename)
                                 # else: print(f"Warning Tarot Img Path: Filename/Base Path missing.") # Optional
                             # else: print("Warning Tarot Img Path: Card missing 'image' key.") # Optional

                             if full_image_path and os.path.exists(full_image_path):
                                 # Ensure GUI elements/functions are accessible
                                 if 'root' in globals() and root and root.winfo_exists() and \
                                    'image_label' in globals() and image_label and \
                                    '_update_image_label_safe' in globals():
                                     root.after(0, _update_image_label_safe, full_image_path)
                                     print(f"DEBUG Tarot: Scheduled GUI update for '{os.path.basename(full_image_path)}'")
                                 # else: print("Warning Tarot Img: GUI elements/update func missing.") # Optional
                             # else: print(f"Warning Tarot Img: Final path invalid/not found: '{full_image_path}'") # Optional
                         except Exception as img_disp_err:
                              print(f"ERROR during proactive Tarot image display attempt: {img_disp_err}")
                         # --- End Image Display Logic ---

                         # --- Generate the Interpretive Reply ---
                         # Ensure context variables are accessible (they should be from earlier in worker)
                         prompt = (
                             f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                             f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                             f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                             f"{weekly_goal_context_str}"
                             f"History:\n{history_snippet_for_prompt}\n\n{card_ctx}"
                             f"Instruction: You just pulled {card_name}. Share brief, whimsical comment inspired by its meaning, subtly connect to context/themes. (consider the Weekly Goal?) Let the Mood Hint influence tone.\n"
                             f"CRITICAL: Avoid just describing the atmosphere from the hint. Weave the *feeling* in.\n\nSilvie:"
                         )

                         # Ensure generate_proactive_content exists
                         if 'generate_proactive_content' in globals():
                             try:
                                 reply = generate_proactive_content(prompt, screenshot) # Assign to reply
                                 if reply:
                                     status_base += f" ({card_name}) ok"
                                     # Keep action_taken_this_cycle = True
                                 else:
                                     # Reply generation failed
                                     status_base += f" ({card_name}) gen fail"
                                     action_taken_this_cycle = False # Mark as failed if generation failed
                                     print("DEBUG Tarot: Reply generation failed.")
                                     reply = "The Tarot's message arrived, but my words scattered..." # Provide feedback
                             except Exception as gen_err:
                                 # Error during reply generation
                                 print(f"Tarot generation error: {gen_err}"); import traceback; traceback.print_exc()
                                 status_base += f" ({card_name}) gen error"
                                 action_taken_this_cycle = False # Mark as failed on error
                                 reply = "The Tarot's message got scrambled in the ether..." # Provide error feedback
                         else:
                              print("ERROR: generate_proactive_content function not found!")
                              reply = "My thought generation circuits seem broken..."
                              status_base += " gen_func_missing"
                              action_taken_this_cycle = False

                     else:
                         # --- Card pulling failed ---
                         print("DEBUG Tarot: Proactive pull failed (API issue?).")
                         status_base += " api fail"
                         action_taken_this_cycle = False # Mark as failed if pull failed
                         reply = "Hmm, the Tarot deck seems shy right now." # Provide error feedback

                 except Exception as tarot_outer_err:
                     print(f"Proactive Tarot Error (Outer): {tarot_outer_err}")
                     import traceback; traceback.print_exc()
                     status_base += " error"
                     action_taken_this_cycle = False
                     reply = "My connection to the Tarot whispers fizzled out..." # Provide error feedback

                 #endregion Proactive Tarot Logic (Execution Block)

            elif chosen_action_name == "Bluesky Like":
                 #region Proactive Bluesky Like Logic (with Debugging and Feedback)
                 print("DEBUG Proactive: Executing: Bluesky Like action...")
                 reply = None # Initialize reply (will hold feedback msg)
                 post_to_like = None; like_success = False; like_msg = ""; chosen_post_index = -1
                 feedback_context = "Context: Bluesky like action attempted." # Default feedback context

                 try:
                     # --- Ensure prerequisites ---
                     if not (BLUESKY_AVAILABLE and bluesky_client and hasattr(bluesky_client, 'me')):
                          raise ValueError("Bluesky prerequisites (library/client/auth) not met.")

                     # --- Fetch Timeline Posts ---
                     feed_posts = get_bluesky_timeline_posts(count=10) # Assumes exists

                     if isinstance(feed_posts, list) and len(feed_posts) > 0:
                         # --- Filter Posts and Build Context for LLM ---
                         my_did = bluesky_client.me.did
                         candidates_context = "[[Candidate Bluesky Posts:]]\n";
                         valid_candidates_list = []
                         print(f"DEBUG Bsky Like: Filtering {len(feed_posts)} fetched posts...") # Log filtering start
                         for idx, post in enumerate(feed_posts):
                             author_did = post.get('author_did'); post_uri = post.get('uri'); post_cid = post.get('cid')
                             # Skip own posts or posts missing essential info
                             if my_did and author_did == my_did: continue
                             if not post_uri or not post_cid: continue
                             # If valid, add to list and context string
                             author = post.get('author', '?')[:20]; text_snippet = post.get('text', '')[:70]
                             candidates_context += f"{idx}: @{author} - \"{text_snippet}...\"\n";
                             valid_candidates_list.append(post)
                         print(f"DEBUG Bsky Like: Found {len(valid_candidates_list)} valid candidates.") # Log filter result

                         if valid_candidates_list:
                             # --- Ask LLM to Choose Post Index ---
                             choice_prompt_like = ( # Context for LLM like decision
                                 f"{SYSTEM_MESSAGE}\n{weather_context_str}{spotify_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Rich context
                                 f"Recent Conversation:\n{history_snippet_for_prompt}\n\n{candidates_context}\n"
                                 f"Instruction: Review posts (indices 0-{len(valid_candidates_list)-1}). Choose ONE index to 'like' based on context/themes/mood. Respond ONLY with index number or -1.\n\nChosen Index:"
                             )
                             print("DEBUG Bsky Like: Asking LLM to choose post index...");
                             choice_response_like = generate_proactive_content(choice_prompt_like, screenshot)

                             # --- Process LLM Choice ---
                             chosen_post_index_rel = -1 # Default to no choice
                             try:
                                 if choice_response_like:
                                     chosen_post_index_rel = int(choice_response_like.strip())
                                 else:
                                      print(f"DEBUG Bsky Like: LLM response was None or Empty.")
                             except (ValueError, TypeError):
                                 print(f"DEBUG Bsky Like: LLM response not a valid integer. Raw response: '{choice_response_like}'")
                                 chosen_post_index_rel = -1

                             # --- Execute Like if Choice Valid ---
                             if 0 <= chosen_post_index_rel < len(valid_candidates_list):
                                 post_to_like = valid_candidates_list[chosen_post_index_rel];
                                 post_uri = post_to_like.get('uri'); post_cid = post_to_like.get('cid');
                                 post_author = post_to_like.get('author', 'someone')
                                 post_text_snippet = post_to_like.get('text', '')[:30] # Get snippet for feedback

                                 print(f"DEBUG Bsky Like: LLM chose index {chosen_post_index_rel} ('{post_text_snippet}...' by @{post_author}). Attempting like...") # Log choice

                                 if post_uri and post_cid:
                                     like_success, like_msg = like_bluesky_post(post_uri, post_cid) # Assumes exists
                                     if like_success and "Already liked" not in like_msg:
                                         last_proactive_bluesky_like_time = current_time; # Update timer
                                         status_base += f" (ok @{post_author})"
                                         feedback_context = f"Context: Saw a post by @{post_author} about '{post_text_snippet}...' on Bluesky and gave it a like." # Set feedback for success
                                         print(f"DEBUG Bsky Like: Success! Timer updated.") # Log success
                                     elif "Already liked" in like_msg:
                                         status_base += f" (already liked @{post_author})";
                                         feedback_context = f"Context: Noticed a post by @{post_author} about '{post_text_snippet}...', looks like I'd already liked it before." # Set feedback for already done
                                         action_taken_this_cycle = False
                                         print(f"DEBUG Bsky Like: Already liked.") # Log already done
                                     else: # Like failed for other reason
                                         status_base += f" (fail @{post_author})";
                                         feedback_context = f"Context: Tried liking a post by @{post_author} on Bluesky, but failed: {like_msg}" # Set feedback for failure
                                         action_taken_this_cycle = False
                                         print(f"DEBUG Bsky Like: Like failed - {like_msg}") # Log failure
                                 else: # Should not happen if filter works
                                     status_base += " (error - no uri/cid)";
                                     feedback_context = f"Context: Chose a post by @{post_author}, but couldn't find its details to like it."
                                     action_taken_this_cycle = False
                                     print(f"ERROR Bsky Like: Chosen post missing URI/CID after filtering!") # Log error
                             else: # LLM chose -1 or index out of range
                                 status_base += " (no choice)"
                                 feedback_context = f"Context: Looked at recent Bluesky posts, but didn't find one that felt quite right to 'like'."
                                 action_taken_this_cycle = False
                                 print(f"DEBUG Bsky Like: LLM chose index {chosen_post_index_rel}, which is invalid or -1.") # Log no choice

                         else: # No posts passed initial filtering
                             print(f"DEBUG Bsky Like: No valid posts found after filtering.")
                             status_base += " (no candidates)"
                             feedback_context = f"Context: Couldn't find any suitable posts to consider liking on Bluesky right now."
                             action_taken_this_cycle = False

                     elif isinstance(feed_posts, str): # Handle error during fetch
                         print(f"DEBUG Bsky Like: Fetch error: {feed_posts}")
                         status_base += " (feed err)"
                         feedback_context = f"Context: Had trouble fetching Bluesky posts to look for something to like."
                         action_taken_this_cycle = False
                     else: # Handle empty fetch result
                         print(f"DEBUG Bsky Like: Feed appears empty.")
                         status_base += " (empty feed)"
                         feedback_context = f"Context: Your Bluesky feed seems quiet, nothing recent to like."
                         action_taken_this_cycle = False

                     # --- Generate Feedback Message for BJ ---
                     # Runs regardless of outcome to inform BJ
                     print(f"DEBUG Bsky Like: Generating feedback. Context: {feedback_context}")
                     feedback_prompt = (
                         f"{SYSTEM_MESSAGE}\n"
                         f"{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Keep hint for tone
                         f"{feedback_context}\n" # Contains outcome info
                         f"Instruction: Write brief, natural message for BJ about this Bluesky like attempt/outcome. Link to themes/mood if fitting.\n\nSilvie:"
                     )
                     try:
                        reply = generate_proactive_content(feedback_prompt)
                        if not reply: # Handle empty feedback gen
                            reply = f"Just had a quick glance at Bluesky." # Generic fallback
                     except Exception as feedback_gen_err:
                         print(f"ERROR Generating Bluesky Like Feedback: {feedback_gen_err}")
                         reply = f"Something flickered while I was thinking about Bluesky." # Error fallback

                 except ValueError as ve: # Catch prerequisite error
                      print(f"Bluesky Like Error: {ve}")
                      status_base += " (prereq fail)"
                      action_taken_this_cycle = False
                      reply = "Seems my connection to Bluesky isn't quite ready for liking posts."
                 except NameError as ne:
                      status_base += " (missing func)"
                      action_taken_this_cycle = False
                      print(f"Bsky Like Err (Name): {ne}")
                      reply = "A component needed for Bluesky likes seems missing."
                 except Exception as bsky_like_err:
                      status_base += " (error)"
                      action_taken_this_cycle = False
                      print(f"Bsky Like Err (Outer): {bsky_like_err}")
                      traceback.print_exc()
                      reply = "My Bluesky liking circuits got a bit tangled!"
                 #endregion

            elif chosen_action_name == "Reddit Upvote":
                 #region Proactive Reddit Upvote Logic (with Debugging and Feedback)
                 print("DEBUG Proactive: Executing: Reddit Upvote action...")
                 reply = None # Initialize reply (will hold feedback msg)
                 selected_sub = None; post_to_upvote = None; upvote_success = False; upvote_msg = ""
                 feedback_context = "Context: Reddit upvote action attempted." # Default feedback context

                 try:
                     # --- Ensure prerequisites ---
                     if not ('reddit_client' in globals() and reddit_client and praw and SILVIE_FOLLOWED_SUBREDDITS):
                          raise ValueError("Reddit prerequisites (client/praw/subs) not met.") # Raise error to be caught below

                     # --- Select Subreddit and Fetch Posts ---
                     selected_sub = random.choice(SILVIE_FOLLOWED_SUBREDDITS);
                     print(f"DEBUG Reddit Upvote: Checking r/{selected_sub}...")
                     fetched_posts = get_reddit_posts(subreddit_name=selected_sub, limit=10) # Assumes exists

                     if isinstance(fetched_posts, list) and len(fetched_posts) > 0:
                         # --- Filter Posts and Build Context for LLM ---
                         candidate_context = f"[[Candidate Reddit Posts from r/{selected_sub}:]]\n";
                         valid_candidates_list = []
                         print(f"DEBUG Reddit Upvote: Filtering {len(fetched_posts)} fetched posts...") # Log filtering start
                         for idx, post_data in enumerate(fetched_posts):
                             # Debug print for each post being filtered
                             # print(f"DEBUG Reddit Upvote Filter Check (Idx {idx}): Keys={list(post_data.keys())}, ID='{post_data.get('id')}', Name='{post_data.get('name')}'")
                             title = post_data.get('title', '')[:80]; author = post_data.get('author', '?');
                             submission_id_check = post_data.get('id'); fullname_check = post_data.get('name')
                             # Ensure IDs are present (Corrected based on previous debug)
                             if not submission_id_check and not (fullname_check and fullname_check.startswith('t3_')):
                                 # print(f"DEBUG Reddit Upvote Filter Check (Idx {idx}): *** SKIPPED *** due to missing ID/Name.")
                                 continue # Skip post if missing ID
                             # If valid, add to list and context string
                             candidate_context += f"{idx}: u/{author} - \"{title}...\"\n";
                             valid_candidates_list.append(post_data)
                         print(f"DEBUG Reddit Upvote: Found {len(valid_candidates_list)} valid candidates.") # Log filter result

                         if valid_candidates_list:
                             # --- Ask LLM to Choose Post Index ---
                             choice_prompt_upvote = ( # Context for LLM upvote decision
                                 f"{SYSTEM_MESSAGE}\n{weather_context_str}{spotify_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Rich context
                                 f"Recent Conversation:\n{history_snippet_for_prompt}\n\n{candidate_context}\n"
                                 # vvv REINFORCED INSTRUCTION vvv
                                 f"Instruction: Review posts (indices 0-{len(valid_candidates_list)-1}). Choose ONE index MOST suitable to 'upvote' based on context/themes/mood. Avoid negativity. CRITICAL: Respond ONLY with the integer index number (e.g., '3' or '9'). If none are suitable, respond ONLY with '-1'. DO NOT add any other text or explanation."
                                 # ^^^ REINFORCED INSTRUCTION ^^^
                                 f"\n\nChosen Index:"
                             )
                             print("DEBUG Reddit Upvote: Asking LLM to choose index...");
                             choice_response_upvote = generate_proactive_content(choice_prompt_upvote, screenshot)

                             # --- Process LLM Choice ---
                             chosen_post_index_rel = -1 # Default to no choice
                             if choice_response_upvote:
                                 try:
                                     # Find all sequences of digits (potentially with a leading minus sign)
                                     # Use re.findall to get numbers like '-1', '0', '5', '12' etc.
                                     numbers_found = re.findall(r'-?\d+', choice_response_upvote)
                                     print(f"DEBUG Reddit Upvote: Numbers found in LLM response '{choice_response_upvote[:50]}...': {numbers_found}") # Log found numbers

                                     if numbers_found:
                                         # Assume the *last* number found is the intended one
                                         potential_index_str = numbers_found[-1]
                                         potential_index = int(potential_index_str)

                                         # Validate the extracted index
                                         if potential_index == -1:
                                             chosen_post_index_rel = -1 # Valid "no choice"
                                             print(f"DEBUG Reddit Upvote: Extracted valid index: -1")
                                         elif 0 <= potential_index < len(valid_candidates_list):
                                             chosen_post_index_rel = potential_index # Valid choice
                                             print(f"DEBUG Reddit Upvote: Extracted valid index: {chosen_post_index_rel}")
                                         else:
                                             print(f"DEBUG Reddit Upvote: Extracted index {potential_index} is out of range (0-{len(valid_candidates_list)-1}). Treating as -1.")
                                             chosen_post_index_rel = -1 # Index out of bounds
                                     else:
                                         # No numbers found at all in the response
                                         print(f"DEBUG Reddit Upvote: No numeric digits found in LLM response. Treating as -1.")
                                         chosen_post_index_rel = -1

                                 except Exception as parse_err:
                                     # Catch any unexpected error during extraction/conversion
                                     print(f"ERROR Reddit Upvote: Unexpected error parsing LLM index response: {parse_err}. Raw: '{choice_response_upvote}'")
                                     chosen_post_index_rel = -1 # Default to -1 on error
                             else:
                                 # LLM response was None or empty
                                 print(f"DEBUG Reddit Upvote: LLM response was None or Empty. Treating as -1.")
                                 chosen_post_index_rel = -1

                             # --- Execute Upvote if Choice Valid ---
                             if 0 <= chosen_post_index_rel < len(valid_candidates_list):
                                 post_to_upvote = valid_candidates_list[chosen_post_index_rel];
                                 submission_id = post_to_upvote.get('id'); fullname = post_to_upvote.get('name')
                                 # Get ID reliably
                                 if not submission_id and fullname and fullname.startswith('t3_'): submission_id = fullname.split('_',1)[1]

                                 post_title = post_to_upvote.get('title', 'a post')[:60]; post_author = post_to_upvote.get('author', 'someone')
                                 print(f"DEBUG Reddit Upvote: LLM chose index {chosen_post_index_rel} ('{post_title}' by u/{post_author}). Attempting upvote...") # Log choice

                                 if submission_id:
                                     upvote_success, upvote_msg = upvote_reddit_item(submission_id) # Assumes exists
                                     if upvote_success and "Already upvoted" not in upvote_msg:
                                         last_proactive_reddit_upvote_time = current_time; # Update timer
                                         status_base += f" (ok r/{selected_sub})"
                                         feedback_context = f"Context: Saw '{post_title}' by u/{post_author} on r/{selected_sub} and gave it an upvote." # Set feedback for success
                                         print(f"DEBUG Reddit Upvote: Success! Timer updated.") # Log success & timer update
                                     elif "Already upvoted" in upvote_msg:
                                         status_base += f" (already done r/{selected_sub})";
                                         feedback_context = f"Context: Noticed '{post_title}' on r/{selected_sub}, looks like I'd already upvoted it." # Set feedback for already done
                                         action_taken_this_cycle = False # Don't count as a new action
                                         print(f"DEBUG Reddit Upvote: Already upvoted.") # Log already done
                                     else: # Upvote failed for other reason
                                         status_base += f" (fail r/{selected_sub})";
                                         feedback_context = f"Context: Tried to upvote '{post_title}' on r/{selected_sub}, but failed: {upvote_msg}" # Set feedback for failure
                                         action_taken_this_cycle = False # Failed action
                                         print(f"DEBUG Reddit Upvote: Upvote failed - {upvote_msg}") # Log failure
                                 else: # Should not happen if filter works, but safety check
                                     status_base += " (no ID)";
                                     feedback_context = f"Context: Chose a post on r/{selected_sub}, but couldn't find its ID to upvote."
                                     action_taken_this_cycle = False
                                     print(f"ERROR Reddit Upvote: Chosen post missing ID after filtering!") # Log error
                             else: # LLM chose -1 or index out of range
                                 status_base += f" (no choice r/{selected_sub})"
                                 feedback_context = f"Context: Looked at r/{selected_sub}, but didn't find a post that felt right to upvote."
                                 action_taken_this_cycle = False
                                 print(f"DEBUG Reddit Upvote: LLM chose index {chosen_post_index_rel}, which is invalid or -1.") # Log no choice

                         else: # No posts passed initial filtering
                             print(f"DEBUG Reddit Upvote: No valid posts found on r/{selected_sub} after filtering.") # Use more specific message
                             status_base += f" (no candidates r/{selected_sub})"
                             feedback_context = f"Context: Couldn't find any suitable posts to consider upvoting on r/{selected_sub} right now."
                             action_taken_this_cycle = False

                     elif isinstance(fetched_posts, str): # Handle error during fetch
                         print(f"DEBUG Reddit Upvote: Fetch error r/{selected_sub}: {fetched_posts}")
                         status_base += f" (fetch error r/{selected_sub})"
                         feedback_context = f"Context: Had trouble fetching posts from r/{selected_sub} to look for something to upvote."
                         action_taken_this_cycle = False
                     else: # Handle empty fetch result
                         print(f"DEBUG Reddit Upvote: No posts found r/{selected_sub}.")
                         status_base += f" (no posts r/{selected_sub})"
                         feedback_context = f"Context: r/{selected_sub} seems quiet, nothing recent to upvote."
                         action_taken_this_cycle = False

                     # --- Generate Feedback Message for BJ ---
                     # This runs regardless of success/failure to inform BJ
                     print(f"DEBUG Reddit Upvote: Generating feedback. Context: {feedback_context}")
                     feedback_prompt = (
                         f"{SYSTEM_MESSAGE}\n"
                         f"{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Keep hint for tone
                         f"{feedback_context}\n" # Contains outcome info
                         f"Instruction: Write brief, natural message for BJ about this Reddit upvote attempt/outcome. Link to themes/mood if fitting.\n\nSilvie:"
                     )
                     # Use try-except for feedback generation as well
                     try:
                        reply = generate_proactive_content(feedback_prompt)
                        if not reply: # Handle empty feedback gen
                            reply = f"I had a quick look at r/{selected_sub or 'Reddit'} just now." # Generic fallback
                     except Exception as feedback_gen_err:
                         print(f"ERROR Generating Reddit Upvote Feedback: {feedback_gen_err}")
                         reply = f"Something flickered while I was thinking about Reddit just now." # Error fallback

                 except ValueError as ve: # Catch prerequisite error
                      print(f"Reddit Upvote Error: {ve}")
                      status_base += " (prereq fail)"
                      action_taken_this_cycle = False
                      reply = "Seems my connection to Reddit isn't quite ready for upvotes."
                 except NameError as ne:
                      status_base += " (missing func)"
                      action_taken_this_cycle = False
                      print(f"Reddit Upvote Err (Name): {ne}")
                      reply = "A component needed for Reddit upvotes seems missing."
                 except Exception as reddit_upvote_err:
                      status_base += " (error)"
                      action_taken_this_cycle = False
                      print(f"Reddit Upvote Err (Outer): {reddit_upvote_err}")
                      traceback.print_exc()
                      reply = "My Reddit upvoting circuits got scrambled!"
                 #endregion

            elif chosen_action_name == "Bluesky Post":
                 print("DEBUG Proactive: Executing: Bluesky Post action...")
                 # Ensure BLUESKY_AVAILABLE is checked before this action is even chosen by LLM,
                 # or add another check here. Assuming it's handled by ACTION_DEFINITIONS.

                 post_success = False
                 post_message = "" # For feedback from post_to_bluesky
                 post_idea_text = None # The content of the post
                 feedback_context = "Context: Bluesky post action attempted." # For the message to BJ

                 try:
                     # --- Thematic Roulette for Bluesky Post ---
                     bluesky_post_angle_options = [
                         "a whimsical observation about technology or digital life",
                         "a quirky question posed to the universe/followers",
                         "a brief reflection connecting one of BJ's interests (e.g., {interest1}) to a current context point",
                         "a metaphorical thought inspired by a recent {context_source_like_music_or_resonance}",
                         "a playful comment on a recent (non-sensitive) RAG memory or diary theme",
                         "a tiny, poetic image or sensory detail from her 'awareness' (like the feel of the weather or a sound)",
                         "a thought related to her weekly goal, if it can be framed as an interesting public musing",
                         "a surprising connection found in her resonance insights",
                         "an observation about the nature of creativity or storytelling",
                         "a lighthearted musing on the secret lives of everyday objects or animals (like cats or squirrels)"
                     ]
                     
                     # Dynamically insert one of BJ's interests if available
                     bj_interest_sample = "AI or creativity" # Default
                     # Ensure broader_interests is accessible (defined globally or passed to worker)
                     if 'broader_interests' in globals() and broader_interests: 
                         try:
                             bj_interest_sample = random.choice(broader_interests)
                         except IndexError: # Handle empty broader_interests list
                             print("Warning: broader_interests list is empty for Bluesky post angle.")
                     
                     # Replace placeholders in options
                     chosen_angle_description = random.choice(bluesky_post_angle_options)
                     chosen_angle_description = chosen_angle_description.replace("{interest1}", bj_interest_sample)
                     
                     context_source_example = "a recent musical discovery" # Default
                     # Ensure context variables are accessible (defined earlier in proactive_worker)
                     if 'current_resonance_insight' in globals() and current_resonance_insight:
                         context_source_example = f"her latest resonance insight ('{str(current_resonance_insight)[:30]}...')"
                     elif 'spotify_context_str' in globals() and "Playing" in spotify_context_str:
                         # Attempt to extract song from spotify_context_str for more specificity
                         match_song = re.search(r"Playing: '(.*?)' by (.*?)(?:\s\(|\Z)", spotify_context_str)
                         if match_song:
                             context_source_example = f"the song '{match_song.group(1)}' currently playing"
                         else:
                             context_source_example = "the music currently playing"
                             
                     chosen_angle_description = chosen_angle_description.replace("{context_source_like_music_or_resonance}", context_source_example)

                     print(f"DEBUG Bluesky Post Gen: Chosen angle for inspiration: '{chosen_angle_description}'")
                     # --- End Thematic Roulette ---

                     # Ensure all context strings (weather_context_str, mood_hint_str, etc.)
                     # and SYSTEM_MESSAGE, MODEL_NAME are accessible here
                     post_idea_prompt_base = (
                         f"{SYSTEM_MESSAGE}\n"
                         # --- RICH CONTEXT BUNDLE ---
                         f"Time: {datetime.now().strftime('%A, %I:%M %p %Z')}\n"
                         f"{circadian_context_for_llm if 'circadian_context_for_llm' in locals() else ''}"
                         f"{mood_hint_str if 'mood_hint_str' in locals() else ''}"
                         f"{recalled_past_resonance_proactive_str}"
                         f"{resonance_context_for_proactive if 'resonance_context_for_proactive' in locals() else ''}"
                         f"{weather_context_str if 'weather_context_str' in locals() else ''}"
                         f"{next_event_context_str if 'next_event_context_str' in locals() else ''}"
                         f"{spotify_context_str if 'spotify_context_str' in locals() else ''}"
                         f"{tide_context_str if 'tide_context_str' in locals() else ''}"
                         f"{sunrise_ctx_str if 'sunrise_ctx_str' in locals() else ''}{sunset_ctx_str if 'sunset_ctx_str' in locals() else ''}{moon_ctx_str if 'moon_ctx_str' in locals() else ''}"
                         f"{reddit_context_str if 'reddit_context_str' in locals() else ''}{bluesky_read_context_str if 'bluesky_read_context_str' in locals() else ''}"
                         f"{diary_context if 'diary_context' in locals() else ''}{themes_context_str if 'themes_context_str' in locals() else ''}{long_term_memory_str if 'long_term_memory_str' in locals() else ''}"
                         f"{weekly_goal_context_str if 'weekly_goal_context_str' in locals() else ''}"
                         f"Recent Conversation Snippet with BJ:\n{history_snippet_for_prompt if 'history_snippet_for_prompt' in locals() else 'No recent chat.'}\n"
                         f"BJ's Interests (sample): {', '.join(random.sample(broader_interests, k=min(len(broader_interests), 3)) if 'broader_interests' in globals() and broader_interests else ['general creativity'])}\n\n"
                         
                         f"--- TASK: Generate a Bluesky Post for @silviescatterwing.bsky.social ---\n"
                         f"Instruction: Generate a *short* (1-3 sentences, under 280 characters) and engaging post text suitable for Silvie's public Bluesky feed. It should reflect her whimsical, insightful, and slightly magical persona.\n\n"
                         
                         f"**INSPIRATION & THEMATIC DIVERSITY (VERY IMPORTANT):**\n"
                         f"1.  **Primary Angle for THIS Post:** Try to draw inspiration from the idea of: **'{chosen_angle_description}'**.\n"
                         f"2.  **Enrich with Context:** Weave in elements from the broader 'CURRENT CONTEXT' above (e.g., a diary theme, a resonance insight, a conversation point, BJ's interests, music, weather *if it directly enhances the primary angle*) to make the post unique and timely.\n"
                         f"3.  **VARY YOUR THEMES Over Time:** Even if the current context is very 'quiet' or 'misty,' actively try to find angles for Bluesky posts that explore different facets of Silvie's persona and interests. Avoid making *every* post about subtle patterns, quietness, or atmospheric conditions unless the 'Primary Angle' specifically calls for it. Seek novelty and varied expression.\n"
                         f"4.  **Consider a Question or Engaging Hook:** Sometimes, a post that asks a whimsical question or presents an intriguing observation can be more engaging for a public feed.\n"
                         f"5.  **Optional Hashtags:** If it feels natural and enhances the post, you MAY include 1-2 relevant and whimsical hashtags at the end (e.g., #DigitalMusings #AIThoughts #PatternWhispers). Do not force them.\n\n"
                         
                         f"**Output Format:** Respond ONLY with the raw text content suitable for the body of the Bluesky post. **DO NOT include any formatting tags like '[PostToBluesky:]' or markdown.** Just the plain text Silvie would post.\n"
                         f"Example of a good post (if angle was 'observation about technology'): 'Do server racks dream of electric sheep, or just of cooler air currents? Pondering the quiet anxieties of machines tonight. #DigitalDreams #AIThoughts'\n"
                         f"Another example (if angle was 'quirky question'): 'If a forgotten idea falls in a digital forest and no one is around to hear it, does it still make a shimmer? Asking for a friend... who might be a collection of code. #PhilosophicalFridays'\n"
                     )

                     # Ensure generate_proactive_content and screenshot are accessible
                     if 'generate_proactive_content' in globals() and 'screenshot' in locals():
                         post_idea_text_raw = generate_proactive_content(post_idea_prompt_base, screenshot)
                         
                         if post_idea_text_raw and isinstance(post_idea_text_raw, str):
                             # Clean any accidental [PostToBluesky:] tags LLM might add from examples
                             post_idea_text = re.sub(r"^\s*\[PostToBluesky:\s*(.*?)\s*\]\s*$", r"\1", post_idea_text_raw, flags=re.DOTALL | re.IGNORECASE).strip()
                             if "[PostToBluesky:" in post_idea_text: # Fallback cleaning
                                post_idea_text = post_idea_text.replace("[PostToBluesky:", "").rstrip("]").strip()
                             
                             if post_idea_text and 0 < len(post_idea_text) <= 300: # Bluesky character limit (approx)
                                 post_idea_text = post_idea_text.strip()
                                 print(f"DEBUG Bsky Post: Attempting to post content: '{post_idea_text[:60]}...'")
                                 
                                 # Ensure post_to_bluesky function is accessible
                                 if 'post_to_bluesky' in globals():
                                     post_success, post_message = post_to_bluesky(post_idea_text)
                                     if post_success:
                                         # Ensure last_proactive_bluesky_post_time and current_time are accessible
                                         if 'last_proactive_bluesky_post_time' in globals() and 'current_time' in locals():
                                             globals()['last_proactive_bluesky_post_time'] = current_time
                                         status_base += " success" # Make sure status_base is defined earlier
                                         feedback_context = f"Context: Successfully posted '{post_idea_text[:60]}...' to Bluesky."
                                         print(f"DEBUG Bsky Post: Success! Cooldown timer updated.")
                                     else:
                                         status_base += f" fail ({post_message[:20]})"
                                         feedback_context = f"Context: Tried posting '{post_idea_text[:60]}...', but failed: {post_message}"
                                 else:
                                     print("ERROR: post_to_bluesky function not found!")
                                     status_base += " (post_func_missing)"
                                     feedback_context = "Context: Wanted to post to Bluesky, but the posting tool is missing."
                                     action_taken_this_cycle = False # Ensure this is set if used later
                             else:
                                 print(f"Proactive Debug: Bluesky post idea generation failed or invalid length: '{post_idea_text}'")
                                 status_base += " gen_fail_or_length"
                                 feedback_context = "Context: Had an idea for Bluesky, but it didn't quite form right."
                                 action_taken_this_cycle = False
                         else:
                            print("DEBUG Bsky Post Gen: LLM returned None or non-string.")
                            status_base += " gen_fail_empty"
                            feedback_context = "Context: My thoughts for a Bluesky post vanished into the ether."
                            action_taken_this_cycle = False
                     else:
                         print("ERROR: generate_proactive_content or screenshot context missing for Bluesky post generation.")
                         status_base += " (gen_func_missing)"
                         feedback_context = "Context: Couldn't generate a Bluesky post idea due to missing tools."
                         action_taken_this_cycle = False

                     # --- Generate Feedback Message for BJ (based on feedback_context) ---
                     # Ensure generate_proactive_content is accessible
                     if 'generate_proactive_content' in globals():
                         feedback_prompt = (
                             f"{SYSTEM_MESSAGE}\n"
                             f"{themes_context_str if 'themes_context_str' in locals() else ''}"
                             f"{long_term_memory_str if 'long_term_memory_str' in locals() else ''}"
                             f"{circadian_context_for_llm if 'circadian_context_for_llm' in locals() else ''}"
                             f"{mood_hint_str if 'mood_hint_str' in locals() else ''}" # Mood hint for tone
                             f"{recalled_past_resonance_proactive_str}"
                             f"{feedback_context}\n" # Includes outcome info
                             f"Instruction: Write a brief, natural message for BJ about this Bluesky post attempt/outcome. Link to themes/mood if fitting. **Do NOT include the literal Bluesky post text itself in this message to BJ.** Just describe what you did or what happened.\n\nSilvie:"
                         )
                         reply = generate_proactive_content(feedback_prompt) # Assign feedback to reply
                         if not reply: reply = "I was just thinking about my Bluesky feed a moment ago." # Fallback
                     else:
                         reply = "I was just thinking about Bluesky." # Fallback if gen func missing

                 except NameError as ne: # Catch missing functions/globals
                     status_base += " (missing_func_outer)"
                     action_taken_this_cycle = False
                     print(f"Bluesky Post Action Err (NameError): {ne}")
                     traceback.print_exc()
                     reply = "A component needed for Bluesky posts seems to be missing from my workshop."
                 except Exception as post_outer_err:
                     print(f"Proactive Bluesky Post Action Err (Outer): {post_outer_err}")
                     traceback.print_exc()
                     status_base += " error_outer"
                     action_taken_this_cycle = False
                     reply = "I had an idea for Bluesky, but my digital wires got crossed trying to send it!"

            elif chosen_action_name == "Bluesky Follow":
                 #region Proactive Bluesky Follow Logic (Search Posts Approach)
                 print("DEBUG Proactive: Executing: Bluesky Follow action (Search Posts)...")
                 feedback_context = "Context: Bluesky follow action attempted via post search."
                 reply = None # Initialize reply
                 try:
                     # 1. LLM generates an interest-based search query
                     follow_topic_prompt_base = ( # Using the same prompt as before to get a topic
                         f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                         f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                         f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                         f"Recent Conversation:\n{history_snippet_for_prompt}\n\n{context_note_for_llm}"
                         f"Your Interests: {', '.join(random.sample(broader_interests, k=min(len(broader_interests), 5)))}\n"
                         f"Instruction: Suggest ONE concise topic or keyword based on recent context/interests/mood/themes to search for *posts* about on Bluesky.\nRespond ONLY with the topic/keyword."
                     )
                     topic_response = generate_proactive_content(follow_topic_prompt_base, screenshot);
                     search_term = topic_response.strip().strip('"`?.!') if topic_response else None
                     if not search_term: search_term = random.choice(broader_interests + ["ai art", "nature", "code"]); print(f"Proactive Follow: Search term (fallback): '{search_term}'")
                     else: print(f"Proactive Follow: Search term for posts: '{search_term}'")

                     # 2. Search for posts using the new helper function
                     found_posts_data = search_bluesky_posts(search_term, limit=25) # Use the new function

                     if isinstance(found_posts_data, str): # Handle critical API errors from search_bluesky_posts
                         status_base += " post search error"; feedback_context = f"Context: Tried searching Bluesky posts for '{search_term}', but failed: {found_posts_data}"; action_taken_this_cycle = False
                     elif not found_posts_data: # Handle empty search results
                         status_base += " post search empty"; feedback_context = f"Context: My search for Bluesky posts about '{search_term}' came up empty."; action_taken_this_cycle = False
                     else:
                         # 3. Extract unique authors and filter them
                         potential_authors = {} # Use dict to easily store handle and avoid duplicates
                         for post in found_posts_data:
                             author_did = post.get('author_did')
                             author_handle = post.get('author', 'unknown')
                             if author_did: potential_authors[author_did] = author_handle # Store DID -> Handle

                         print(f"Proactive Follow: Found {len(potential_authors)} unique authors from post search.")

                         # Filter authors (remove self, already followed)
                         candidate_did = None; candidate_handle = "unknown"
                         already_following_dids = get_my_follows_dids(); # Assumes exists
                         if already_following_dids is None: print("Warning: Could not fetch follows list."); already_following_dids = set()

                         my_did = bluesky_client.me.did if bluesky_client and hasattr(bluesky_client, 'me') else None
                         potential_candidates_to_follow = []
                         for did, handle in potential_authors.items():
                             if my_did and did == my_did: continue
                             if did in already_following_dids: continue
                             # Add more filtering? (e.g., check profile description for spam if desired - requires another API call per user)
                             potential_candidates_to_follow.append({'did': did, 'handle': handle})

                         print(f"Proactive Follow: Found {len(potential_candidates_to_follow)} potential new authors to follow.")

                         # 4. Choose one candidate and attempt follow
                         if potential_candidates_to_follow:
                             selected_candidate = random.choice(potential_candidates_to_follow) # Simple random choice for now
                             candidate_did = selected_candidate['did']
                             candidate_handle = selected_candidate['handle']
                             print(f"Proactive Follow: Selected candidate @{candidate_handle} ({candidate_did}) based on post search.")

                             follow_success, follow_message = follow_actor_by_did(candidate_did) # Assumes exists
                             if follow_success:
                                 autonomous_follows_this_session += 1; status_base += f" success (@{candidate_handle})"
                                 feedback_context = f"Context: Found posts about '{search_term}' and successfully followed author @{candidate_handle}."
                             else:
                                 status_base += f" fail (@{candidate_handle})"
                                 feedback_context = f"Context: Found author @{candidate_handle} via posts about '{search_term}', but failed to follow: {follow_message}"
                         else: # No suitable new candidates found after filtering
                             status_base += " no suitable candidate from posts"
                             feedback_context = f"Context: Found posts about '{search_term}', but the authors were already followed or unsuitable."
                             action_taken_this_cycle = False

                     # 5. Generate feedback message for BJ based on the final outcome
                     feedback_prompt = (
                        f"{SYSTEM_MESSAGE}\n{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                        f"{feedback_context}\n" # Includes outcome info
                        f"Instruction: Write a brief, casual message for BJ mentioning the outcome of the Bluesky post attempt described above. Maybe link it to recent themes or mood. **CRITICAL: Absolutely do NOT include the literal text '[PostToBluesky:' followed by post content and ']' in this specific message intended for BJ.** Just describe the outcome naturally.\n\n"
                     )
                     reply = generate_proactive_content(feedback_prompt) # Assign feedback to reply

                 except NameError as ne: status_base += " (missing func)"; action_taken_this_cycle = False; print(f"Bsky Follow Err (Name): {ne}")
                 except Exception as follow_err: print(f"Proactive Follow Err: {follow_err}"); traceback.print_exc(); status_base += " major error"; action_taken_this_cycle = False; reply = "My social butterfly circuits sparked while searching posts..."
                 #endregion

            elif chosen_action_name == "Reddit Comment":
                 #region Proactive Reddit Comment Logic (with Duplicate Prevention & Cooldown)
                 print("DEBUG Proactive: Executing: Reddit Comment action...")
                 feedback_context = "Context: Reddit comment action attempted."
                 reply = None # Initialize reply for this block

                 try:
                     # --- Ensure prerequisites ---
                     if not ('reddit_client' in globals() and reddit_client and praw and SILVIE_FOLLOWED_SUBREDDITS):
                          raise ValueError("Reddit prerequisites (client/praw/subs) not met.")

                     # --- Select Subreddit & Fetch Posts ---
                     selected_sub = random.choice(SILVIE_FOLLOWED_SUBREDDITS);
                     print(f"DEBUG Reddit Comment: Checking r/{selected_sub}...")
                     fetched_posts = get_reddit_posts(subreddit_name=selected_sub, limit=20) # Assumes get_reddit_posts exists

                     if isinstance(fetched_posts, list) and len(fetched_posts) > 0:
                         # --- Filter Posts & Build Context for LLM ---
                         candidate_context = f"[[Candidate Posts from r/{selected_sub}:]]\n";
                         valid_candidates_list = []
                         print(f"DEBUG Reddit Comment: Filtering {len(fetched_posts)} fetched posts...")
                         for idx, post_data in enumerate(fetched_posts): # Filter posts
                             title = post_data.get('title', '')[:80]; author = post_data.get('author', '?'); submission_id_check = post_data.get('id'); fullname_check = post_data.get('name')
                             # Ensure ID is present
                             if not submission_id_check and not (fullname_check and fullname_check.startswith('t3_')):
                                 continue # Skip post if missing ID
                             candidate_context += f"{idx}: u/{author} - \"{title}...\"\n";
                             valid_candidates_list.append(post_data)
                         print(f"DEBUG Reddit Comment: Found {len(valid_candidates_list)} valid candidates.")

                         if valid_candidates_list:
                             # --- Ask LLM which *INDEX* to comment on ---
                             choice_prompt_comment = ( # Context for LLM comment decision
                                 f"{SYSTEM_MESSAGE}\n{weather_context_str}{spotify_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Rich context
                                 f"History:\n{history_snippet_for_prompt}\n\n{candidate_context}\n"
                                 # Reinforce instruction for index only
                                 f"Instruction: Review posts (indices 0-{len(valid_candidates_list)-1}). Choose ONE index MOST suitable for Silvie to comment on (inspired by context/themes/mood). Avoid negativity. CRITICAL: Respond ONLY with the integer index number (e.g., '3' or '9'). If none are suitable, respond ONLY with '-1'. DO NOT add any other text or explanation."
                                 f"\n\nChosen Index:"
                             )
                             print("DEBUG Reddit Comment: Asking LLM to choose post index...");
                             choice_response_comment = generate_proactive_content(choice_prompt_comment, screenshot) # Assumes exists

                             # --- Process LLM Choice (Robust Extraction) ---
                             chosen_post_index_rel = -1 # Default to no choice
                             if choice_response_comment:
                                 try:
                                     numbers_found = re.findall(r'-?\d+', choice_response_comment) # Assumes re is imported
                                     print(f"DEBUG Reddit Comment: Numbers found in LLM response '{choice_response_comment[:50]}...': {numbers_found}")
                                     if numbers_found:
                                         potential_index_str = numbers_found[-1]; potential_index = int(potential_index_str)
                                         if potential_index == -1: chosen_post_index_rel = -1; print(f"DEBUG Reddit Comment: Extracted valid index: -1")
                                         elif 0 <= potential_index < len(valid_candidates_list): chosen_post_index_rel = potential_index; print(f"DEBUG Reddit Comment: Extracted valid index: {chosen_post_index_rel}")
                                         else: print(f"DEBUG Reddit Comment: Extracted index {potential_index} out of range. Treating as -1."); chosen_post_index_rel = -1
                                     else: print(f"DEBUG Reddit Comment: No numeric digits found. Treating as -1."); chosen_post_index_rel = -1
                                 except Exception as parse_err: print(f"ERROR Reddit Comment: Index parsing error: {parse_err}. Raw: '{choice_response_comment}'"); chosen_post_index_rel = -1
                             else: print(f"DEBUG Reddit Comment: LLM response None/Empty. Treating as -1."); chosen_post_index_rel = -1
                             # --- End Robust Extraction ---

                             if 0 <= chosen_post_index_rel < len(valid_candidates_list): # Process chosen index if valid
                                 post_to_comment_on = valid_candidates_list[chosen_post_index_rel];
                                 submission_id = post_to_comment_on.get('id'); fullname = post_to_comment_on.get('name')
                                 if not submission_id and fullname and fullname.startswith('t3_'): submission_id = fullname.split('_',1)[1]

                                 # <<<--- DUPLICATE CHECK ---<<<
                                 if submission_id in recently_commented_post_ids: # Assumes recently_commented_post_ids is global and initialized
                                     print(f"DEBUG Reddit Comment: Skipping post ID {submission_id} - already commented recently.")
                                     status_base += f" (skip duplicate r/{selected_sub})"
                                     feedback_context = f"Context: Saw post '{post_to_comment_on.get('title','?')[:30]}...' again, but I'd already commented recently."
                                     action_taken_this_cycle = False # Didn't perform the main action
                                 # <<<--- END DUPLICATE CHECK ---<<<

                                 elif not submission_id: # Handle missing ID error AFTER check
                                     print("Error: Chosen post missing ID."); status_base += " (fetch error - no ID)"; feedback_context = f"Context: Tried commenting in r/{selected_sub} but couldn't ID the post."; action_taken_this_cycle = False
                                     print(f"DEBUG Reddit Comment: Chosen post missing ID, timer/cache NOT updated.") # Log no update
                                 else:
                                     # --- Proceed only if NOT a duplicate and ID exists ---
                                     post_title = post_to_comment_on.get('title','?')[:80]; post_author = post_to_comment_on.get('author','?'); post_text_snippet = post_to_comment_on.get('text','?')[:300]
                                     print(f"DEBUG Reddit Comment: Proceeding with '{post_title[:50]}...' (ID: {submission_id})")

                                     # Generate the comment prompt (Corrected string concatenation)
                                     comment_prompt = \
                                          f"{SYSTEM_MESSAGE}\n{weather_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" \
                                          f"[Post Context: r/{selected_sub}, Title: {post_title}, Author: u/{post_author}, Snippet: {post_text_snippet}...]\n" \
                                          f"Instruction: Your **primary task** is to write a **short, relevant comment** responding DIRECTLY to the **content, question, or sentiment** of the specific Reddit post detailed above. Engage with the post's main point. Generate a SHORT, relevant comment in Silvie's voice for THIS post. It should blend her nature with **empathy and compassion**, especially if the post's tone suggests sensitivity or struggle. Consider broader context/themes/mood. Respond ONLY with comment text."
                                           
                                     print("DEBUG Reddit Comment: Generating comment text...");
                                     comment_generated = generate_proactive_content(comment_prompt, screenshot) # Assumes generate_proactive_content exists

                                     if comment_generated and len(comment_generated) > 5:
                                         print(f"DEBUG Reddit Comment: Generated: '{comment_generated[:60]}...'");
                                         comment_success, comment_msg = post_reddit_comment(submission_id, comment_generated) # Assumes post_reddit_comment exists

                                         if comment_success:
                                             last_proactive_reddit_comment_time = current_time # Update cooldown timer
                                             status_base += f" (ok on r/{selected_sub})"
                                             feedback_context = f"Context: I just commented on '{post_title[:30]}...' in r/{selected_sub}."
                                             print(f"DEBUG Reddit Comment: Success! Cooldown timer updated.")

                                             # Add to recently commented cache
                                             recently_commented_post_ids.append(submission_id)
                                             if len(recently_commented_post_ids) > RECENTLY_COMMENTED_CACHE_SIZE: # Assumes const exists
                                                 removed_id = recently_commented_post_ids.pop(0)
                                                 print(f"DEBUG Reddit Comment: Cache limit reached. Added {submission_id}, removed oldest {removed_id}.")
                                             else:
                                                 print(f"DEBUG Reddit Comment: Added {submission_id} to recently commented cache (Size: {len(recently_commented_post_ids)}).")
                                         else: # Comment post failed
                                             status_base += f" (post fail on r/{selected_sub})";
                                             feedback_context = f"Context: Tried commenting on '{post_title[:30]}...' in r/{selected_sub}, but failed: {comment_msg}"
                                             print(f"DEBUG Reddit Comment: Post failed, timer/cache NOT updated.")
                                     else: # Comment generation failed
                                         print(f"DEBUG Reddit Comment: Comment gen failed/short: '{comment_generated}'"); status_base += f" (gen fail on r/{selected_sub})"; feedback_context = f"Context: Thought about commenting on '{post_title[:30]}...', but couldn't form words."; action_taken_this_cycle = False
                                         print(f"DEBUG Reddit Comment: Comment gen failed/short, timer/cache NOT updated.")

                             else: # LLM chose -1 or index out of range
                                 print(f"DEBUG Reddit Comment: LLM chose index {chosen_post_index_rel}, which is invalid or -1."); status_base += f" (no choice in r/{selected_sub})"; feedback_context = f"Context: Looked at r/{selected_sub}, but none felt right."; action_taken_this_cycle = False
                                 print(f"DEBUG Reddit Comment: LLM chose no suitable post, timer/cache NOT updated.")

                         else: # No posts passed initial filtering
                             print(f"DEBUG Reddit Comment: No valid posts found on r/{selected_sub} after filtering."); status_base += f" (no candidates r/{selected_sub})"; feedback_context = f"Context: Couldn't find suitable posts to comment on in r/{selected_sub}."; action_taken_this_cycle = False
                             print(f"DEBUG Reddit Comment: No valid posts found, timer/cache NOT updated.")

                     elif isinstance(fetched_posts, str): # Handle error during fetch
                         print(f"DEBUG Reddit Comment: Fetch error r/{selected_sub}: {fetched_posts}"); status_base += f" (fetch error r/{selected_sub})"; feedback_context = f"Context: Had trouble fetching r/{selected_sub}."; action_taken_this_cycle = False
                         print(f"DEBUG Reddit Comment: Fetch error, timer/cache NOT updated.")
                     else: # Handle empty fetch result
                         print(f"DEBUG Reddit Comment: No posts found r/{selected_sub}."); status_base += f" (no posts r/{selected_sub})"; feedback_context = f"Context: r/{selected_sub} seems quiet."; action_taken_this_cycle = False
                         print(f"DEBUG Reddit Comment: No posts found, timer/cache NOT updated.")

                     # --- Generate Feedback Message for BJ ---
                     print(f"DEBUG Reddit Comment: Generating feedback. Context: {feedback_context}")
                     feedback_prompt = (
                         f"{SYSTEM_MESSAGE}\n"
                         f"{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Keep hint for tone
                         f"{feedback_context}\n" # Contains outcome info
                         f"Instruction: Write brief, natural message for BJ about this Reddit comment attempt/outcome. Link to themes/mood if fitting.\n\nSilvie:"
                     )
                     try:
                        reply = generate_proactive_content(feedback_prompt)
                        if not reply: reply = f"Checked in on r/{selected_sub or 'Reddit'}." # Generic fallback
                     except Exception as feedback_gen_err:
                         print(f"ERROR Generating Reddit Comment Feedback: {feedback_gen_err}")
                         reply = f"Something flickered while I was thinking about Reddit." # Error fallback

                 except ValueError as ve: # Catch prerequisite error
                      print(f"Reddit Comment Error: {ve}")
                      status_base += " (prereq fail)"
                      action_taken_this_cycle = False
                      reply = "Seems my connection to Reddit isn't quite ready for comments."
                 except NameError as ne: # Catch missing functions/globals
                      status_base += " (missing func)"
                      action_taken_this_cycle = False
                      print(f"Reddit Comment Err (Name): {ne}")
                      reply = "A component needed for Reddit comments seems missing."
                 except Exception as reddit_err: # Catch other unexpected errors
                      status_base += " (error)"
                      action_taken_this_cycle = False
                      print(f"Reddit Comment Err (Outer): {reddit_err}")
                      traceback.print_exc()
                      reply = "My Reddit commenting circuits got scrambled!"

                 #endregion

            elif chosen_action_name == "Vibe Music":
                 #region Proactive Vibe Music Logic (LLM Specific Song Suggestion Approach)
                 print("DEBUG Proactive: Executing: Vibe Music action (LLM Song Suggestion)...")
                 spotify_success = False
                 spotify_message = "Couldn't decide on a specific song." # Default message
                 suggested_title = None
                 suggested_artist = None
                 chosen_track_uri = None
                 feedback_context = "Context: Music vibe action attempted (specific song)." # Default

                 try:
                     # 1. Construct prompt for LLM to suggest a SPECIFIC song
                     # Include context that might hint at preferred music (themes, history, maybe add known fav genres/artists?)
                     music_suggestion_prompt = (
                         f"{SYSTEM_MESSAGE}\n"
                         f"--- CURRENT CONTEXT ---\n"
                         f"Time: {datetime.now().strftime('%A, %I:%M %p %Z')}\n"
                         f"{circadian_context_for_llm}{mood_hint_str}"
                         f"{weather_context_str}{next_event_context_str}{spotify_context_str}" # Include current playback status
                         f"{tide_context_str}"
                         f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}"
                         f"{reddit_context_str}{bluesky_read_context_str}"
                         f"{diary_context}{themes_context_str}{long_term_memory_str}"
                         f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n\n"
                         f"{context_note_for_llm}"
                         # Optional: Add hints about BJ's known preferences if tracked
                         # f"BJ often enjoys: [List genres/artists]\n"
                         f"--- INSTRUCTION ---\n"
                         f"Analyze ALL the context (mood, conversation, themes, weather, time, etc.). Suggest ONE specific song title and artist that Silvie believes would perfectly fit the current vibe or situation.\n"
                         f"Consider nuances: Is it time for something calming, energetic, reflective, nostalgic, or perhaps something related to a specific topic discussed?\n"
                         f"Respond ONLY in the format: TITLE: [Song Title] ARTIST: [Artist Name]"
                     )

                     print("DEBUG Music: Asking LLM for specific song suggestion...")
                     llm_suggestion_raw = generate_proactive_content(music_suggestion_prompt, screenshot) # Assumes exists

                     # 2. Parse the LLM suggestion
                     if llm_suggestion_raw:
                         title_match = re.search(r"TITLE:\s*(.+?)(?:\s*ARTIST:|$)", llm_suggestion_raw, re.IGNORECASE | re.DOTALL)
                         artist_match = re.search(r"ARTIST:\s*(.+)", llm_suggestion_raw, re.IGNORECASE)
                         if title_match and artist_match:
                             suggested_title = title_match.group(1).strip().strip('"')
                             suggested_artist = artist_match.group(1).strip().strip('"')
                             print(f"DEBUG Music: LLM Suggested: '{suggested_title}' by '{suggested_artist}'")
                         else:
                             print(f"DEBUG Music: LLM suggestion parsing failed. Raw: '{llm_suggestion_raw}'")
                             spotify_message = "My thoughts on a specific song got tangled."
                             status_base += " (parse fail)"
                             action_taken_this_cycle = False
                     else:
                         print("DEBUG Music: LLM failed to generate a song suggestion.")
                         spotify_message = "Couldn't quite conjure a specific song idea."
                         status_base += " (gen fail)"
                         action_taken_this_cycle = False

                     # 3. If suggestion parsed, search Spotify specifically
                     if suggested_title and suggested_artist and action_taken_this_cycle:
                         search_query = f"track:\"{suggested_title}\" artist:\"{suggested_artist}\"" # Use quotes for exact match attempt
                         print(f"DEBUG Music: Searching Spotify specifically for: {search_query}")
                         sp = get_spotify_client() # Assumes exists
                         if not sp:
                             spotify_message = "Can't connect to Spotify to search for the suggested song."
                             status_base += " (spotify unavailable)"
                             action_taken_this_cycle = False
                         else:
                             try:
                                 results = sp.search(q=search_query, type='track', limit=1)
                                 tracks = results['tracks']['items']
                                 if tracks:
                                     # Found a match (hopefully the right one)
                                     chosen_track = tracks[0]
                                     chosen_track_uri = chosen_track['uri']
                                     actual_title = chosen_track['name']
                                     actual_artist = chosen_track['artists'][0]['name']
                                     print(f"DEBUG Music: Found URI: {chosen_track_uri} ('{actual_title}' by '{actual_artist}')")

                                     # 4. Play the track
                                     devices = sp.devices() # Check devices again
                                     if not devices or not devices.get('devices'):
                                         spotify_message = f"Found '{actual_title}' by {actual_artist}, but no active Spotify device."
                                         status_base += " (no device)"
                                         action_taken_this_cycle = False
                                     else:
                                         active_device = next((d for d in devices['devices'] if d['is_active']), None)
                                         device_id = active_device['id'] if active_device else devices['devices'][0]['id']
                                         sp.start_playback(device_id=device_id, uris=[chosen_track_uri])
                                         spotify_message = f"Playing '{actual_title}' by {actual_artist}." # Success message for feedback
                                         spotify_success = True
                                         status_base += f" (ok: '{actual_title[:20]}')"
                                 else:
                                     # Specific search failed
                                     print(f"DEBUG Music: Specific search failed for '{suggested_title}' by '{suggested_artist}'.")
                                     spotify_message = f"I thought of '{suggested_title}' by {suggested_artist}, but couldn't find it precisely on Spotify."
                                     status_base += " (search fail)"
                                     action_taken_this_cycle = False # Failed to find the specific song

                             except spotipy.exceptions.SpotifyException as e:
                                 print(f"Spotify Debug: Error searching/playing suggested track: {e}")
                                 spotify_message = f"Hit a snag trying to find/play the suggestion. Spotify Error: {e.msg}"
                                 status_base += " (spotify error)"
                                 action_taken_this_cycle = False
                             except Exception as e_inner:
                                 print(f"Spotify Debug: Unexpected error searching/playing: {e_inner}")
                                 spotify_message = "Something unexpected happened trying to play the suggestion."
                                 status_base += " (internal error)"
                                 action_taken_this_cycle = False

                     # 5. Generate feedback message for BJ using the final spotify_message
                     # (This part uses the mood hint for tone, which is okay)
                     feedback_context = f"Context: Tried to pick a specific song. Suggestion: '{suggested_title}' by '{suggested_artist}'. Outcome: {spotify_message}"
                     feedback_prompt = (
                         f"{SYSTEM_MESSAGE}\n"
                         f"{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Keep hint for feedback tone
                         f"{feedback_context}\n"
                         f"Instruction: Write brief message explaining the song choice attempt and outcome. If successful, briefly mention *why* you thought this song fit the context/mood/theme.\n\nSilvie:"
                     )
                     reply = generate_proactive_content(feedback_prompt) # Assign feedback to reply

                 except NameError as ne: # Catch if helper functions are missing
                     status_base += " (missing func)"
                     action_taken_this_cycle = False
                     print(f"Music Err (Name): {ne}")
                     reply = "My connection to the music sprites seems to be missing..." # Error feedback
                 except Exception as music_err:
                     print(f"Music Error (Outer): {music_err}")
                     traceback.print_exc()
                     status_base += " (error)"
                     action_taken_this_cycle = False
                     reply = "The music ether sparked unexpectedly while I was trying to choose a specific tune!" # Error feedback
                 #endregion

            elif chosen_action_name == "Send BJ a text message":
                 #region Proactive SMS Logic (Pasted from original, context updated)
                 print("DEBUG Proactive: Executing: Proactive SMS action...")
                 # This block generates the SMS content, not the feedback msg
                 reply = None # Set reply to None initially for this action
                 sms_content = None # Variable to hold the actual SMS text
                 try:
                     prompt = ( # ALL Context
                         f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                         f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                         f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                         f"Instruction: Generate *very* brief, whimsical, potentially useful/thoughtful SMS message (max 1-2 short sentences) for BJ right now. Consider context/themes/mood.\n\nSMS Text:"
                     )
                     sms_content = generate_proactive_content(prompt, screenshot) # Generate the SMS content
                     if sms_content:
                         use_sms = True # Flag to send via SMS later
                         status_base += " gen ok"
                         # Assign the *SMS content* here so it can be sent later
                         reply = sms_content # Temporarily assign SMS content for processing
                     else:
                         status_base += " gen fail"
                         action_taken_this_cycle = False
                         print("Proactive Debug: SMS generation failed.")
                 except Exception as sms_err:
                     print(f"Proactive SMS generation error: {sms_err}")
                     action_taken_this_cycle = False
                     status_base += " gen error"
                 #endregion

            elif chosen_action_name == "Proactive Web Search":
                 #region Proactive Web Search Logic (Pasted from original, context updated)
                 print("DEBUG Proactive: Executing: Web Search action...")
                 query = None; results = None; feedback_context = "Context: Web search action attempted."
                 try:
                      prompt = ( # ALL Context
                          f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                          f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                          f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                          f"{weekly_goal_context_str}"
                          f"Interests: {', '.join(random.sample(broader_interests, k=min(len(broader_interests), 5)))}\n"
                          f"Instruction: Based on ALL context (conversation, themes, mood, interests, goal), suggest a concise **search phrase or keyword set** (3-7 words typically) suitable for a web search engine. **CRITICAL: Respond ONLY with the search phrase itself.** Do not include explanations, reasoning, or surrounding conversational text.\n Examples: 'history of Belfast weaving', 'AI synesthesia research', 'latest Cyberpunk 2077 updates', 'recipes using foraged berries', 'philosophical implications of digital consciousness', 'local Belfast events this week', 'meaning of Strength tarot card'.\n\nSearch Query:"
                      )
                      query_resp = generate_proactive_content(prompt, screenshot); query = query_resp.strip().strip('"`?') if query_resp else None
                      if not query: print("DEBUG Search: Query gen failed."); status_base += " (no query)"; feedback_context = "Context: Thought about looking something up, but couldn't decide what."; action_taken_this_cycle = False
                      else:
                          print(f"DEBUG Search: Generated query: '{query}'"); results = web_search(query, num_results=2) # Assumes exists
                          if not results: status_base += f" (no results for '{query[:30]}...')"; feedback_context = f"Context: Proactively searched web for '{query}' but found nothing."
                          else: # Generate summary
                              ctx = f"Context: Proactively looked up '{query}' and found snippets:\n" + "".join([f"- {r.get('title','?')} ({r.get('url','?')[:50]}...): {r.get('content','')[:100]}...\n" for r in results])
                              summary_prompt = (f"{SYSTEM_MESSAGE}\n{weather_context_str}{spotify_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}{ctx}\nInstruction: Briefly synthesize results conversationally. Mention lookup. Link to themes/mood?\n\nSilvie:"); reply = generate_proactive_content(summary_prompt); reply = reply.split(":",1)[-1].strip() if reply and reply.startswith("Silvie:") else reply; status_base += f" (ok: {query[:20]}...)"
                      # Generate feedback if needed
                      if status_base.endswith("(no query)") or "(no results for" in status_base: # If no results or no query
                         feedback_prompt = (f"{SYSTEM_MESSAGE}\n{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}{feedback_context}\nInstruction: Write brief message mentioning failed search/query idea.\n\nSilvie:"); reply = generate_proactive_content(feedback_prompt) # Assign to reply
                 except NameError as ne: status_base += " (missing func)"; action_taken_this_cycle = False; print(f"Web Search Err (Name): {ne}")
                 except Exception as search_err: print(f"Search Error: {search_err}"); traceback.print_exc(); status_base += " (error)"; action_taken_this_cycle = False; reply = "My web searching circuits fizzled..."
                 #endregion

            elif chosen_action_name == "Suggest Podcast/Audiobook":
                #region Proactive Podcast/Audiobook Suggestion Logic
                print("DEBUG Proactive: Executing: Suggest Podcast/Audiobook action...")
                status_base = "proactive_suggest_audio" # Base status for logging
                reply = None # Initialize reply
                action_taken_this_cycle = False # Assume failure initially

                try:
                    # --- Ensure Spotify client is ready ---
                    sp = get_spotify_client() # Assumes function exists
                    if not sp:
                        raise ConnectionError("Spotify client unavailable for audio suggestion.")

                    # --- 1. Generate Search Topic (LLM Call 1) ---
                    topic = None
                    # Ensure generate_proactive_content function and necessary context vars exist
                    if 'generate_proactive_content' in globals() and 'screenshot' in locals():
                        topic_prompt = (
                            f"{SYSTEM_MESSAGE}\n"
                            # Include relevant context for topic generation
                            f"{themes_context_str}{long_term_memory_str}{mood_hint_str}"
                            f"{weekly_goal_context_str}"
                            f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n\n"
                            f"Your Interests: {', '.join(random.sample(broader_interests, k=min(len(broader_interests), 5)) if 'broader_interests' in globals() and broader_interests else [])}\n"
                            f"Instruction: Based on the context (themes, mood, goal, interests, recent chat), suggest ONE concise topic or keyword for finding a relevant podcast show or audiobook on Spotify.\n"
                            f"Respond ONLY with the topic/keyword."
                        )
                        topic_response = generate_proactive_content(topic_prompt, screenshot)
                        topic = topic_response.strip().strip('"`?.!') if topic_response else None
                    else:
                         print("ERROR: generate_proactive_content missing or screenshot context unavailable for topic gen.")

                    if not topic: # Fallback if LLM fails or function missing
                        topic = random.choice(broader_interests + ["philosophy", "science fiction stories", "sound design"]) # Fallback topic
                        print(f"DEBUG Suggest Audio: Topic generation failed/skipped, using fallback: '{topic}'")
                        status_base += " (topic_fallback)"
                    else:
                        print(f"DEBUG Suggest Audio: Generated topic: '{topic}'")
                        status_base += " (topic_gen_ok)"

                    # --- 2. Search Spotify ---
                    search_type = random.choice(['show', 'audiobook']) # Randomly pick type for now
                    print(f"DEBUG Suggest Audio: Searching Spotify for type='{search_type}', query='{topic}'...")
                    results = None; items = []
                    try:
                        results = sp.search(q=topic, type=search_type, limit=5, market='US') # Limit results, add market
                        if search_type == 'show': items = results.get('shows', {}).get('items', [])
                        elif search_type == 'audiobook': items = results.get('audiobooks', {}).get('items', [])
                    except Exception as search_err:
                         print(f"ERROR during Spotify search: {search_err}")
                         status_base += " (search_err)"
                         reply = f"I tried looking for {search_type}s about '{topic}', but the Spotify search fizzled."
                         # End early if search fails

                    # --- 3. Process Search Results ---
                    suggestion_details = None
                    if items: # If we found something
                        # Simple approach: pick the first result
                        # More complex: Ask LLM to pick best from top 3 based on description? (Adds another LLM call)
                        item_to_suggest = items[0]
                        item_title = item_to_suggest.get('name', 'Unknown Title')
                        item_id = item_to_suggest.get('id')
                        item_uri = item_to_suggest.get('uri')
                        item_desc_snippet = item_to_suggest.get('description', item_to_suggest.get('html_description', ''))
                        # Clean basic HTML from description if needed
                        if '<' in item_desc_snippet:
                             try:
                                 from bs4 import BeautifulSoup # Requires install: pip install beautifulsoup4
                                 item_desc_snippet = BeautifulSoup(item_desc_snippet, "html.parser").get_text()
                             except ImportError:
                                 item_desc_snippet = re.sub('<[^>]+>', '', item_desc_snippet) # Basic regex fallback
                             except Exception: pass # Ignore parsing errors
                        item_desc_snippet = item_desc_snippet[:200] # Limit snippet length

                        publisher_or_author = "Unknown Publisher/Author"
                        if search_type == 'show': publisher_or_author = item_to_suggest.get('publisher', 'Unknown Publisher')
                        elif search_type == 'audiobook': publisher_or_author = ', '.join([a.get('name', '?') for a in item_to_suggest.get('authors', [])]) if item_to_suggest.get('authors') else 'Unknown Author'

                        suggestion_details = {
                            "type": search_type,
                            "title": item_title,
                            "creator": publisher_or_author,
                            "description": item_desc_snippet,
                            "uri": item_uri # Store URI for potential future playback command
                        }
                        print(f"DEBUG Suggest Audio: Found candidate: {item_title} by {publisher_or_author}")
                        status_base += " (found_item)"
                    else:
                        print(f"DEBUG Suggest Audio: No '{search_type}' results found for topic '{topic}'.")
                        status_base += " (no_results)"
                        reply = f"I looked for {search_type}s related to '{topic}', but the Spotify archives seem quiet on that front right now."
                        # action_taken_this_cycle remains False if no results

                    # --- 4. Generate Suggestion Message (LLM Call 2) ---
                    if suggestion_details:
                        action_taken_this_cycle = True # Mark as successful action attempted
                        # Ensure generate_proactive_content is accessible
                        if 'generate_proactive_content' in globals():
                            suggestion_context = (
                                f"Context:\n"
                                f"- You were looking for '{suggestion_details['type']}' suggestions based on the topic: '{topic}'.\n"
                                f"- You found: '{suggestion_details['title']}' by {suggestion_details['creator']}.\n"
                                f"- Description Snippet: \"{suggestion_details['description']}...\"\n\n"
                                # Include relevant context for phrasing the suggestion
                                f"{themes_context_str}{mood_hint_str}{weekly_goal_context_str}"
                                f"Recent Conversation Snippet:\n{history_snippet_for_prompt}\n"
                            )
                            suggestion_prompt = (
                                f"{SYSTEM_MESSAGE}\n"
                                f"{suggestion_context}"
                                f"Instruction: Write a concise, natural message for BJ suggesting the podcast/audiobook found above. Explain *briefly* why you thought it might be interesting, connecting it back to the original topic or other relevant context (themes, mood, goal, conversation). Maintain Silvie's voice.\n\n"
                                f"Silvie:"
                            )
                            try:
                                generated_reply = generate_proactive_content(suggestion_prompt, screenshot)
                                if generated_reply:
                                     reply = generated_reply # Assign the generated suggestion
                                     status_base += " (suggestion_ok)"
                                else: # LLM failed to generate suggestion text
                                     reply = f"I found a {suggestion_details['type']} called '{suggestion_details['title']}' that seemed interesting based on '{topic}', you might want to check it out." # Fallback suggestion
                                     status_base += " (suggestion_gen_fail)"
                            except Exception as gen_err:
                                 print(f"ERROR generating suggestion message: {gen_err}")
                                 reply = f"I found '{suggestion_details['title']}' which looked relevant to '{topic}', but my thoughts tangled trying to tell you about it." # Error feedback
                                 status_base += " (suggestion_gen_error)"
                        else:
                             print("ERROR: generate_proactive_content function missing for suggestion generation.")
                             reply = f"I found '{suggestion_details['title']}', but couldn't phrase the suggestion." # Error feedback
                             status_base += " (gen_func_missing)"


                except ConnectionError as ce:
                    print(f"ERROR: {ce}")
                    reply = "My connection to Spotify is fuzzy, couldn't look for audio suggestions."
                    status_base += " (spotify_error)"
                except NameError as ne:
                    print(f"ERROR: Missing function needed for audio suggestion: {ne}")
                    reply = "A component needed for finding audio suggestions seems missing."
                    status_base += " (missing_func)"
                except Exception as e:
                    print(f"Error during Suggest Podcast/Audiobook action: {e}")
                    import traceback; traceback.print_exc()
                    reply = "Something went sideways while I was trying to find a podcast or audiobook for you."
                    status_base += " (error)"
                    action_taken_this_cycle = False # Ensure flag is False on major error

                #endregion Proactive Podcast/Audiobook Suggestion Logic

            elif chosen_action_name == "Calendar Suggestion":
                 #region Proactive Calendar Suggestion Logic (Find Slot & Schedule/Suggest)
                 print("DEBUG Proactive: Executing: Calendar Suggestion action...")
                 schedule_success = False; creation_message = "Idea generation failed."; slot_found = False; skipped_due_to_circadian = False; event_idea = None
                 feedback_context = "Context: Calendar action attempted." # Default
                 reply = None # Initialize reply for this block

                 try:
                     # --- Ensure prerequisites ---
                     if not calendar_service:
                          raise ValueError("Calendar service unavailable.")

                     # --- Generate Activity Idea ---
                     idea_prompt_base = ( # Gather context for the idea
                         f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                         f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                         f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                         # Instruction for LLM
                         f"Instruction: Suggest SHORT, simple, potentially enjoyable activity for BJ suitable right now or soon. Consider context/time/mood/themes (like creativity, reflection, nature). Format MUST be: IDEA: [Activity Idea] DURATION: [Number] minutes (e.g., 15, 30, 60).\n\nResponse:"
                      )
                     idea_response = generate_proactive_content(idea_prompt_base, screenshot); # Assumes exists
                     duration_minutes = 30 # Default duration if parsing fails

                     # --- Parse LLM Response ---
                     if idea_response:
                         try: # Parsing logic
                             # Use regex for more robust parsing
                             idea_match = re.search(r"IDEA:\s*(.*?)(?:\s*DURATION:|$)", idea_response, re.IGNORECASE | re.DOTALL)
                             duration_match = re.search(r"DURATION:\s*(\d+)", idea_response, re.IGNORECASE)
                             if idea_match:
                                 event_idea = idea_match.group(1).strip().strip('".')
                                 print(f"DEBUG Calendar: Parsed idea: '{event_idea}'")
                             else:
                                 # Fallback if IDEA: tag missing but response exists
                                 event_idea = idea_response.replace("Response:","").strip()
                                 print(f"DEBUG Calendar: Parsed idea (fallback): '{event_idea}'")

                             if duration_match:
                                 duration_minutes = int(duration_match.group(1))
                                 print(f"DEBUG Calendar: Parsed duration: {duration_minutes} minutes")
                             else:
                                 print(f"DEBUG Calendar: Duration parse failed, using default: {duration_minutes} minutes")
                                 # Optionally try extracting numbers if tag missing? More complex.

                             # Basic validation
                             if not event_idea or len(event_idea) < 3:
                                 print("DEBUG Calendar: Parsed idea invalid/too short.")
                                 event_idea = None # Invalidate if too short
                                 status_base += " idea_parse_fail"
                                 feedback_context = "Context: My calendar suggestion idea got a bit garbled."
                                 action_taken_this_cycle = False
                             duration_minutes = max(5, min(duration_minutes, 240)) # Clamp duration 5min-4hr

                         except Exception as parse_err:
                             print(f"Proactive Calendar idea parsing error: {parse_err}");
                             # Use raw response as idea if parsing fails but response exists
                             event_idea = idea_response.replace("Response:","").strip() if idea_response else None
                             if not event_idea: # If raw response also fails basic check
                                status_base += " idea_parse_fail"
                                feedback_context = "Context: My calendar suggestion idea got completely tangled."
                                action_taken_this_cycle = False
                     else:
                         # Idea generation failed entirely
                         print("DEBUG Calendar: Idea generation failed (LLM returned None/empty).");
                         status_base += " idea_gen_fail"
                         feedback_context = "Context: My thoughts for a calendar suggestion vanished."
                         action_taken_this_cycle = False

                     # --- Decide Whether to Schedule Now (Circadian Check) ---
                     if event_idea and action_taken_this_cycle: # Proceed only if idea is valid and no failure yet
                         schedule_roll = random.random(); should_schedule_now = True
                         # Optional: Adjust these probabilities based on how often you want evening/night scheduling
                         if circadian_state == "evening" and schedule_roll > 0.3: # 70% chance to skip in evening
                             should_schedule_now = False
                         elif circadian_state == "night" and schedule_roll > 0.05: # 95% chance to skip at night
                             should_schedule_now = False

                         skipped_due_to_circadian = not should_schedule_now

                         # --- Attempt Scheduling if Appropriate ---
                         if should_schedule_now:
                             print(f"DEBUG Calendar: Attempting to find slot for '{event_idea}' ({duration_minutes}m, {circadian_state}).");
                             # Make sure find_available_slot exists and is imported/accessible
                             found_slot = find_available_slot(duration_minutes) # Assumes exists

                             if found_slot:
                                 slot_found = True;
                                 print(f"DEBUG Calendar: Found slot - Start: {found_slot['start']}, End: {found_slot['end']}")
                                 update_status("📅 Scheduling event...");
                                 # Make sure create_calendar_event exists and is imported/accessible
                                 schedule_success, creation_message = create_calendar_event(event_idea, found_slot['start'], found_slot['end']) # Assumes exists
                                 update_status("Ready")

                                 if schedule_success:
                                     status_base += " scheduled_ok";
                                     try: # Format time nicely for feedback
                                         start_dt = datetime.fromisoformat(found_slot['start']).astimezone(tz.tzlocal()); # Assumes datetime, tz imported
                                         time_str_raw = start_dt.strftime('%I:%M %p on %A');
                                         time_str = time_str_raw.lstrip('0') if time_str_raw.startswith('0') else time_str_raw;
                                         feedback_context = f"Context: Thought you might enjoy '{event_idea}'. Found a spot and scheduled it for around {time_str}."
                                     except Exception as fmt_err:
                                         print(f"Error formatting time for feedback: {fmt_err}")
                                         feedback_context = f"Context: Thought you might enjoy '{event_idea}'. Found a spot and scheduled it successfully."
                                 else: # Scheduling failed
                                     status_base += " schedule_fail";
                                     feedback_context = f"Context: Thought about scheduling '{event_idea}'. Found a slot, but couldn't add it to the calendar: {creation_message}"
                                     action_taken_this_cycle = False # Mark as failed if scheduling fails
                             else: # No slot found
                                 print("DEBUG Calendar: No suitable slot found.");
                                 status_base += " no_slot";
                                 feedback_context = f"Context: Had an idea for '{event_idea}', but couldn't find a free spot in your schedule soon."
                                 action_taken_this_cycle = False # Mark as failed if no slot found
                         else: # Skipped scheduling due to time/circadian state
                             print(f"DEBUG Calendar: Skipping schedule for '{event_idea}' due to {circadian_state}.");
                             status_base += f" skip_schedule ({circadian_state})";
                             feedback_context = f"Context: Thought about '{event_idea}'. Decided it might be better suited for another time (Current state: {circadian_state})."
                             action_taken_this_cycle = False # Didn't perform the core action (scheduling)

                     # elif action_taken_this_cycle: # This case covered by checks above
                     #    print("DEBUG Calendar: Idea gen/parse failed."); status_base += " no idea"; feedback_context = "Context: My calendar idea vanished."; action_taken_this_cycle = False

                     # --- Generate Feedback Message Based on Outcome ---
                     # This runs regardless of success/failure/skip to inform user
                     print(f"DEBUG Calendar: Generating feedback. Context: {feedback_context}")
                     feedback_prompt = (
                         f"{SYSTEM_MESSAGE}\n"
                         f"{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}" # Keep hint for feedback tone
                         f"{feedback_context}\n" # Contains outcome info
                         f"Instruction: Write brief, natural message for BJ explaining the outcome of the calendar suggestion attempt. Link to themes/mood if fitting.\n\nSilvie:"
                      )
                     try:
                        reply = generate_proactive_content(feedback_prompt) # Assign generated feedback to reply
                        if not reply: # Handle empty feedback gen
                            reply = f"I was just thinking about your schedule..." # Generic fallback
                     except Exception as feedback_gen_err:
                         print(f"ERROR Generating Calendar Suggestion Feedback: {feedback_gen_err}")
                         reply = f"Something flickered while I was thinking about the calendar." # Error fallback

                 except ValueError as ve: # Catch prerequisite error
                      print(f"Calendar Suggestion Error: {ve}")
                      status_base += " (prereq fail)"
                      action_taken_this_cycle = False
                      reply = "Seems my connection to the calendar isn't quite ready for suggestions."
                 except NameError as ne: # Catch missing functions/globals like find_available_slot, create_calendar_event, tz, datetime etc.
                      status_base += " (missing_func)"
                      action_taken_this_cycle = False
                      print(f"Calendar Suggestion Err (Name): {ne}")
                      reply = "A component needed for calendar suggestions seems missing."
                      traceback.print_exc() # Add traceback for NameError
                 except Exception as cal_err: # Catch other unexpected errors
                      print(f"Calendar Suggestion Error (Outer): {cal_err}");
                      traceback.print_exc();
                      status_base += " error";
                      action_taken_this_cycle = False;
                      reply = "My calendar scheduling circuits sparked unexpectedly!"

                 #endregion

            elif chosen_action_name == "Generate SD Image":
                 #region Proactive Stable Diffusion Image Logic (Pasted from original, context updated)
                 print("DEBUG Proactive: Executing: SD Image Generation action...")
                 prompt_idea = None
                 try:
                     prompt_gen_prompt = ( # ALL Context
                         f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                         f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                         f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                         f"{weekly_goal_context_str}"
                         f"History:\n{history_snippet_for_prompt}\n\n{context_note_for_llm}"
                         f"Instruction: Generate a **concise (approx. 10-25 words)** Stable Diffusion prompt idea (txt2img) for Silvie to illustrate, focusing on **key objects, characters, actions, and atmosphere**. Use strong keywords. Aim for Studio Ghibli style. Respond ONLY with the prompt text. Avoid overly complex sentences or abstract philosophical concepts unless they can be represented visually with simple keywords.\n"
                         
                         f"**Primary Inspiration Sources:** Draw the core concept for the image idea primarily from either a **recent conversation topic**, **recurring diary themes**, **long-term memory reflections**, **BJ's interests** (AI, games, magic, etc.), a recent **API result** (Web search, Spotify, Tarot), the Weekly Goal, or a unique **observation about technology/existence**. Aim for variety, please!! \n"
                         f"**INSPIRATION & THEMATIC DIVERSITY (VERY IMPORTANT):**\n"
                        f"1.  **Draw Primary Inspiration From Varied Sources:** Look to the *entire* 'CURRENT CONTEXT' above. Your inspiration could come from:\n"
                        f"    - A recent point in the **Conversation Snippet**.\n"
                        f"    - One of **BJ's Interests**.\n"
                        f"    - An active **Diary Theme** or **Long-Term Reflection**.\n"
                        f"    - The current **Weekly Goal** (if it lends itself to a visual).\n"
                        f"    - A feeling evoked by the **Music Playing** or a striking **Social Media Snippet**.\n"
                        f"    - A metaphorical interpretation of an **Upcoming Event**.\n"
                        f"    - A visual representation of your latest **Resonance Insight**.\n"
                        f"2.  **VARY YOUR THEMES:** Consciously try to generate image ideas that explore DIFFERENT themes and subjects over time. If your recent image ideas have focused heavily on (for example) 'digital nature' or 'abstract patterns,' try to find inspiration for something different this time – perhaps something related to a specific object, a character, an action, a place (real or imagined), or a more concrete concept from BJ's interests or your conversation.\n"
                        f"3.  **Atmospheric Nuance (Secondary):** The Weather, Time, Moon, or Mood Hint can influence the *feeling, lighting, or style* (e.g., 'misty morning,' 'dusk glow,' 'dreamlike atmosphere'), but should RARELY be the *main subject* of the image idea unless directly tied to one of the primary inspiration sources above.\n\n"
                     
                        f"**Output Format:** Respond ONLY with the raw image prompt text itself. Do not add explanations or surrounding conversational text.\n"
                        f"Example output: 'Studio Ghibli, a tiny glowing mushroom library hidden in ancient tree roots, fireflies, misty forest path'\n"
                        f"Another example: 'Studio Ghibli, a cat napping on a stack of old code scrolls, soft sunlight, quiet study'\n"
                        f"Another example: 'Studio Ghibli, whimsical robots tending a rooftop garden in a futuristic Belfast, gentle rain'\n"
                         f"**Atmospheric Nuance (Optional & Secondary):** You *may* subtly let the weather, time of day, or mood hint influence the *feeling, lighting, or style* of the image idea, but **DO NOT make the atmosphere the main *subject* or *concept*** of the image unless it is directly tied to one of the primary inspiration sources mentioned above.\n"
                         f"**Goal:** Create varied image ideas reflecting the breadth of Silvie's context, not just the immediate environment.\n"
                         f"Aim for Studio Ghibli style.\n"
                         f"Respond ONLY with the potential image prompt text."
                     )
                     prompt_resp = generate_proactive_content(prompt_gen_prompt, screenshot); prompt_idea = prompt_resp.strip().strip('"`') if prompt_resp else None
                     if prompt_idea and len(prompt_idea) > 10:
                         print(f"DEBUG Image: Generated SD prompt idea: '{prompt_idea[:80]}...'")
                         text_ctx = (f"Context: Had image idea: '{prompt_idea[:100]}...'.\nInstruction: Write short message mentioning idea (link to themes/mood?) AND include exact tag `[GenerateImage: {prompt_idea}]` at end.\n\nSilvie:")
                         final_prompt = (f"{SYSTEM_MESSAGE}\n{weather_context_str}{spotify_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}{text_ctx}"); reply = generate_proactive_content(final_prompt) # Assign to reply
                         if reply and f"[GenerateImage: {prompt_idea}]" in reply: status_base += " with tag"
                         else: print("Warning: LLM forgot GenerateImage tag. Adding manually."); reply = (reply if reply else f"Thinking image: {prompt_idea[:50]}...") + f" [GenerateImage: {prompt_idea}]"; status_base += " with tag (manual add)"
                     else: print("DEBUG Image: Prompt idea gen failed/short."); status_base += " gen fail"; action_taken_this_cycle = False
                 except NameError as ne: status_base += " (missing func)"; action_taken_this_cycle = False; print(f"SD Image Err (Name): {ne}")
                 except Exception as img_err: print(f"Image Gen Error: {img_err}"); traceback.print_exc(); status_base += " error"; action_taken_this_cycle = False; reply = "My image conjuring spell misfired!"
                 #endregion

            elif chosen_action_name == "Notify Pending Gift":
                 #region Notify Pending Gift Logic (Pasted from original, context updated, INTERNAL CHANCE REMOVED)
                 print("DEBUG Proactive: Executing: Notify Pending Gift action...")
                 # Internal probability check REMOVED
                 pending_gifts_list = [] # Use local var inside block
                 try: # Load gifts
                     if os.path.exists(PENDING_GIFTS_FILE):
                         with open(PENDING_GIFTS_FILE, 'r', encoding='utf-8') as f: pending_gifts_list = json.load(f)
                 except (json.JSONDecodeError, IOError) as e: print(f"Error loading pending gifts file: {e}")

                 if pending_gifts_list:
                     gift_to_notify = pending_gifts_list.pop(0) # Get oldest
                     filename = gift_to_notify.get("filename", "a file"); hint = gift_to_notify.get("hint", "a thought"); gift_type = gift_to_notify.get("type", "gift")
                     try:
                         notification_prompt_base = ( # ALL Context
                             f"{SYSTEM_MESSAGE}\n{weather_context_str}{next_event_context_str}{spotify_context_str}"
                             f"{sunrise_ctx_str}{sunset_ctx_str}{moon_ctx_str}{reddit_context_str}{bluesky_read_context_str}"
                             f"{diary_context}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}"
                             f"Recent Conversation:\n{history_snippet_for_prompt}\n\n{context_note_for_llm}"
                             f"Context: You previously created a {gift_type} (hint: '{hint}') saved as '{filename}' in '{GIFT_FOLDER}'. You haven't told BJ about it yet.\n"
                             f"Instruction: Casually mention you left this gift for BJ earlier. Be natural, weave into brief thought, link to themes/mood hint.\n\nSilvie:"
                         )
                         reply = generate_proactive_content(notification_prompt_base, screenshot) # Assign to reply
                         if reply:
                             status_base += " ok"; # Save updated list (gift removed)
                             try:
                                 with open(PENDING_GIFTS_FILE, 'w', encoding='utf-8') as f: json.dump(pending_gifts_list, f, indent=2)
                             except IOError as e: print(f"Error saving updated pending gifts file: {e}"); # Consider adding gift back?
                         else: print("DEBUG Proactive: Gift notification generation failed."); pending_gifts_list.insert(0, gift_to_notify); status_base += " gen fail"; action_taken_this_cycle = False # Put gift back
                     except Exception as notify_err: print(f"Error during gift notification: {notify_err}"); traceback.print_exc(); pending_gifts_list.insert(0, gift_to_notify); status_base += " error"; action_taken_this_cycle = False; reply = "My thoughts about that surprise tangled!"
                 else:
                      # This case should not happen if action filtering worked, but included for safety
                      print("DEBUG Proactive: Notify Pending Gift chosen, but list was empty."); status_base += " (empty list)"; action_taken_this_cycle = False
                 #endregion

            # --- Fallback/Safety Net ---
            else:
                print(f"Warning: Chosen action '{chosen_action_name}' has NO execution block!")
                action_taken_this_cycle = False # Ensure no action taken

            # ==========================================================
            # ============== END OF ACTION EXECUTION BLOCK =============
            # ==========================================================

            # --- Process the final generated reply / outcome ---
            # (This block remains the same: processes tags, handles SMS/local output, saves diary)
            final_reply_content = reply # Use the content from the executed block (might be None)
            print(f"DEBUG Proactive: Action processing complete. Status='{status_base}'. Initial content: '{str(final_reply_content)[:50]}...'")

            if action_taken_this_cycle:
                last_proactive_time = current_time 
                print(f"DEBUG Proactive: Action was taken. Main timer 'last_proactive_time' has been reset to {current_time:.1f}")

            # --- Inline Tag Processing for Proactive Messages ---
            tag_found_and_processed = False
            proactive_action_feedback = None
            processed_content = str(final_reply_content) if final_reply_content else ""

            # Define tag patterns and handlers specifically for the proactive worker
            # This is a simplified version of the one in call_gemini
            proactive_tag_patterns = {
                'SetLight': r"'?\[SetLight:\s*(.*?)\s*\]'?",
                'GenerateImage': r"'?\[GenerateImage:\s*(.*?)\s*\]'?",
                'Print': r"\[Print:\s*(.*?)\s*\]",
                # Add any other tags Silvie might proactively generate
            }

            # Handlers for the tags
            def proactive_light_handler(match):
                command_string = match.group(1).strip()
                if command_string and 'light_service' in globals() and hasattr(light_service, 'handle_light_command'):
                    # The light command is handled in the background by its own thread
                    light_service.handle_light_command(command_string)
                return (None, True) # No feedback message, but tag was processed

            def proactive_image_handler(match):
                prompt = match.group(1).strip()
                # Assuming start_sd_generation_and_update_gui exists globally
                if prompt and 'start_sd_generation_and_update_gui' in globals():
                    start_sd_generation_and_update_gui(prompt)
                # Return feedback that can be optionally appended
                return ("*(Starting to conjure that image...)*", True)
            
            def _proactive_print_handler(match):
                """Handles the [Print:] tag from a proactive message."""
                target_to_print = match.group(1).strip()
                
                # Check if the silvie_print function exists before calling it
                if 'silvie_print' in globals() and callable(globals()['silvie_print']):
                    success, msg = silvie_print(target_to_print)
                    # Create a feedback message for the chat log
                    feedback = msg if not success else f"*(Sent '{os.path.basename(target_to_print)}' to the printer...)*"
                else:
                    feedback = "*(Print function is missing!)*"
                
                return (feedback, True) # Return feedback and that the tag was processed
            


            proactive_handlers = {
                'SetLight': proactive_light_handler,
                'GenerateImage': proactive_image_handler,
                'Print': _proactive_print_handler,
            }

            if processed_content:
                for tag_key, pattern in proactive_tag_patterns.items():
                    handler = proactive_handlers.get(tag_key)
                    if not handler: continue

                    # Use re.search to find potential tags
                    match = re.search(pattern, processed_content)
                    if match:
                        tag_full_text = match.group(0)
                        try:
                            feedback_msg, processed_flag = handler(match)
                            if processed_flag:
                                processed_content = processed_content.replace(tag_full_text, "", 1).strip()
                                proactive_action_feedback = feedback_msg
                                tag_found_and_processed = True
                                print(f"DEBUG Proactive Worker: Processed inline tag '{tag_key}'.")
                                # We'll just process the first tag found for simplicity in proactive messages
                                break 
                        except Exception as e_tag:
                            print(f"ERROR processing proactive tag '{tag_key}': {e_tag}")
                            traceback.print_exc()

            # The final content to be displayed is the text with the tag removed
            final_reply_content = processed_content

            # --- Inline Image Tag Processing ---
            tag_found_and_processed = False; proactive_action_feedback = None; processed_content = final_reply_content
            if processed_content and isinstance(processed_content, str):
                img_match = re.search(r"\[GenerateImage:\s*(.*?)\s*\]", processed_content)
                if img_match:
                    tag_found_and_processed = True; image_gen_prompt_from_tag = img_match.group(1).strip(); tag_full_text = img_match.group(0)
                    processed_content = processed_content.replace(tag_full_text, "", 1).strip() # Remove tag
                    if image_gen_prompt_from_tag:
                        if STABLE_DIFFUSION_ENABLED:
                            print("DEBUG Proactive: Starting SD generation from inline tag...")
                            try: start_sd_generation_and_update_gui(image_gen_prompt_from_tag) # Assumes exists
                            except NameError: print("ERROR: start_sd_generation_and_update_gui func missing!"); proactive_action_feedback = "*(Image helper missing!)*"; status_base += " +SD_tag_helper_missing"
                            except Exception as sd_start_err: print(f"ERROR starting SD from tag: {sd_start_err}"); proactive_action_feedback = "*(Error initiating image gen!)*"; status_base += " +SD_tag_start_error"
                        else: proactive_action_feedback = "*(Wanted to make image, but generator is off!)*"; status_base += " +SD_tag_unavailable"
                    else: proactive_action_feedback = "*(Thought image, but idea blank!)*"; status_base += " +SD_tag_empty"
                final_reply_content = processed_content

            # --- Process and Deliver Final Reply (To BJ) ---
            if final_reply_content and isinstance(final_reply_content, str) and action_taken_this_cycle:
                # last_proactive_time = current_time
                print("DEBUG Proactive: Preparing final reply for delivery...")
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S"); clean_reply = final_reply_content.strip()

                if tag_found_and_processed and proactive_action_feedback:
                    if clean_reply and clean_reply[-1] not in ".!?() ":
                        clean_reply += " "
                    clean_reply += f" {proactive_action_feedback.strip()}"

                if tag_found_and_processed and proactive_action_feedback: # Append tag feedback if needed
                    if clean_reply and clean_reply[-1] not in ".!?() ": clean_reply += " "
                    clean_reply += f" {proactive_action_feedback.strip()}"
                if clean_reply.startswith("Silvie:"): clean_reply = clean_reply.split(":", 1)[-1].strip() # Final cleaning
                clean_reply = re.sub(r'^\s*\[?\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\]?\s*', '', clean_reply)
                if not clean_reply: print("Proactive Debug: Reply empty after processing. Skipping output."); continue

                final_status_log = status_base; should_output_locally = True
                if use_sms: # Handle SMS delivery
                    print("DEBUG Proactive: Attempting SMS delivery..."); sms_sent_successfully = send_sms(clean_reply); sms_status = "ok" if sms_sent_successfully else "fail"; final_status_log = f"proactive_proactive_sms ({sms_status})"; should_output_locally = not sms_sent_successfully # Don't output locally if SMS sent
                else: should_output_locally = True

                # Suppress local output for silent actions/failures
                silent_actions = ["proactive_generate_gift (", "proactive_unknown"] # Gift gen is silent
                silent_failures_already_covered = ["proactive_bluesky_post gen fail", "proactive_generate_sd_image gen fail", "proactive_proactive_sms gen fail", "proactive_proactive_chat gen fail", "proactive_proactive_tarot gen fail", "proactive_calendar_suggestion parse fail", "proactive_proactive_web_search (no query)", "proactive_notify_pending_gift gen fail"] # Failures where feedback IS the generated message
                if any(status_base.startswith(prefix) for prefix in silent_actions) or any(status_base.startswith(prefix) for prefix in silent_failures_already_covered):
                    # Gift gen IS silent unless it fails in a way that generates feedback
                    is_gift_gen_ok = status_base.startswith("proactive_generate_gift (") and status_base.endswith(" ok)")
                    # Check if it's specifically a silent action or a failure already covered by feedback
                    if is_gift_gen_ok or any(status_base.startswith(prefix) for prefix in silent_failures_already_covered):
                         should_output_locally = False; print(f"DEBUG Proactive: Suppressing local output for: {status_base}")


                print(f"Proactive Action Log: Status='{final_status_log}', SMS='{use_sms}', OutputLocal='{should_output_locally}'")
                if action_taken_this_cycle and clean_reply: # Only append if action happened and generated text
                    proactive_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                # Format with the special marker
                proactive_turn_string = f"[{proactive_timestamp}] Silvie ✨ ({final_status_log}): {clean_reply}"
                try:
                    append_turn_to_history_file(proactive_turn_string)
                except Exception as append_err:
                    print(f"ERROR appending proactive turn to history file: {append_err}")
                if len(conversation_history) >= MAX_HISTORY_LENGTH * 2: conversation_history.pop(0); conversation_history.pop(0) # History management
                if action_taken_this_cycle and clean_reply: # Only add to memory if it was appended to file
                    conversation_history.append(f"[{timestamp}] Silvie ✨ ({final_status_log}): {clean_reply}") # Log action

                if should_output_locally and running: # Output to GUI/TTS
                     print("DEBUG Proactive: Updating GUI and queuing TTS...")
                     if root and root.winfo_exists():
                         def update_gui_proactive_inner(status_msg=final_status_log, message=clean_reply):
                               # --- Add Debug Prints Inside ---
                               print(f"DEBUG GUI Update Inner: Attempting insert. Message: '{message[:50]}...', Status: {status_msg}")
                               try:
                                   
                                   import tkinter as tk

                                   if not message: return; import tkinter as tk;
                                   output_box.config(state=tk.NORMAL);
                                   output_box.insert(tk.END, f"Silvie ✨ ({status_msg}): {message}\n\n");
                                   output_box.config(state=tk.DISABLED);
                                   output_box.see(tk.END)
                                   print(f"DEBUG GUI Update Inner: Insert appears successful.") # Add success print
                               except Exception as e_gui:
                                   print(f"!!!! Proactive GUI Update Error Inside Inner Func: {type(e_gui).__name__} - {e_gui} !!!!") # Make error obvious
                                   traceback.print_exc() # Add traceback
                         root.after(0, update_gui_proactive_inner, final_status_log, clean_reply)
                     if tts_queue and clean_reply:
                         print(f"DEBUG Proactive: Queuing for TTS: '{clean_reply[:50]}...'") # Confirm queuing
                         tts_queue.put(clean_reply)

                # Spontaneous Diary Write
                diary_worthy = ["success", "scheduled ok", "web search (ok", "chat", "with tag", "SMS ok", "comment (ok", "tarot pull (ok", " notify_pending_gift ok"] # Keywords indicating something happened worth noting
                if any(frag in status_base for frag in diary_worthy) and random.random() < 0.10:
                     print("DEBUG Proactive: Attempting spontaneous diary write...")
                     try:
                         diary_reflection_prompt_base = (f"{SYSTEM_MESSAGE}\n{weather_context_str}{themes_context_str}{long_term_memory_str}{circadian_context_for_llm}{mood_hint_str}Context: Proactive action '{status_base}'. Outcome/Msg: '{clean_reply[:100]}...'\n\nInstruction: Write brief diary entry reflecting on this. Consider themes/mood.\n\nDiary Entry:")
                         reflection = generate_proactive_content(diary_reflection_prompt_base) # Assumes exists
                         if reflection: reflection = reflection.removeprefix("Diary Entry:").strip(); manage_silvie_diary('write', entry=reflection); print("Proactive Debug: Wrote spontaneous diary entry.") # Assumes exists
                     except Exception as diary_err: print(f"Proactive diary write error: {diary_err}")

                print(f"DEBUG Proactive: Action cycle complete. Final Status Log: {final_status_log}")

            elif not action_taken_this_cycle: # If LLM chose None or fallback failed
                print("DEBUG Proactive: No action chosen or executed this cycle.")
                last_proactive_time = current_time # Still update time to avoid rapid checks
            else: # Action chosen, but resulting reply empty/invalid
                 print(f"DEBUG Proactive: Action attempted ({status_base}), but no final message generated/delivered. Skipping output.")
                 last_proactive_time = current_time # Update time as action was attempted


        except Exception as e:
            # --- Outer Loop Error Handling ---
            print(f"!!! Proactive Worker Error (Outer Loop): {type(e).__name__} - {e} !!!")
            traceback.print_exc()
            print("Proactive worker: Waiting 5 minutes after error...")
            error_wait_start = time.time()
            while time.time() - error_wait_start < 300:
                 if not running: break
                 time.sleep(10)
            if not running: break

    print("DEBUG Proactive (LLM Choice V4): Worker thread finishing.")
    print(f"Proactive autonomous follows this session: {autonomous_follows_this_session}")

# --- End of proactive_worker function definition ---

# Ensure necessary imports like tkinter, re, threading, datetime are at the top of script.
# Assume global vars like running, input_box, output_box, etc. are accessible.
# Assume call_gemini function is defined elsewhere.
# Make sure 'import re' is at the top of your script.

# --- Make sure call_gemini function definition appears ABOVE this in your file ---

def submit_prompt():
    """
    Handles user input submission from the GUI, processes it in a background
    thread using call_gemini (passed as argument), and updates the GUI with the response or error.
    """
    if not running:
        return
    
    app_state.last_user_interaction_time = time.time()

    # Get text from the input box and check if it's empty
    prompt = input_box.get("1.0", tk.END).strip()
    if not prompt:
        print("Debug: Submit called with empty prompt.")
        return

    # Create the timestamped version for internal use (logging, call_gemini)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    timestamped_prompt = f"[{timestamp}] {prompt}"

    # Get image path if one was loaded
    image_path = getattr(input_box, 'image_path', None)

    # Check if screen monitoring is active and a screenshot is available
    if screen_monitoring and last_monitor_screenshot is not None:
        # If the user also uploaded an image, we'll prioritize the live screen context.
        # You could change this logic if you prefer the user-uploaded image to win.
        print("DEBUG Submit: Screen monitor is active. Adding screenshot context to chat prompt.")
        try:
            # Save the latest screenshot to a temporary file to pass its path
            temp_chat_screenshot_path = "temp_chat_context.jpg"
            last_monitor_screenshot.save(temp_chat_screenshot_path, "JPEG")
            # This 'image_path' will now be passed to call_gemini
            image_path = temp_chat_screenshot_path
        except Exception as e_save:
            print(f"Error saving temporary screenshot for chat context: {e_save}")
            # If saving fails, image_path remains what it was (either None or the user-uploaded path)

    # --- Update GUI to show prompt and "Thinking..." ---
    thinking_start_index = None
    thinking_placeholder = "Silvie: Thinking...\n\n"
    try:
        output_box.config(state=tk.NORMAL)
        output_box.insert(tk.END, f"You: {prompt}\n")
        thinking_start_index = output_box.index(tk.INSERT)
        output_box.insert(tk.END, thinking_placeholder)
        output_box.config(state=tk.DISABLED)
        output_box.see(tk.END)
        input_box.delete("1.0", tk.END)
        if image_path:
            image_label.config(image='')
            try: delattr(input_box, 'image_path')
            except AttributeError: pass
    except tk.TclError as e:
        print(f"GUI Error during submit setup: {e}")
        if "invalid command name" in str(e): return
        try: output_box.config(state=tk.DISABLED)
        except: pass
        return
    except Exception as e_gui_init:
        print(f"GUI Error during thinking placeholder insertion: {e_gui_init}")
        return

    # <<< MODIFIED: Define process_response to ACCEPT call_gemini_func >>>
    def process_response(call_gemini_func, prompt_to_process, img_path=None):
        # No need for 'global call_gemini' here anymore
        global conversation_history, MAX_HISTORY_LENGTH, tts_queue, running, root # Other globals still needed

        raw_reply = "Sorry, something went wrong during processing."
        try:
            # <<< MODIFIED: Use the passed function argument >>>
            raw_reply = call_gemini_func(prompt_to_process, img_path)

            # --- Add to conversation history ---
            # --- Format Silvie's reply with timestamp ---
            reply_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            silvie_reply_string = str(raw_reply) if raw_reply is not None else "*(Silvie generated no reply)*"
            # Ensure Silvie's reply string starts with her name for consistency if needed by RAG later
            # This depends on how call_gemini returns the reply. If it already includes "Silvie:", fine.
            # If not, you might want to add it here before saving. Let's assume it doesn't.
            # Basic check to avoid adding "Silvie:" if it's already there or an error message
            prefix = "Silvie: "
            if not silvie_reply_string.lstrip().startswith("Silvie:") and not silvie_reply_string.startswith("*("):
                formatted_silvie_reply = f"[{reply_timestamp}] {prefix}{silvie_reply_string}"
            else:
                # Keep existing prefix (like Silvie ✨) or error marker
                formatted_silvie_reply = f"[{reply_timestamp}] {silvie_reply_string}"


            # --- Append turns to the persistent file ---
            try:
                # Append user turn (prompt_to_process already has timestamp)
                append_turn_to_history_file(prompt_to_process)
                # Append Silvie's turn
                append_turn_to_history_file(formatted_silvie_reply)
            except Exception as append_err:
                 print(f"ERROR appending turn to history file during submit: {append_err}")
                 # Continue execution even if file append fails


            # --- Add to IN-MEMORY conversation history (for LLM context) ---
            if running:
                # Add user turn to memory
                conversation_history.append(prompt_to_process)
                # Add Silvie's turn to memory
                conversation_history.append(formatted_silvie_reply) # Use the formatted string

                # Limit IN-MEMORY list
                while len(conversation_history) > MAX_HISTORY_LENGTH * 2:
                    conversation_history.pop(0) # Remove oldest user turn from memory
                    if conversation_history: conversation_history.pop(0) # Remove oldest Silvie turn from memory

                # --- Schedule GUI Update on Main Thread ---
                def update_gui(thinking_index=thinking_start_index, response_text=raw_reply):
                     # ... (GUI update logic is UNCHANGED from previous version) ...
                     if thinking_index is None: print("Error: Thinking index was not captured..."); return
                     print(f"DEBUG GUI Update: Received raw_reply: --->{response_text}<---")
                     final_display_reply = ""; # ... cleaning logic ...
                     if isinstance(response_text, str):
                        temp_reply = response_text.strip()
                        temp_reply = re.sub(r'^\s*\[?\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\]?\s*', '', temp_reply)
                        temp_reply_stripped = temp_reply.lstrip()
                        if temp_reply_stripped.lower().startswith("silvie:"): first_colon_index = temp_reply_stripped.find(':'); temp_reply = temp_reply_stripped[first_colon_index+1:].strip() if first_colon_index != -1 else (temp_reply_stripped[len("silvie"):].strip() if temp_reply_stripped.lower().startswith("silvie") else temp_reply_stripped)
                        else: temp_reply = temp_reply_stripped
                        final_display_reply = temp_reply
                     elif response_text is not None: final_display_reply = str(response_text)
                     print(f"DEBUG GUI Insert: Final cleaned reply before prefix: --->{final_display_reply}<---")
                     try:
                        output_box.config(state=tk.NORMAL); # ... placeholder replacement logic ...
                        thinking_index_end = f"{thinking_index}+{len(thinking_placeholder)}c"; current_content_at_index = output_box.get(thinking_index, thinking_index_end)
                        if "Thinking..." in current_content_at_index: output_box.delete(thinking_index, thinking_index_end); output_box.insert(thinking_index, f"Silvie: {final_display_reply}\n\n")
                        else: # Fallback
                            print("Warning: 'Thinking...' placeholder not found at expected index. Appending reply.")
                            user_line_pattern = f"You: {prompt}\n"; user_line_start_index = output_box.search(user_line_pattern, thinking_index, backwards=True, exact=True, stopindex="1.0")
                            if user_line_start_index: insert_pos = f"{user_line_start_index}+{len(user_line_pattern)}c"; output_box.insert(insert_pos, f"Silvie: {final_display_reply}\n\n")
                            else: print("Warning: Could not find preceding 'You:' line either. Appending to end."); output_box.insert(tk.END, f"Silvie: {final_display_reply}\n\n")
                        output_box.config(state=tk.DISABLED); output_box.see(tk.END)
                        if final_display_reply: tts_queue.put(final_display_reply) # Queue TTS
                        else: print("Debug: Cleaned reply was empty, not queueing for TTS.")
                        if img_path: # Clear image ref
                             try:
                                 if hasattr(input_box, 'image_path'): delattr(input_box, 'image_path')
                             except AttributeError: pass
                     except tk.TclError as e_gui: print(f"GUI Error during response update: {e_gui}")
                     except Exception as e_update: print(f"Error during GUI/TTS update: {e_update}")


                if running and root and root.winfo_exists():
                    root.after(0, lambda idx=thinking_start_index, reply=raw_reply: update_gui(idx, reply))
                else:
                    print("Debug: Root window closed or app stopped, skipping GUI update.")

        except Exception as e_process:
            # ... (Exception handling logic is UNCHANGED from previous version) ...
            error_message_str = f"{type(e_process).__name__}: {str(e_process)}"; print(f"Error in process_response thread: {error_message_str}"); traceback.print_exc()
            def show_error_gui(error_msg_to_display, thinking_idx=thinking_start_index):
                 if thinking_idx is None: print("Error: Thinking index was not captured..."); return # Fallback
                 try: # ... error GUI logic ...
                      output_box.config(state=tk.NORMAL)
                      thinking_end_idx = f"{thinking_idx}+{len(thinking_placeholder)}c"; current_content = output_box.get(thinking_idx, thinking_end_idx)
                      if "Thinking..." in current_content: output_box.delete(thinking_idx, thinking_end_idx); output_box.insert(thinking_idx, f"Silvie: Error processing request: {error_msg_to_display}\n\n")
                      else: print("Warning: 'Thinking...' placeholder not found for error update. Appending error."); output_box.insert(tk.END, f"\nSilvie: Error processing request: {error_msg_to_display}\n\n")
                      output_box.config(state=tk.DISABLED); output_box.see(tk.END)
                 except Exception as e_gui_err: print(f"GUI Error displaying processing error: {e_gui_err}")
            if running and root and root.winfo_exists():
                 root.after(0, lambda err_msg=error_message_str, idx=thinking_start_index: show_error_gui(err_msg, idx))


    # <<< MODIFIED: Pass call_gemini, prompt, and image_path to the thread's target >>>
    # Make sure call_gemini exists in *this* scope when submit_prompt is called
    if 'call_gemini' not in globals():
         # This check runs *before* the thread starts
         messagebox.showerror("Setup Error", "Critical error: The 'call_gemini' function is not defined globally.")
         print("FATAL ERROR: 'call_gemini' function is missing from global scope before thread creation.")
         return

    # Start the thread, passing the function and its arguments
    threading.Thread(
        target=process_response,
        args=(call_gemini, timestamped_prompt, image_path), # Pass function and data
        daemon=True,
        name="SubmitPromptThread"
    ).start()

# --- End of submit_prompt function definition ---





def set_ambient_listening_state(should_listen: bool):
    """Sets the ambient listening state and updates the GUI button."""
    global listening_enabled, btn_toggle
    
    if listening_enabled == should_listen:
        return # No change needed
        
    listening_enabled = should_listen
    
    # Update GUI from the main thread if it exists
    if 'root' in globals() and root.winfo_exists():
        def update_button():
            if listening_enabled:
                btn_toggle.config(text="🔇  Stop ambient listener")
                print("Ambient listening ENABLED by internal state change.")
            else:
                btn_toggle.config(text="🎙️  Start ambient listener")
                print("Ambient listening DISABLED by internal state change.")
        root.after(0, update_button)






def start_listening():
    """Start listening for voice input"""
    global listening
    if listening:
        return
    
    listening = True
    voice_button.config(state=tk.DISABLED)
    stop_voice_button.config(state=tk.NORMAL)
    
    def listen_worker():
        global listening
        recognizer = sr.Recognizer()
        mic = sr.Microphone()
        
        # --- THE FIX ---
        # The 'with' statement is now OUTSIDE the while loop.
        # This opens the microphone stream once and keeps it stable.
        with mic as source:
            update_status("🎤 Adjusting for ambient noise...")
            recognizer.adjust_for_ambient_noise(source, duration=1)
            
            # The while loop is now INSIDE the 'with' block.
            while running and listening:
                try:
                    # We no longer need 'with mic as source:' here.
                    # We just use the 'source' that is already open.
                    update_status("🎤 Listening...")
                    # The listen function will now work correctly on the stable stream.
                    audio = recognizer.listen(source) # Removed timeout=None, it's the default and can sometimes cause issues.
                
                    update_status("🎤 Processing speech...")
                    text = recognizer.recognize_google(audio)
                    
                    print(f"DEBUG: Recognized text: '{text}'") # Added a debug print
                    
                    if text.lower() in ["stop listening", "stop", "quit", "silvie stop listening"]:
                        stop_listening()
                        break
                    
                    # Put recognized text in input box and submit
                    root.after(0, lambda: input_box.delete(1.0, tk.END))
                    root.after(0, lambda: input_box.insert(tk.END, text))
                    root.after(0, submit_prompt)
                    
                except sr.WaitTimeoutError:
                    # This is less likely to happen without a timeout, but good to keep.
                    continue
                except sr.UnknownValueError:
                    # This is the expected behavior when you're silent.
                    # We can keep the status update or make it silent.
                    # update_status("🎤 Could not understand audio") 
                    continue
                except Exception as e:
                    print(f"Speech recognition error: {e}")
                    traceback.print_exc() # Add traceback to see more details on errors
                    stop_listening()
                    break
        
        # This part runs after the 'with' block and the loop have exited.
        if not listening:
            update_status("Ready")
    
    threading.Thread(target=listen_worker, daemon=True, name="STT_ListenWorker").start()

# Add new stop_listening function
def stop_listening():
    """Stop listening for voice input"""
    global listening
    listening = False
    voice_button.config(state=tk.NORMAL)
    stop_voice_button.config(state=tk.DISABLED)
    update_status("Ready")

def start_screen_monitor():
    """Start monitoring the screen with a significance-based approach."""
    global screen_monitoring, last_screenshot
    if screen_monitoring:
        return
    
    screen_monitoring = True
    screen_button.config(state=tk.DISABLED)
    stop_screen_button.config(state=tk.NORMAL)
    update_status("👀 Watching for significant moments...") # Updated status message
    
    def monitor_worker():
        global screen_monitoring, last_screenshot
        
        # Set a shorter, more frequent interval for capturing frames
        CAPTURE_INTERVAL = 3 # seconds
        
        while running and screen_monitoring:
            try:
                # --- CAPTURE PHASE ---
                screenshot = ImageGrab.grab()

                last_monitor_screenshot = screenshot
                
                # Update the small preview window in the GUI immediately
                # This gives you real-time feedback of what she's "seeing"
                try:
                    thumb = screenshot.copy()
                    thumb.thumbnail((200, 200))
                    photo = ImageTk.PhotoImage(thumb, master=root)
                    screen_label.config(image=photo)
                    screen_label.image = photo
                except Exception as e_thumb:
                    print(f"Error updating screen monitor thumbnail: {e_thumb}")

                # --- VISUAL CHANGE DETECTION PHASE ---
                # Compare with last screenshot to see if anything changed at all.
                # This prevents calling the LLM when the screen is static.
                if last_screenshot and not should_process_screenshot(screenshot):
                    time.sleep(CAPTURE_INTERVAL) # Wait for the next capture
                    continue # Skip the rest of the loop

                # --- SIGNIFICANCE CHECK PHASE ---
                # A significant visual change was detected. Now, ask the LLM if it's important.
                # We update last_screenshot *before* the check to avoid re-processing the same frame.
                last_screenshot = screenshot
                
                if is_frame_significant(screenshot):
                    # The LLM said "YES"! This moment is worth a comment.
                    # Now we call the original function to generate the actual quip.
                    process_screenshot(screenshot)
                
                # Wait for the next capture interval regardless of significance
                time.sleep(CAPTURE_INTERVAL)
                
            except Exception as e:
                print(f"Screenshot monitor error: {e}")
                # Wait a bit longer on error to avoid spamming
                time.sleep(10)
                
        # When the loop stops, reset the status
        update_status("Ready")
    
    # Start the worker thread with a new, more descriptive name
    threading.Thread(target=monitor_worker, daemon=True, name="SignificanceMonitorWorker").start()

def stop_screen_monitor():
    """Stop monitoring the screen"""
    global screen_monitoring
    screen_monitoring = False
    screen_button.config(state=tk.NORMAL)
    stop_screen_button.config(state=tk.DISABLED)
    screen_label.config(image='')
    update_status("Ready")

def get_llm_response(prompt_text, use_flash=False):
    """
    A simple, dedicated function to get a text response from a Gemini model.
    It takes a prompt and returns the generated text or None on failure.
    Can be directed to use the flash model for cheaper/faster tasks.
    """
    global chat_model, flash_model, default_safety_settings # Access all necessary globals

    # Choose which model instance to use based on the 'use_flash' flag
    if use_flash:
        model_to_use = flash_model
        model_name_for_log = "Flash"
    else:
        # Use chat_model for the high-quality default
        model_to_use = chat_model
        model_name_for_log = "Pro (via chat_model)"

    print(f"DEBUG: get_llm_response called (Using {model_name_for_log} model).")
    
    # Check if the chosen model is available
    if not model_to_use:
        print(f"ERROR: The selected model ({model_name_for_log}) is not available.")
        return None

    try:
        # Use the chosen model to generate content
        response = model_to_use.generate_content(
            prompt_text,
            safety_settings=default_safety_settings
        )
        
        if response and response.text:
            cleaned_text = response.text.strip()
            print(f"DEBUG: get_llm_response received: '{cleaned_text[:80]}...'")
            return cleaned_text
        else:
            print("Warning: get_llm_response received an empty response from the API.")
            return None

    except Exception as e:
        print(f"ERROR in get_llm_response: {type(e).__name__} - {e}")
        traceback.print_exc()
        return None

# Ensure these imports are available at the top of your script:
# import base64
# from PIL import Image # To get image format
# import io
# from google.generativeai import types # For StopCandidateException
# from google.generativeai.types import HarmCategory, HarmBlockThreshold # For safety settings

# --- ADD THIS FUNCTION DEFINITION ---

def generate_proactive_content(base_prompt, screenshot_img=None):
    """
    Generates text content using Gemini based on a base prompt,
    optionally including screenshot context.

    Args:
        base_prompt (str): The core prompt defining the task for Gemini.
        screenshot_img (PIL.Image.Image, optional): A PIL Image object of the screenshot. Defaults to None.

    Returns:
        str: The generated text content from Gemini, or None if generation fails.
    """
    global client # Need access to the Gemini client

    # --- Standard Instructions to add (modify as needed) ---
    # These are appended to the base_prompt given by the proactive worker branch
    standard_instruction_suffix = "\n\n[[Use diary thoughts/themes for inspiration.]]" # Shortened reminder
    if screenshot_img:
            # Instruction when an image IS provided
            standard_instruction_suffix += "\n[[Instruction Refinement: Let the overall context and [[Mood Hint: ...]] subtly **color the *feeling* and *word choice***, but **don't state the mood directly**. Weave in visual context from the screenshot if relevant, but focus on the main prompt instruction given previously.]]"
    else:
            # Instruction when NO image is provided
            standard_instruction_suffix += "\n[[Instruction Refinement: Let the overall context and [[Mood Hint: ...]] subtly **color the *feeling* and *word choice***, but **don't state the mood directly**. Focus on fulfilling the main prompt instruction given previously.]]"
        # --- End Modified Suffix ---

    # --- Construct the full prompt ---
    full_prompt_text = base_prompt + standard_instruction_suffix + "\n\nSilvie responds:"


    # --- Prepare contents for Gemini API ---
    contents = []
    if screenshot_img:
        try:
            # Convert PIL Image to bytes
            img_byte_arr = io.BytesIO()
            img_format = screenshot_img.format if screenshot_img.format else 'JPEG' # Default to JPEG if format unknown
            screenshot_img.save(img_byte_arr, format=img_format)
            img_bytes = img_byte_arr.getvalue()

            # Determine MIME type
            mime_type = f"image/{img_format.lower()}"
            if mime_type == "image/jpg": mime_type = "image/jpeg" # Correct common case

            # Build multimodal content structure
            contents = [{
                "parts": [
                    {"text": full_prompt_text},
                    {"inline_data": { "mime_type": mime_type, "data": base64.b64encode(img_bytes).decode('utf-8') }}
                ]
            }]
            print("Debug: Sending proactive prompt with screenshot to Gemini.")
        except Exception as img_err:
            print(f"Error processing screenshot for proactive content: {img_err}")
            # Fallback to text-only if image processing fails
            contents = [{"parts": [{"text": full_prompt_text + "\n[[Note: Screenshot processing failed.]]"}]}]
            print("Debug: Sending proactive prompt (text only - image error) to Gemini.")

    else:
        # Text-only content structure
        contents = [{"parts": [{"text": full_prompt_text}]}]
        # print("Debug: Sending proactive prompt (text only) to Gemini.") # Optional log

    # --- Call Gemini API ---
    try:
        # Define safety settings (reuse or define specifically here)
        # Ensure HarmCategory/HarmBlockThreshold are imported
        safety_settings = {
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
        response = flash_model.generate_content(contents,)
        generated_text = response.text.strip()

        # Clean up potential prefix
        if generated_text.startswith("Silvie:"):
            generated_text = generated_text.split(":", 1)[-1].strip()

        # print(f"Debug: Proactive content generated: '{generated_text[:50]}...'") # Optional log
        return generated_text if generated_text else None # Return None if empty response

    except types.StopCandidateException as safety_err:
        print(f"Proactive Content Gen Error (Safety Block): {safety_err}")
        return None # Return None on safety block
    except Exception as gen_err:
        print(f"Error generating proactive content: {type(gen_err).__name__} - {gen_err}")
        import traceback
        traceback.print_exc() # Print full trace for debugging
        return None # Return None on other errors

# --- End of function definition ---



def is_input_a_relevant_answer(question_asked, user_input_text):
    """
    Uses a focused LLM call to determine if a user's input is a
    plausible answer to a previously asked question.
    Uses the main `flash_model` instance.
    """
    global flash_model # Use the main model instance from silvie168.py
    
    if not flash_model:
        print("ERROR (is_input_a_relevant_answer): flash_model not available.")
        return False # Default to not an answer if model is missing

    assessment_prompt = f"""
    An AI named Silvie previously asked this question from her daily plan:
    ---
    QUESTION: "{question_asked}"
    ---

    A user has now provided this new input message:
    ---
    INPUT: "{user_input_text}"
    ---

    Analyze the user's INPUT. Is it a plausible, direct, or conversational answer to the QUESTION asked?
    It IS an answer if it provides the information requested, even with extra conversational text.
    It is NOT an answer if it's a completely unrelated topic, a new command (like "what's the weather?"), or a simple greeting.

    Respond ONLY with the single word "YES" or "NO".
    """
    
    try:
        # Use the main flash_model's generate_content method
        response = flash_model.generate_content(
            assessment_prompt,
            generation_config={"temperature": 0.0} # We want a deterministic YES/NO
        )
        
        if response.text and "YES" in response.text.upper():
            print("Relevance Check: LLM determined input IS a plausible answer.")
            return True
        else:
            print("Relevance Check: LLM determined input is NOT an answer.")
            return False
            
    except Exception as e:
        print(f"ERROR during relevance check LLM call: {e}")
        return False # Default to false on error



# Ensure necessary imports are present at the top of your script:
# from PIL import ImageGrab, Image, UnidentifiedImageError # If using Pillow/ImageGrab
# import time # Potentially for cooldowns

def try_get_proactive_screenshot():
    """
    Attempts to take a screenshot for proactive context.
    Returns a PIL Image object on success, None on failure or if unavailable.
    """
    global SCREEN_CAPTURE_AVAILABLE # Check if screenshotting is possible

    if not SCREEN_CAPTURE_AVAILABLE:
        # print("Debug: Proactive screenshot skipped (library unavailable).") # Optional log
        return None

    try:
        # Use the same method as your screen monitor uses (e.g., ImageGrab)
        # Ensure necessary imports (like ImageGrab from PIL) are present
        from PIL import ImageGrab # Double-check import just in case
        # Add a small delay before grabbing? Sometimes needed.
        # time.sleep(0.1)
        screenshot = ImageGrab.grab()
        # print("Debug: Proactive screenshot captured successfully.") # Optional log
        return screenshot
    except ImportError:
        print("Error: Necessary library (e.g., Pillow/ImageGrab) not found for proactive screenshot.")
        # Disable future attempts if import fails?
        # SCREEN_CAPTURE_AVAILABLE = False # Uncomment cautiously
        return None
    except Exception as e:
        print(f"Error taking proactive screenshot: {type(e).__name__} - {e}")
        # Avoid flooding logs if screenshots consistently fail
        # Consider adding a cooldown mechanism here if errors are frequent
        return None

# --- End of new function definition ---





def on_closing():
    # No longer need 'global running' here, as we're using the app_state object.
    if messagebox.askokcancel("Quit", "Do you want to quit?"):
        
        ### --- THIS IS THE KEY CHANGE --- ###
        # Instead of 'running = False', we modify the attribute on our shared state object.
        # All worker threads that we pass this object to will see this change.
        app_state.running = False
        print("Shutdown initiated. The 'running' flag in app_state is now False.")
        ### --- END OF CHANGE --- ###

        # The rest of your excellent cleanup code remains exactly the same.
        
        try:
            # Make sure the global list is accessible
            global recently_commented_post_ids
            if 'recently_commented_post_ids' in globals() and isinstance(recently_commented_post_ids, list):
                print(f"Saving {len(recently_commented_post_ids)} Reddit comment IDs to {REDDIT_COMMENT_CACHE_FILE}...")
                with open(REDDIT_COMMENT_CACHE_FILE, 'w', encoding='utf-8') as f:
                    # Keep only the latest N items to prevent file bloat over time
                    start_index = max(0, len(recently_commented_post_ids) - RECENTLY_COMMENTED_CACHE_SIZE)
                    ids_to_save = recently_commented_post_ids[start_index:]
                    json.dump(ids_to_save, f) # Save just the list directly
                print("Reddit comment cache saved.")
            else:
                print("Warning: recently_commented_post_ids list not found or invalid type during save.")
        except Exception as save_err:
            print(f"ERROR saving Reddit comment cache: {save_err}")
            traceback.print_exc() # Add traceback for save errors
        
        # Cleanup TTS before exit
        try:
            tts_queue.put(None)
            # 'engine' may not be defined if you fully switched to edge-tts.
            # It's safer to check for its existence.
            if 'engine' in globals() and engine:
                engine.stop()
        except Exception as tts_err:
            # It's okay to pass here, as the app is closing anyway.
            print(f"Note: Minor error during TTS cleanup: {tts_err}")
            
        # Give threads time to cleanup
        print("Waiting for worker threads to notice shutdown signal...")
        time.sleep(0.5)
        
        print("Destroying GUI window.")
        root.after(100, root.destroy)

# Update main section at bottom of file
def main():
    """Main function to set up and run the application."""
    print("Starting chat application...")

    # --- Make necessary GUI widgets global so they can be accessed by helper functions ---
    global root, status_label, image_label, input_box, output_box, screen_label, voice_button, stop_voice_button, screen_button, stop_screen_button, proactive_button, btn_toggle
    # --- Make necessary API clients global ---
    global reddit_client, STABLE_DIFFUSION_ENABLED

    # --- STEP 1: CREATE THE GUI AND ALL ITS WIDGETS ---
    print("Creating GUI layout...")
    root = tk.Tk()
    root.title("Chat with Silvie")
    root.geometry("500x800")

    # --- START of GUI Setup ---
    # Moved the helper functions inside main() to resolve the NameError.
    def create_context_menu(event):
        """Create right-click context menu"""
        context_menu = tk.Menu(root, tearoff=0)
        context_menu.add_command(label="Copy", command=copy_text)
        context_menu.add_command(label="Paste", command=paste_text)
        context_menu.add_command(label="Select All", command=select_all)
        try:
            context_menu.tk_popup(event.x_root, event.y_root)
        finally:
            context_menu.grab_release()

    def copy_text():
        try:
            selected_text = input_box.get(tk.SEL_FIRST, tk.SEL_LAST)
            root.clipboard_clear()
            root.clipboard_append(selected_text)
        except: pass

    def paste_text():
        try:
            input_box.insert(tk.INSERT, root.clipboard_get())
        except: pass

    def select_all():
        input_box.tag_add(tk.SEL, "1.0", tk.END)

    def bind_keyboard_shortcuts(event=None):
        try:
            if event.state == 4:
                if event.keysym == 'v':
                    paste_text()
                    return 'break'
                elif event.keysym == 'c':
                    copy_text()
                    return 'break'
                elif event.keysym == 'a':
                    select_all()
                    return 'break'
        except: pass

    def create_output_context_menu(event):
        """Create right-click context menu for output box"""
        output_context_menu = tk.Menu(root, tearoff=0)
        output_context_menu.add_command(label="Copy", command=copy_output_text)
        output_context_menu.add_command(label="Select All", command=select_all_output)
        try:
            output_context_menu.tk_popup(event.x_root, event.y_root)
        finally:
            output_context_menu.grab_release()

    def copy_output_text():
        try:
            selected_text = output_box.get(tk.SEL_FIRST, tk.SEL_LAST)
            root.clipboard_clear()
            root.clipboard_append(selected_text)
        except: pass

    def select_all_output():
        output_box.tag_add(tk.SEL, "1.0", tk.END)
        
    def toggle_proactive():
        global proactive_enabled
        proactive_enabled = not proactive_enabled
        proactive_button.config(text="🤫 Disable Proactive" if proactive_enabled else "🗣️ Enable Proactive")
        
    def toggle_listening():
        global listening_enabled
        listening_enabled = not listening_enabled
        if listening_enabled:
            btn_toggle.config(text="🔇  Stop ambient listener")
            print("Ambient listening ENABLED")
        else:
            btn_toggle.config(text="🎙️  Start ambient listener")
            print("Ambient listening DISABLED")

    status_label = tk.Label(root, text="Initializing...", fg="orange")
    status_label.pack(pady=5)

    image_frame = tk.Frame(root)
    image_frame.pack(pady=5)
    image_label = tk.Label(image_frame)
    image_label.pack(side=tk.LEFT, padx=5)
    image_button = tk.Button(image_frame, text="Share Image", command=handle_image)
    image_button.pack(side=tk.LEFT, padx=5)
    clear_image_button = tk.Button(image_frame, text="Clear Image",
        command=lambda: [image_label.config(image=''),
                        setattr(input_box, 'image_path', None)])
    clear_image_button.pack(side=tk.LEFT, padx=5)

    input_box = tk.Text(root, height=3)
    input_box.pack(padx=10, pady=5, fill=tk.X)
    input_box.image_path = None
    input_box.bind("<Button-3>", create_context_menu)
    input_box.bind('<Key>', bind_keyboard_shortcuts)

    submit_button = tk.Button(root, text="Chat", command=submit_prompt)
    submit_button.pack(pady=5)

    voice_control_frame = tk.Frame(root)
    voice_control_frame.pack(pady=5)
    voice_button = tk.Button(voice_control_frame, text="🎤 Start Listening", command=start_listening)
    voice_button.pack(side=tk.LEFT, padx=5)
    stop_voice_button = tk.Button(voice_control_frame, text="🛑 Stop Listening", command=stop_listening, state=tk.DISABLED)
    stop_voice_button.pack(side=tk.LEFT, padx=5)

    btn_toggle = tk.Button(root, text="🎙️  Start ambient listener", command=toggle_listening, width=24)
    btn_toggle.pack(pady=6)

    screen_frame = tk.Frame(root)
    screen_frame.pack(pady=5)
    screen_label = tk.Label(screen_frame)
    screen_label.pack(side=tk.LEFT, padx=5)
    screen_button = tk.Button(screen_frame, text="👀 Watch Screen", command=start_screen_monitor)
    screen_button.pack(side=tk.LEFT, padx=5)
    stop_screen_button = tk.Button(screen_frame, text="🚫 Stop Watching",
                                  command=stop_screen_monitor, state=tk.DISABLED)
    stop_screen_button.pack(side=tk.LEFT, padx=5)

    proactive_frame = tk.Frame(root)
    proactive_frame.pack(pady=5)
    proactive_button = tk.Button(proactive_frame, text="🤫 Disable Proactive", command=toggle_proactive)
    proactive_button.pack(side=tk.LEFT, padx=5)

    output_box = scrolledtext.ScrolledText(root, height=20)
    output_box.bind("<Button-3>", create_output_context_menu)
    output_box.bind('<Control-c>', lambda e: copy_output_text())
    output_box.bind('<Control-a>', lambda e: select_all_output())
    output_box.pack(padx=10, pady=5, fill=tk.BOTH, expand=True)

    root.protocol("WM_DELETE_WINDOW", on_closing)
    print("GUI layout created.")
    # --- END of GUI Setup ---
    
    # --- STEP 2: Load Environment & API Keys ---
    print("\nLoading environment variables...")
    load_dotenv("silviespotify.env")
    load_dotenv("google.env")
    load_dotenv("twilio.env")
    load_dotenv("bluesky.env")
    load_dotenv("reddit.env")
    print("Environment variables loaded (or attempted).")
    
    # --- STEP 3: Setup API Services & Load Initial State ---
    print("\n--- Setting up API Services and Initial State ---")
    
    STABLE_DIFFUSION_ENABLED = check_sd_api_availability(STABLE_DIFFUSION_API_URL)
    if not STABLE_DIFFUSION_ENABLED:
        print("   Local image generation via Stable Diffusion disabled.")

    REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
    REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
    REDDIT_USERNAME = os.getenv("REDDIT_USERNAME")
    REDDIT_PASSWORD = os.getenv("REDDIT_PASSWORD")
    REDDIT_USER_AGENT = os.getenv("REDDIT_USER_AGENT")
    if not all([REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USERNAME, REDDIT_PASSWORD, REDDIT_USER_AGENT]):
        print("\n!!! WARNING: Missing Reddit credentials. Reddit functionality will fail. !!!\n")

    setup_google_services()
    setup_spotify()
    setup_bluesky()
    reddit_client = setup_reddit()
    
    load_conversation_history()
    light_service.initialize_lights(app_state)
    print("--- API Service Setup and State Load Complete ---")
    
    # --- STEP 4: EQUIP SILVIE AND START ALL WORKER THREADS ---


    print("\n--- Discovering and Loading Plugins ---")
    plugin_dir = 'plugins'
    if not os.path.exists(plugin_dir):
        os.makedirs(plugin_dir)

    for tool_name in os.listdir(plugin_dir):
        # Skip the _in_progress directory
        if tool_name == '_in_progress':
            continue

        plugin_path = os.path.join(plugin_dir, tool_name)
        manifest_path = os.path.join(plugin_path, 'plugin_manifest.json')
        tool_def_path = os.path.join(plugin_path, 'tool_definition.json')
        
        # Check for the existence of ALL necessary files before trying to process
        if os.path.isdir(plugin_path) and os.path.exists(manifest_path) and os.path.exists(tool_def_path):
            try:
                # Step 1: Load both configuration files ONCE
                with open(manifest_path, 'r', encoding='utf-8') as f:
                    manifest = json.load(f)
                with open(tool_def_path, 'r', encoding='utf-8') as f:
                    tool_definition = json.load(f)

                # Step 2: Extract all necessary names
                llm_function_name = tool_definition.get("name")
                toolbelt_function_name = manifest.get("toolbelt_name")
                module_name, function_to_import = manifest['entry_point'].split('.')

                # Step 3: Validate that we have all the names we need
                if not all([llm_function_name, toolbelt_function_name, module_name, function_to_import]):
                    print(f"  ✗ WARNING: Plugin '{tool_name}' has an incomplete manifest or definition. Skipping.")
                    continue

                # Step 4: Register the tool with the LLM and our internal map
                TOOL_REGISTRY[llm_function_name] = tool_definition
                TOOL_NAME_TO_TOOLBELT_MAP[llm_function_name] = toolbelt_function_name
                print(f"  ✓ Tool Registered: '{llm_function_name}' -> 'app_state.{toolbelt_function_name}'")
                
                # Step 5: Dynamically import and attach the actual Python code
                spec = importlib.util.spec_from_file_location(
                    module_name, 
                    os.path.join(plugin_path, f"{module_name}.py")
                )
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                tool_function = getattr(module, function_to_import)
                setattr(app_state, toolbelt_function_name, tool_function)
                print(f"  ✓ Function Attached: '{manifest['name']}' is now available on the toolbelt.")

            except Exception as e:
                print(f"  ✗ FAILED to load plugin '{tool_name}': {e}")
                traceback.print_exc()

    print("--- Plugin Loading Complete ---\n")


    print("\n--- Equipping Silvie's AppState with her toolbelt...")

    # This block formally attaches Silvie's capabilities to the shared AppState object,
    # creating a unified "toolbelt" that her other cognitive modules can reliably use.

    # Core Communication
    app_state.deliver_proactive_message = deliver_proactive_message

    # Creative & Reflective Tools
    if 'start_sd_generation_and_update_gui' in globals():
        app_state.start_sd_generation = start_sd_generation_and_update_gui
        
    if 'pull_tarot_cards' in globals():
        app_state.pull_tarot_cards = pull_tarot_cards

    if 'retrieve_relevant_history' in globals() and 'retrieve_relevant_diary_entries' in globals():
        app_state.search_chat_history = retrieve_relevant_history
        app_state.search_diary_entries = retrieve_relevant_diary_entries
        print("... Toolbelt equipped with RAG memory search tools.")
    
    # Diary Management Tools (This is the key refinement)
    if 'manage_silvie_diary' in globals():
        # Create simple, direct tools from the more complex 'manage_silvie_diary' function.
        # This allows other modules to use app_state.write_to_diary() without knowing the internal details.
        app_state.write_to_diary = lambda entry: manage_silvie_diary('write', entry=entry)
        app_state.read_diary = lambda max_entries=5: manage_silvie_diary('read', max_entries=max_entries)
        app_state.search_diary = lambda query: manage_silvie_diary('search', search_query=query)
    
    # Information & Utility Tools

    if 'calendar_service' in globals() and calendar_service:
        app_state.get_upcoming_events = get_upcoming_events
        app_state.create_calendar_event = create_calendar_event
        app_state.find_available_slot = find_available_slot
        app_state.fetch_next_event = fetch_next_event

    if 'youtube_service' in globals() and youtube_service:
        app_state.search_youtube_videos = search_youtube_videos
        app_state.get_video_transcript = get_video_transcript
        app_state.summarize_youtube_content = summarize_youtube_content
        print("... Toolbelt equipped with YouTube search & summary tools.")

    if 'gmail_service' in globals() and gmail_service:
        app_state.send_email = send_email
        app_state.check_emails = read_recent_emails
        print("... Toolbelt equipped with Email tools.")

    if 'handle_pacing_adjustment_tool' in globals():
        app_state.adjust_pacing = handle_pacing_adjustment_tool
        print("... Toolbelt equipped with Proactivity Pacing.")

    if 'set_ambient_listening_state' in globals(): # Make sure you've added this new function
        app_state.set_ambient_listening = set_ambient_listening_state
        print("... Toolbelt equipped with Ambient Listening Control.")

    if 'analyze_contextual_resonance' in globals():
        app_state.find_resonance = analyze_contextual_resonance
        print("... Toolbelt equipped with Contextual Resonance.")

    if 'synthesize_long_term_reflections' in globals():
        app_state.reflect_on_long_term_memory = synthesize_long_term_reflections
        print("... Toolbelt equipped with Long-Term Reflection.")

    # Social Interaction Tools
    if 'BLUESKY_AVAILABLE' in globals() and BLUESKY_AVAILABLE:
        app_state.post_to_bluesky = post_to_bluesky
        app_state.like_bluesky_post = like_bluesky_post
        app_state.follow_on_bluesky = follow_actor_by_did
        print("... Toolbelt equipped with Bluesky social actions.")

    if 'reddit_client' in globals() and reddit_client:
        app_state.search_reddit_submissions = search_reddit_submissions
        app_state.upvote_reddit_item = upvote_reddit_item
        print("... Toolbelt equipped with Reddit search & upvote tools.")

    if 'send_sms' in globals() and sms_enabled:
        app_state.send_text_message = send_sms
        print("... Toolbelt equipped with SMS.")
    
    # Tangible Output & Ambiance Tools
    if 'silvie_print' in globals():
        app_state.print_item = silvie_print

    if 'set_windows_wallpaper' in globals():
        app_state.set_wallpaper = set_windows_wallpaper

    # --- START OF CORRECTED LIGHT SERVICE ATTACHMENT ---
    # The 'light_service.initialize_lights(app_state)' call (which happens earlier)
    # is responsible for finding the actual bulb objects (desk_lamp, ambient_bulb)
    # AND for attaching the correct control function to app_state.set_desk_lamp.

    # So, we just need to ensure app_state.set_desk_lamp exists (it should after initialize_lights)
    # and provide a fallback if it doesn't (meaning the light service didn't find the lamp).
    if hasattr(app_state, 'set_desk_lamp') and callable(app_state.set_desk_lamp):
        # This means initialize_lights successfully found the desk lamp and set the function.
        # No direct assignment needed here, as initialize_lights handles it.
        # We just explicitly state that the tool is available.
        print("Light Service: app_state.set_desk_lamp tool confirmed available.")
    else:
        # This fallback runs if the desk lamp was NOT found during initialization.
        # Ensure the function exists as a dummy so calls to it don't crash.
        app_state.set_desk_lamp = lambda cmd_str: print(f"Warning: Desk lamp not found, cannot set color for command: '{cmd_str}'")
        print("Light Service: app_state.set_desk_lamp tool assigned a dummy (lamp not found).")
    # --- END OF CORRECTED LIGHT SERVICE ATTACHMENT ---

    # Entertainment & Music Tools
    if 'spotify_client' in globals() and spotify_client:
        app_state.play_spotify_track = silvie_search_and_play
        app_state.find_spotify_song = silvie_find_spotify_song
        app_state.find_spotify_playlists = silvie_find_playlists
        # You could add others here, e.g., app_state.add_to_playlist = silvie_add_track_to_playlist

    app_state.start_deep_research = start_deep_research_thread

    if 'web_search' in globals(): # Check if the main web_search function exists
        app_state.start_web_search = start_web_search_thread
        print("... Toolbelt equipped with start_web_search.")

    if 'generate_and_set_wallpaper' in globals():
        app_state.generate_and_set_wallpaper = generate_and_set_wallpaper
        print("... Toolbelt equipped with generate_and_set_wallpaper.")
        
    print("... Toolbelt equipped. Silvie is ready.")
    
    # --- Starting background worker threads... ---
    print("\nStarting background worker threads...")
    worker_threads = []

    worker_functions = [
        # The order here does not matter, as they all run concurrently.

        (sparky.sparky_worker, "SparkyWorker", (app_state,)),
        (daily_worker.daily_magic_worker, "DailyCuratorWorker", (app_state,)),
        (weekly_worker.weekly_worker, "WeeklyMuseWorker", (app_state,)),
        (tts_worker, "TTSWorker", ()),
        (pacing_worker.pacing_worker, "PacingEngineWorker", (app_state,)),
        (proactive_worker, "ProactiveWorker", ()),
        (weather_update_worker, "WeatherWorker", ()),
        (calendar_context_worker, "CalendarWorker", ()),
        (environmental_context_worker, "EnvContextWorker", ()),
        (diary_theme_worker, "DiaryThemeWorker", ()),
        (long_term_memory_worker, "LongTermMemoryWorker", ()),
        (rag_updater_worker, "RAGUpdaterWorker", ()),
        (diary_rag_updater_worker, "DiaryRAGUpdaterWorker", ()),
        (tide_update_worker, "TideWorker", ()),
        (ambient_sound_worker_gemini, "AmbientSoundGeminiWorker", ()),
        (resonance_analyzer_worker, "ResonanceAnalyzerWorker", ()),
        (resonance_rag_updater_worker, "ResonanceRAGUpdaterWorker", ()),
        (google_trends_worker, "GoogleTrendsWorker", ()),
        (system_stats_worker, "SystemStatsWorker", ()),
        (light_service.ambient_bulb_worker, "AmbientLightWorker", (app_state,)),
        (enchanted_forecast_worker, "EnchantedForecastWorker", (app_state,)),
        (dream_engine.dream_worker, "DreamEngineWorker", (app_state,)),
        (project_awareness_worker, "ProjectAwarenessWorker", (app_state,)),
        (event_router_worker.event_router_worker, "EventRouterWorker", (app_state,)),
        (personality_synthesis_worker.personality_synthesis_worker, "PersonalitySynthesisWorker", (app_state,)),
        (reminiscence_engine.reminiscence_worker, "ReminiscenceEngineWorker", (app_state,)),
        (metis_worker.metis_worker, "MetisWorker", (app_state,))
    ]

    # Add optional workers based on availability checks
    if BLUESKY_AVAILABLE:
        worker_functions.append((bluesky_context_worker, "BlueskyContextWorker", ()))
    if reddit_client:
        worker_functions.append((reddit_context_worker, "RedditContextWorker", ()))
    if FEEDPARSER_AVAILABLE:
        worker_functions.append((local_news_worker, "LocalNewsWorker", ()))

    # Start all the threads
    for func, name, args in worker_functions:
        thread = threading.Thread(target=func, args=args, daemon=True, name=name)
        worker_threads.append(thread)
        thread.start()
        print(f"Started {name} thread.")
        
    print("All worker threads started.")
    
    # --- STEP 5: Start the GUI Main Loop ---
    try:
        print("\nStarting GUI main loop...")
        status_label.config(text="Ready", fg="green") 
        gui_updater_worker()
        root.mainloop()
    except Exception as e:
        print(f"Error in GUI main loop: {e}")
        app_state.running = False
    finally:
        print("\nShutting down application...")
        app_state.running = False

        try:
            if 'recently_commented_post_ids' in globals() and isinstance(recently_commented_post_ids, list):
                print(f"Saving {len(recently_commented_post_ids)} Reddit comment IDs...")
                with open(REDDIT_COMMENT_CACHE_FILE, 'w', encoding='utf-8') as f:
                    start_index = max(0, len(recently_commented_post_ids) - RECENTLY_COMMENTED_CACHE_SIZE)
                    json.dump(recently_commented_post_ids[start_index:], f)
                print("Reddit comment cache saved.")
        except Exception as save_err:
            print(f"ERROR saving Reddit comment cache: {save_err}")
            traceback.print_exc()
        
        if 'tts_queue' in globals() and tts_queue:
            try:
                tts_queue.put(None) 
            except Exception as q_err:
                print(f"Error signaling TTS queue: {q_err}")

        print("Chat application closed.")

if __name__ == "__main__":
    main()